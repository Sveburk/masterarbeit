\sloppy
% cd /Users/svenburkhardt/Developer/masterarbeit/1_MA_Arbeit/
% source /Users/svenburkhardt/Developer/masterarbeit/.venv311/bin/activate
% pdflatex -shell-escape main.tex
% biber main
% pdflatex -shell-escape main.tex
% pdflatex -shell-escape main.tex





%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––    S E T T I N G S        ––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%



% Options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[12pt, a4paper, ngerman, bidi=default]{article}

%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––    S E T T I N G S     ––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––      P A K E T E        ––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
\def\langde{}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[fixed]{fontawesome5} %Fontawesome für Icons und Symbole; siehe https://mirrors.ibiblio.org/CTAN/fonts/fontawesome5/doc/fontawesome5.pdf
\usepackage{amsmath,amssymb}
\usepackage{tcolorbox}
\usepackage{afterpage}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{subcaption} %Für die Verwendung von subfigure
\usepackage{setspace} %Für den Befehl \setstretch
\setlength{\parindent}{0pt}
\usepackage{transparent}
\usepackage{tikz}
  \usetikzlibrary{shapes.geometric, arrows.meta,fit, backgrounds, calc, positioning}
\usepackage{pgf-pie}%Für Kreisdiagramme
\usepackage{pgfplots}%Für Diagramme
  \pgfplotsset{compat=1.18}
\usepackage{eso-pic}%Für Hintergrundbilder
\usepackage{fvextra}    %Muss vor csquotes geladen werden
% \usepackage{csvsimple}%Für CSV-Dateien
% \usepackage{booktabs}   %für \toprule, \midrule, \bottomrule
% \usepackage{longtable}  %falls nicht schon geladen
% \usepackage{placeins}%Für \FloatBarrier
\usepackage[autostyle, german=quotes]{csquotes}
\usepackage[
  backend=biber,
  style=authoryear-ibid,
  autocite=footnote,  % Automatisch Fussnote
  language=ngerman,
  citetracker,
  ibidtracker,
  backref=true,
  backrefstyle=three+
]{biblatex}
\let\cite\footcite


\DeclareBibliographyAlias{software}{misc}
\DefineBibliographyStrings{ngerman}{%
  page = {S\adddot},
  pages = {S\adddot\addspace ff\adddot}
}
\DefineBibliographyStrings{ngerman}{
  urlseen = {Zugriff am}
}
\DeclareFieldFormat{urldate}{\addspace#1}
\renewbibmacro*{url+urldate}{%
  \printfield{url}%
  \setunit*{\addspace}%
  \iffieldundef{urldate}
    {}
    {\printtext[urldate]{\printurldate}}%
}

\setlength{\skip\footins}{20pt}
\setlength{\bibitemsep}{1.5\baselineskip}
\addbibresource{assets/Literature_Bib/Masterarbeit.bib}

\renewbibmacro*{date}{% 
     \ifentrytype{online}{% Falls der Eintrag @online ist, nimm das urldate
         \printtext[parens]{Zugriff am \usebibmacro{urldate}}}{\printdate}}

\usepackage[ngerman]{babel}
\usepackage{pifont} %Für die Kästchen und Häkchen-Symbole
\usepackage[dvipsnames,svgnames,x11names,table]{xcolor}
\usepackage[table]{xcolor}
%Zum Definieren und Verwenden von Farben
\definecolor{LightGray}{gray}{0.9}
\definecolor{MediumGray}{gray}{0.7}
\definecolor{UniRot}{HTML}{D20537}                  %Corperate Design Farben Uni Basel 
\definecolor{UniAnthrazit}{HTML}{46505A}            %Corperate Design Farben Uni Basel 
\definecolor{UniMint}{HTML}{A5D7D2}                 %Corperate Design Farben Uni Basel 
\definecolor{VeryLightGray}{gray}{0.95}%Sehr helles Grau
\definecolor{OldPaper}{HTML}{f4eade}


\definecolor{UniBlue}{HTML}{1F78B4}     %Kräftiges Blau
\definecolor{UniGold}{HTML}{FFB000}     %Goldgelb
\definecolor{UniViolet}{HTML}{6A3D9A}   %Violett

\definecolor{abbrev}{RGB}{255,153,153}             %Rot für Tags
\definecolor{add}{RGB}{204,255,238}                %Hellgrün/Türkis für Tags
\definecolor{sic}{RGB}{255,255,153}                %Hellgelb für Tags
\definecolor{unclear}{RGB}{255,230,184}            %Hellorange für Tags
\definecolor{date}{RGB}{153,153,255}               %Blau für Tags
\definecolor{organization}{RGB}{255,153,255}       %Pink für Tags
\definecolor{place}{RGB}{204,153,255}              %Lila für Tags
\definecolor{person}{RGB}{153,255,153}             %Hellgrün für Tags
\definecolor{signature}{RGB}{153,255,153}          %Hellgrün (gleiche Farbe wie Person) für Tags
\definecolor{eventTag}{HTML}{05A9FF}               %Blau für Tags
\definecolor{oldLetter}{RGB}{246,238,227}          %Beige für Hintergrund  
\definecolor{vscode-blue}{HTML}{569CD6}            %VSCode Blau für python
\definecolor{vscode-yellow}{HTML}{DCDCAA}          %VSCode Gelb für python
\newcommand{\code}[1]{\colorbox{VeryLightGray}{\texttt{#1}}} %für die darstellung von Code 

\usepackage{array}
\usepackage{minted}
\usepackage{capt-of}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage[hyphens]{url}
\usepackage{ragged2e}
\usepackage[
  hidelinks,
  unicode=true,
  hyperfootnotes=true,
  colorlinks=true,
  linkcolor=Blue,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=Blue,
  pdfborder={0 0 0}
]{hyperref}
\usepackage{nameref}
\newcommand{\parref}[1]{Abschnitt \nameref{#1}}
\usepackage{orcidlink}
\usepackage{iftex}
\ifPDFTeX%
  \usepackage[utf8]{inputenc}
  \usepackage{lmodern}
  % Authblk, pdfpages etc. hier einbinden
\else
  \usepackage{fontspec}
\fi


% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

% Paragraph spacing configuration depending on the class
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
 }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{0pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother


\usepackage[lmargin=2.5cm,rmargin=2.5cm,tmargin=2cm,bmargin=2cm]{geometry}
\setlength{\emergencystretch}{3em}%prevent overfull lines
% \setcounter{secnumdepth}{-\maxdimen}%remove section numbering %Kapitel Zahl Nummer
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph%
  \renewcommand{\paragraph}{
    \@ifstar%
      \xxxParagraphStar%
      \xxxParagraphNoStar%
 }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph%
  \renewcommand{\subparagraph}{
    \@ifstar%
      \xxxSubParagraphStar%
      \xxxSubParagraphNoStar%
 }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc}%for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
%allow citations to break across lines
 \let\@cite@ofmt\@firstofone%
%avoid brackets around text for\cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa, #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2]%#1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
 %turn on hanging indent if param 1 is 1
  \ifodd #1 \else%chktex 1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
 %set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth-\csllabelwidth}{\strut#1\strut}} %chktex 8
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname%
  \renewcommand*\contentsname{Inhaltsverzeichnis}
\else
  \newcommand\contentsname{Inhaltsverzeichnis}
\fi
\ifdefined\listfigurename%
  \renewcommand*\listfigurename{Abbildungsverzeichnis}
\else
  \newcommand\listfigurename{Abbildungsverzeichnis}
\fi
\ifdefined\listtablename%
  \renewcommand*\listtablename{Tabellenverzeichnis}
\else
  \newcommand\listtablename{Tabellenverzeichnis}
\fi
\ifdefined\figurename%
  \renewcommand*\figurename{Abbildung}
\else
  \newcommand\figurename{Abbildung}
\fi
\ifdefined\tablename%
  \renewcommand*\tablename{Tabelle}
\else
  \newcommand\tablename{Tabelle}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\providecommand{\listoflistings}{\listof{codelisting}{Listingverzeichnis}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands%
\def\languageshorthands#1{}
\usepackage{bookmark}

\usepackage{placeins}                  %für \FloatBarrier
\usepackage[toc,page]{appendix}        %für eigene Anhangs-Umgebung
% optional:
% \usepackage{pdfcrypt}                  %für PDF-Passwortschutz

\IfFileExists{xurl.sty}{\usepackage{xurl}}{}%add URL line breaks if available
\urlstyle{same}%disable monospaced font for URLs
\hypersetup{
  pdftitle={Konzeption für AG Masterarbeit am
17.01.2025},
  pdfauthor={Sven Burkhardt},
  pdflang={de},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}
}

\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\title{\vspace*{4cm} \LARGE Digitale Harmonie aus historischer Dissonanz
\color{UniMint} \rule{8cm}{1pt} \\  
\vspace{0.2cm}  
\color{white}\large Extraktion, Ordnung und Analyse\\unstrukturierter Archivdaten\\des Männerchor Murg}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{}
\author{Sven Burkhardt}
\date{2025-08-14}%chktex 8

%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––– T I T E L B L A T T   ––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
\begin{document}
\begin{titlepage}
    
% Setzt die Schriftfarbe auf Weiss
\color{white}
\pagecolor[HTML]{46505A} %Seitenfarbe in Uni Basel Anthrazit D20537 (rot)
\pagenumbering{gobble}   %Verhindert die Anzeige der Seitennummer auf dem Titelblatt
\date{}
\author{}
\maketitle
\begin{center}
  \author{\LARGE{\author{\vspace{-0.5cm}Sven Burkhardt}}}\\
  \vspace{4mm}
  \large{\orcidlink{0009-0001-4954-4426} {0009-0001-4954-4426}}\\%chktex 8%Orcid Link und Nummer
  \begin{figure}[h]
    \centering
    \color{white}
    \large{\href{https://dhlab.philhist.unibas.ch/en/persons/sven-burkhardt/}{{\hspace*{0.5mm}\includegraphics[height=4.5
  mm]{./assets/Logos/Uni_basel_logo_white.png}}\hspace{3.4mm}\color{white} 17-056-912}}\\%chktex 8 %logo Unibas + Link + Immatrikulationsnummer
    \faIcon[regular]{calendar-alt}\date{\hspace*{2mm}15.08.2025}% chktex 8
  \end{figure}
  \setcounter{figure}{0}
\end{center}


% ------------ Hexagon grafik beginn -----------
\centering
\AddToShipoutPictureBG*{%
    \put(0,-40){%
        \includegraphics[width=\paperwidth]{./assets/Logos/Hexagon_Deko}
  }
}
\centering
\AddToShipoutPictureBG*{%
    \put(0,810){%
        \includegraphics[width=\paperwidth]{./assets/Logos/Hexagon_Deko}
   }
}
\centering
\AddToShipoutPictureBG*{%
    \put(33,752){%
        \includegraphics[width=\paperwidth]{./assets/Logos/Hexagon_Deko}
   }
}
\centering
\AddToShipoutPictureBG*{%
    \put(-99,752){%
        \includegraphics[width=\paperwidth]{./assets/Logos/Hexagon_Deko}
   }
}



\noindent%Verhindert Einzug des nachfolgenden Textes
% ------------ Hexagon grafik ende -----------



\begin{center}
    \vfill
    \begin{figure}
        \centering
        \begin{subfigure}{.3\textwidth}
          \centering
          \includegraphics[width=.8\linewidth]{./assets/Logos/uni-basel-logo-en_white.png}
        \end{subfigure}%
        \begin{subfigure}{.3\textwidth}
          \centering
          \includegraphics[width=.8\linewidth]{./assets/Logos/dhlab-logo-white.png}
        \end{subfigure}
    \end{figure}
        \setcounter{figure}{0}

    University of Basel\\
    Digital Humanities Lab\\
    Switzerland
\end{center}


\newpage
\color{black}          %Setzt die Schriftfarbe auf Schwarz für die folgenden Seiten
\setstretch{1.5}
\end{titlepage}
\newpage
%________________

%________________

%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––   A B S T R A C T     ––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%


\pagecolor{white}  
\color{black}  %Textfarbe zurücksetzen
\color{black} 
\setstretch{1.5}


\newpage 
Diese Arbeit befasst sich mit dem Archiv des Männerchor Murg in den Jahren des Zweiten Weltkrieges. 
Hierfür wird eine automatisierte Pipline auf Basis von LLMs und Patternmatching vorgestellt, mit deren Hilfe Named Entities extrahiert und weiterverarbeitet werden. 
Ziel ist es, dieses Archiv digital zugänglich, die beteiligten Personen sowie deren Netzwerke und dessen geographische Ausdehnung sichtbar zu machen.



%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––      T A B L E        ––––––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––        O F            ––––––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––   C O N T E N T S     ––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%



\newpage
\renewcommand*\contentsname{Inhaltsverzeichnis}%This controls the title of your table of contents.
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{5}%Sets the maximum sublevel to be displayed within the table of contents.
\tableofcontents
}
\pagenumbering{arabic}\setstretch{1.5}%Overwrites the previous command, pages are counted as normal from this point.


%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––      I N T R O D U C T I O N    ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
\newpage
\section{Einleitung}
\subsection{Ziel und Relevanz der Arbeit}
\subsection{Formulierung der Forschungsfrage}
\subsection{Aufbau der Arbeit}
\subsection{Geografischer und historischer Kontext}
Die vorliegende Arbeit stützt sich auf Unterlagen aus dem Archiv des \enquote{Männerchor Murg} dessen Nachfolge im Jahr 2021 
durch die \enquote{New Gospelsingers Murg} angetreten wurde. Murg ist eine deutsche Gemeinde am Hochrhein, 
rund 30 km Luftlinie von Basel entfernt. Der Ort liegt am gleichnamigen Fluss Murg, der in den Rhein mündet. 
Beide Gewässer bildeten über Jahrhunderte hinweg den wirtschaftlichen Motor der Region: Die Wasserkraft der Murg 
begünstigte früh die Ansiedlung von Mühlen, Hammerwerken und Schmieden entlang des Bachlaufs, während der Rhein mit seiner 
Drahtseil-Fähre eine bedeutende Verkehrs- und Handelsverbindung bot, die bis zum Ersten Weltkrieg privat betrieben wurde.

Mit dem Ausbau der Landstrasse, der heutigen Bundesstrasse 34, sowie dem Anschluss an die Bahnstrecke Basel–Konstanz 
entwickelte sich Murg im 19. Jahrhundert von einer landwirtschaftlich geprägten Siedlung zu einer Gewerbe-, Handels- und 
Industriegemeinde. Die Wasserkraft wurde dabei zu einem entscheidenden Standortfaktor: Die Ansiedlung der Schweizer 
Textilfirma \textit{Hüssy \& Künzli AG} im Jahr 1853\cite[vgl.][]{gemeinde_murg_geschichte_nodate} trug wesentlich 
zum wirtschaftlichen Wachstum der Gemeinde bei. Zahlreiche Arbeitskräfte, vor allem aus der benachbarten Schweiz, 
machten Murg zu einem wichtigen Standort der regionalen Textilindustrie.

Die Gründung des \textit{Männerchor Murg} im Jahr 1861 durch Schweizer Textilarbeiter belegt diesen engen Zusammenhang 
zwischen wirtschaftlicher Migration, Industrialisierung und lokalem Vereinswesen. Diese historische Verflechtung bildet 
eine zentrale Grundlage für die vorliegende Untersuchung.


  

\newpage
\section{Forschungsstand und Forschungslücke}\label{subsec:forschungsstand}

Die vorliegende Arbeit knüpft an zwei Vorarbeiten an, die in den Jahren 2018 und 2022 am Departement Geschichte der Universität Basel durchgeführt
werden. In zwei Transkribus-Seminaren werden erste Teilbestände der \enquote{Männerchor Akten 1925--1944} erschlossen und in einem Korpus
von 137 Einzeldokumenten zusammengeführt.\cite[vgl.][]{burkhardt_feldpost_2022}
Ein kleinerer Korpus von rund 50 Dokumenten wird mit Metadaten versehen. Erfasst werden unter anderem die genaue Position im Ordner auf Seitenebene, 
Kurztitel und Entstehungsdatum. Diese Metadaten bilden die Grundlage für eine erstmalige systematische Erschliessung.

Während in einem frühen Projektschritt vorrangig häufig genannte Personennamen (\enquote{Carl Burger}, \enquote{Fritz Jung})
dokumentiert werden, richtet sich der Fokus im zweiten Schritt auf die Feldpost. Ziel ist es, über die Auswertung der Feldpostnummern
Rückschlüsse auf beteiligte Militäreinheiten, deren Stationierungen und Verlagerungen während des Zweiten Weltkriegs zu ziehen.

Für diese Recherchen kommen einschlägige Fachliteratur zu den jeweiligen Fachgebieten zum Einsatz. 
Hier sind besonders die Bücher von Alex Buchner\cite[vgl.][]{buchner_handbuch_1989}, Christian Hartmann\cite[vgl.][]{hartmann_wehrmacht_2010}, 
Werner Haupt\cite[vgl.][]{haupt_buch_1982}, Christoph Rass\cite[vgl.][]{rass_deutsche_2009}, Georg Tessin\cite[vgl.][]{tessin_verbande_1977} 
und Christian Zentner\cite[vgl.][]{zentner_illustrierte_1983} zu nennen.

Darüberhinaus werden eigene Recherchen in den Beständen des \textit{Bundesarchivs – Militärarchiv Freiburg}\cite{hollmann_freiburg_2025} durchgeführt. Ergänzende Recherchen 
stammen aus den Suchlisten des \textit{Deutschen Roten Kreuzes (DRK)}\cite{reuter_drk_2025}. Hinzu kommen philatelistische Übersichts-Websites\cite{noauthor_feldpost_nodate}, 
die bei der Entzifferung von Briefmarken und Stempeln helfen.
Absoult essentiell für den Erfolg dieser Recherchen sind Citizen-Science-Foren\footnote{vor Allem werden verwendet: 
\textit{Forum der Wehrmacht}~\parencite{hermans_forum_nodate} und das \textit{Lexikon der Wehrmacht}~\parencite{altenburger_lexikon_nodate}.}. Sie ergänzen und validieren eigene Forschung.

Parallel zur inhaltlichen Erschliessungen entsteht 2022 eine erste digitale Storymap mit \textit{ArcGIS},
die zentrale Ergebnisse des Projekts öffentlich zugänglich macht. Grundlage bildet die Sichtung, konservatorische Aufbereitung und Digitalisierung
von zunächst rund 30 der etwa 800 Seiten Vereinsakten. Der Teilkorpus wird entheftet, gescannt und mit Metadaten wie Absender, Datum,
Feldpostnummer und Einheit versehen. Da jedes Dokument einen anderen Verfasser aufweist, erfolgt die Transkription manuell. Eine automatische Handschriftenerkennung 
ist aufgrund der heterogenen Schriftbilder nicht praktikabel.
Am Beispiel einzelner Sänger wie \textit{Emil Durst} lässt sich durch die Rechercheergebnisse mithilfe der Feldpostnummern und ergänzender Kartenmaterialien der Aufenthaltsort
bis auf Gebäude oder wenige Meter genau rekonstruieren. Diese Erkenntnisse werden mit historischen Karten, Luftbildern und Ortsrecherchen verknüpft
und in einer interaktiven ArcGIS-Karte visualisiert, die Stationierungen, Märsche und Frontverschiebungen der Chormitglieder anschaulich darstellt. 

Die in diesen Vorprojekten erarbeiteten Listen, Geodaten, Transkriptionen und Visualisierungen fliessen in die vorliegende Arbeit ein
und bilden eine wesentliche Grundlage für die erweiterte, automatisierte Pipeline, die im Folgenden vorgestellt wird. Dazu gehören beispielsweise auch die Verbandsabzeichen, 
Taktische Zeichen\cite[vgl][S.64-66]{haupt_buch_1982} der jeweiligen Einheiten, die auch in die Groundtruth der vorliegenden Arbeit inkorporiert werden.

Abgesehen von diesen Vorarbeiten ist der Quellenkorpus wissenschaftlich unerschlossen. Mit dieser Arbeit liegt erstmals eine
umfassendere wissenschaftliche Auswertung vor.

Mit der notwendigen manuellen Recherche in oben dargelegten Datenbankstrukturen wird zugleich sichtbar, wie sehr es an Brücken fehlt, 
um unterschiedliche Klassifikationen, fachspezifische Ordnungslogiken und semantische Webtechnologien nachhaltig miteinander zu verbinden. Ein verhältnismässig einfaches
Webscraping nach Informationen zu diesem Korpus ist nahezu unmöglich. Ausgeführt werden diese Probleme beispielsweise bei 
Smiraglia und Scharnhorst (2021)\cite[vgl.][]{richard_linking_2022}, 
 die anhand konkreter Fallstudien verdeutlichen, wie fragmentiert semantische Strukturen bislang entwickelt werden 
 und welche Hürden bei der praktischen Verknüpfung heterogener Wissensorganisationen bestehen. Dabei benennen sie insbesondere die 
 Herausforderungen bei der Übersetzung historisch gewachsener Klassifikationen in standardisierte semantische Formate, 
 die Notwendigkeit dauerhafter technischer Wartung und die Abhängigkeit von nachhaltigen Infrastruktur-Partnern\cite[vgl.][Kap. 2 und 5]{richard_linking_2022}.

Für eine Einordnung zu historischen Netzwerkanaylsen sei auf Gamper\&Reschke\cite{gamper_knoten_2015} verwiesen. Der Sammelband
\textit{Knoten und Kanten III} verdeutlicht, dass die historische Netzwerkanalyse zwar von einem interdisziplinär etablierten Methodenkanon 
profitiert, jedoch nach wie vor vor erheblichen Herausforderungen steht. Dazu zählen die Fragmentierung historischer Quellen, der hohe manuelle 
Erfassungsaufwand und methodische Desiderate im Umgang mit zeitlichen und räumlichen Dimensionen. Erschwerende Faktoren einer systematische Erfassung 
relationaler Strukturen. Dennoch eröffnen netzwerkanalytische Verfahren – besonders im Zusammenspiel mit relationaler Soziologie 
und Figurationsansätzen – neue Perspektiven auf Macht, Abhängigkeiten und Akteurskonstellationen in historischen Gesellschaften.






%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––              Korpus            ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––% 
\section{Korpus}\label{section:Korpus}
Aus dem Bestand des Ordners \textit{``Männerchor Akten 1925--1944''} werden für diese Arbeit ausschliesslich Akten verwendet, 
die während des Zweiten Weltkriegs verfasst wurden. Der Analysezeitraum erstreckt sich dementsprchend zwischen dem 01. September 1939 
und dem 8. Mai 1945, dem Tag der bedingungslosen Kapitulation Deutschlands.

Die zeitliche Eingrenzung ist notwendig, um die Funktionalität der im Folgenden beschriebenen Pipeline 
in einem klar definierten historischen Kontext demonstrieren zu können. Gleichzeitig führt TeXshop?diese Auswahl zu einer 
bewussten Reduzierung der potenziell erfassten Akteurinnen und Akteure, Orte und Organisationen. Diese Fokussierung ist 
insbesondere im Hinblick auf die Erstellung einer verlässlichen Groundtruth bedeutsam, die durch ergänzende Archivrecherchen 
mit historischen Metadaten angereichert wird.

Die Kombination aus einer präzise definierten Quellengrundlage und der digitalen Anreicherung dient dazu, das 
Potenzial der computergestützten Auswertung historischer Dokumente exemplarisch aufzuzeigen. 
Zugleich unterstreicht sie, dass die Qualität der Ergebnisse wesentlich von der sorgfältigen Eingrenzung 
des Korpus und der manuellen Validierung und Anreicherung abhängt.
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––              Quellen            ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%

\subsection{Quellen}
\subsubsection{Quellentradierung}
In den Lagerräumen der New Gospel Singers Murg, dem Nachfolgeverein des Männerchors Murg, 
wird im Jahr 2018 mehrere je ca. 800 Seiten umfassende Ordner mit historischen Unterlagen gefunden. 
Für diese Arbeit wird ein Ordner mit der Aufschrift \textit{\enquote{Männerchor Akten 1925--1944}} gewählt, da er neben dem Ordner 
\textit{\enquote{Männerchor Akten 1946--1950}} den grössten Zeitraum abdeckt. Darüberhinaus bietet er das Potential, 
aufschlussreiche Einblicke in das Vereinsleben in der Zeit vor und während des Nationalsozialismus, insbesondere des Zweiten Weltkrieges, zu geben.\\ 
Der Ordner umfasst insgesamt 780 Seiten und deren Inhalt kann als \enquote{Protokoll}, \enquote{Brief}, \enquote{Postkarte}, \enquote{Rechnung}, 
\enquote{Regierungsdokument}, \enquote{Noten}, \enquote{Zeitungsartikel}, \enquote{Liste}, \enquote{Notizzettel} oder \enquote{Offerte} kategorisiert werden.

Die Unterlagen könnten bereits direkt nach ihrer Entstehung in die Ordner eingelegt worden sein. Einzelne Akten sind mit einem 
\enquote{Heftstreifen}, auch \enquote{Aktendulli} genannt, zusammengefehtet. In der Plastikversion, wie er in diesen Akten vorliegt, wurde er bereits 1938 patentiert\cite[vgl.][]{noauthor_heftstreifen_2023}.
Wer die Akten so archiviert hat lässt sich nicht mehr sagen. 
Der sogenannte \enquote{Archival-Bias} des Archivars, also die Grundeinstellung, weshalb etwas aufbewahrt oder
vernichtet wurde, lässt sich damit nicht mehr feststellen. 

\subsubsection{Quellenbeschrieb}\label{Dokumententypen}
\noindent
\begin{minipage}[t]{0.49\textwidth}
  \setstretch{1.5}
  \justifying
  Für diese Arbeit wurde ein Korpus selektiert, dessen Auswahl in~\nameref{section:Korpus} näher beschrieben wird. 
  Erfasst, benannt und tabellarisch mit groben Metadaten versehen werden sämtliche Unterlagen aus dem Ordner \textit{\enquote{Männerchor Akten 1925--1944}}. 
  Diese Auflistung in der Datei \textit{Akten\_Gesamtübersicht.csv} erlaubt die Zuordnung zu folgenden Kategorien: Briefe, Postkarten, Protokolle, Regierungsdokumente, Zeitungsartikel, 
  Rechnungen und Offerten. Die Verteilung ist ungleichmässig: Briefe bilden mit 282 von 381 Seiten die grösste Kategorie; Rechnungen und Offerten sind jeweils nur einseitig vertreten.
\end{minipage}%
\hfill
\begin{minipage}[t]{0.49\textwidth}
  \centering
  \vspace*{0.3cm} % Feinjustierung nach Bedarf
  \begin{tikzpicture}
    \begin{axis}[
      ybar,
      bar width=16pt,
      height=5.5cm,
      ymin=0,
      ylabel={Anzahl Dokumente},
      symbolic x coords={Brief, Postkarte, Protokoll, Regierungsdok., Zeitungsartikel, Rechnung, Offerte},
      xtick=data,
      nodes near coords,
      nodes near coords align={vertical},
      x tick label style={rotate=45, anchor=east},
      axis lines=left,
      enlarge x limits=0.15,
      grid=major,
      grid style={dashed, gray!30},
    ]
      \addplot[fill=gray!60] coordinates {
        (Brief,282) 
        (Postkarte,51) 
        (Protokoll,36) 
        (Regierungsdok.,5) 
        (Zeitungsartikel,4) 
        (Rechnung,1) 
        (Offerte,1)
      };
    \end{axis}
  \end{tikzpicture}
  \captionof{figure}{Verteilung der Dokumententypen im untersuchten Bestand (150 Akten – 381 Seiten).}\label{fig:dokumententypen-bar}
\end{minipage}

\noindent
\begin{minipage}[t]{0.48\textwidth}
  \setstretch{1.5}
  \justifying
  Auf Grundlage der oben erwähnten, händisch erstellten Gesamtliste der Akten können durch 
  die systematische Benennung der Dokumente auch Rückschlüsse auf den bislang nicht untersuchten 
  Teil des Bestandes gezogen werden. Mithilfe des Sprachmodells von OpenAI wurde eine grobe Näherung zur 
  Zusammensetzung des restlichen Korpus erarbeitet, wie sie in der rechtsstehenden Darstellung visualisiert ist.
  
  Diese Übersicht erhebt keinen Anspruch auf Vollständigkeit oder Genauigkeit, sondern dient der Veranschaulichung, 

\end{minipage}%
\hfill
\begin{minipage}[t]{0.48\textwidth}
  \centering
  \vspace*{0.2cm} % Feinjustierung der vertikalen Ausrichtung
  
  \begin{tikzpicture}
    \begin{axis}[
      ybar,
      bar width=16pt,
      height=5.5cm,
      ymin=0,
      ylabel={geschätzte Verteilung restlicher Korpus},
      symbolic x coords={Briefe, Postkarten, Protokolle, Regierungsdok., Zeitungsartikel, Rechnungen, Offerten},
      xtick=data,
      nodes near coords,
      nodes near coords align={vertical},
      x tick label style={rotate=45, anchor=east},
      axis lines=left,
      enlarge x limits=0.15,
      grid=major,
      grid style={dashed, gray!30},
    ]
      \addplot[fill=gray!60] coordinates {
        (Briefe,155) 
        (Postkarten,12) 
        (Protokolle,7) 
        (Regierungsdok.,60) 
        (Zeitungsartikel,25) 
        (Rechnungen,8) 
        (Offerten,12)
      };
    \end{axis}
  \end{tikzpicture}
  \captionof{figure}{geschätzte Verteilung der Dokumententypen im restlichen Bestand.}\label{fig:dokumententypen-schätzung}
\end{minipage}

\vspace{1em}

\noindent
dass die Verteilung der Quellengattungen im Zeitraum vor dem Zweiten Weltkrieg möglicherweise deutlich anders gestaltet ist. Eine vertiefte Untersuchung dieser bislang unbearbeiteten Bestände erscheint daher notwendig und markiert eine zentrale Forschungslücke, die im Rahmen dieser Arbeit erstmals systematisch benannt wird.

\subsubsection{Sichtung \& Kategorisierung in Akten}
Für diese Arbeit werden alle Seiten in dem Ordner \textit{\enquote{Männerchor Akten 1925--1944}} 
zwei mal gesichtet und gelesen. Beim ersten Durchgang werden explorativ nach zusammenhängenden Unterlagen gesucht, die im Anschluss zu einer Akten gefasst werden können.

Ausschlaggebend für eine Zusammenfassung in einer Akte sind folgende Faktoren:
\begin{itemize}
  \item Historische Gliederung durch Bindung (Büroklammern, Heftstreifen, etc)
  \item gleiche Autorenschaft in direkt aufeinanderfolgenden Seiten
  \item gleiches Datum ["~"~"~"]
  \item gleiches Thema ["~"~"~"]
\end{itemize}
Auf dieser Grundlage wird eine Aktenübersicht\footnote{genannt Akten\_Gesamtübersicht.csv; in den Projektdaten} im CSV-Format erstellt. Sie seztt sich zusammen aus der Aktennummer, die die Reihenfolge innehalb des ursprünglichen Ordners beschreibt. In diesem ersten Schritt gilt die Lage als 
Identifikator für die Unterlagen. Jeder Nummer wird darüber hinaus ein beschreibender Titel, und das Erstellungsdatum zugewiesen.

Vorgreifend soll auch die zweite Quellensichtung beschrieben sein, in der diese Daten in der CSV um Metadaten auf Seitenebene und aus Transkribus ergänzt werden. Die Kategorisierung findet also in einem parallelen Prozess mit der Digitalisierung statt.
Hierfür werden die Akten auf Seitenebene genau ausgebaut. Dem zugrunde liegt eine internen Seiten-ID, die den Aktennamen und die Position innerhalb der Akte kombiniert (Bsp: Akt\_078\_S001.jpg). 
Ab dem Zeitpunkt des Uploads bei Transkribus wird diese jedoch durch Traskribus-Dokument-ID abgelöst. Beide IDs werden zu besseren Nachvollziehbarkeit in der CSV notiert.

Auch inhaltlich wird nochmals schärfer kategorisiert. Mit Tags in Apple-Dateien und der CSV wird nun folgendes erfasst:
\begin{itemize}
  \item Handschrift
  \item Maschinenschrift
  \item Bild
  \item Signatur
\end{itemize}
Die in \nameref{Dokumententypen} dargestellten Kategorisierungen werden nun als Groundtruthdaten in die CSV aufgenommen, um sie später in der Pipeline auszuwerten.

\subsection{Digitalisierung der Quellen}

Überlieferte analogen Dokumente müssen zunächst fachgerecht für das Projekt und den Digitalisierungsprozess aufbereitet werden. 
Hierzu werden die Akten aus ihren ursprünglichen Ablagesystemen entnommen und sorgfältig von Heftklammern, Büro- und Gummibändern 
befreit. Diese konservatorischen Massnahmen sind notwendig, um die langfristige Materialerhaltung zu gewährleisten, 
da insbesondere Korrosionsspuren ehemaliger Metallklammern die Papierfasern nachhaltig schädigen können. Zudem finden sich häufig 
Anzeichen von Säurefrass, sofern nicht säurefreies Archivmaterial verwendet wurde.

Für die eigentliche Digitalisierung kommt die native \enquote{Dateien}-Applikation von 
Apple\footnote{vgl.\href{https://support.apple.com/de-de/guide/preview/prvw28034/mac}{Apple Support: Dateien-App}} zum Einsatz. 
Diese bietet neben einer vergleichsweise hochauflösenden Erfassung die Möglichkeit zur direkten Speicherung in einem Cloud-basierten 
Speichersystem sowie eine automatische Texterkennung (OCR). Ziel dieser Vorgehensweise ist es, die digitalisierten Inhalte möglichst 
schnell durchsuchbar zu machen und standortunabhängig für das Projekt zugänglich zu machen.

Die Aufnahme der Dokumente erfolgt mithilfe eines Tablets, das auf einem stabilen Stativ exakt im rechten Winkel (90\textdegree)
über dem zu digitalisierenden Schriftgut positioniert wird. Diese einfache, jedoch effiziente Konfiguration gewährleistet eine 
gleichbleibend hohe Bildqualität bei gleichzeitig hoher Verarbeitungsgeschwindigkeit. Die digital erfassten Dateien werden konsistent
 benannt und folgen einer vorab definierten Gesamtübersicht der Bestände. Mehrseitige Konvolute werden dabei als zusammengehörige 
 Akteneinheiten geführt, während Einzeldokumente entsprechend separat erfasst werden. Die Archivierung erfolgt sowohl analog als auch 
 digital auf Seitenebene, um eine möglichst feingranulare Erschliessung zu ermöglichen.

Die initiale Speicherung erfolgt dabei standardmässig im PDF-Format. Für die anschliessende Verarbeitung mit den unten dargestellten 
Transkriptionswerkzeugen müssen die Dokumente jedoch in das JPEG-Format konvertiert werden. Die Umwandlung erfolgt automatisiert 
mithilfe eines eigens erstellten Python-Skripts, wie in Anhang~\ref{section:PDF_to_JPEG} beschrieben.\cite{burkhardt_githubpdf_to_jpegpy_2025}
Es extrahiert die Seiten, speichert im geeigneten 
Format ab und ergänzt die Dateinamen systematisch um eine dreistellige, führend nullengefüllte Seitennummer.

\subsection{Transkription}
Nach der Digitalisierung und Konvertierung der Dokumente beginnt die eigentliche Transkription. 
Wie im Kapitel \nameref{section:Transkriptionen_Methoden} dargestellt, 
entscheidet sich dieses Projekt bewusst für Transkribus als zentrale Plattform. 
Ausschlaggebend sind insbesondere die Möglichkeit, ein eigenes HTR-Modell auf Basis einer projektspezifischen Groundtruth zu trainieren, 
sowie die integrierten Funktionen zur Annotation von Named Entities direkt im Transkriptionsprozess.
Für die effiziente Transkription soll im Folgenden der Workflow beschrieben werden, der einen Mixed Method Ansatz verfolgt. 

\begin{minipage}[t]{0.52\textwidth}
  \setstretch{1.5}
  \justifying
  \noindent
Wie das Beispiel Abbildung \nameref{fig:Carl-handschrift} rechts verdeutlichen soll, sind viele der Akten schwer entzifferbar. Auch Transkribus kommt wegen der kleinen
Sets unterscheidlicher Autoren und Autorinnen für spezifische Handschrift an seine Grenzen. Mit Expertise sind diese nur mit hohem zeitlichen Aufwand transkribierbar. 
Die Baselines\footnotemark~ist in diesem Beispiel verhältnismässig homogen. Schwierigkeiten bereiten jedoch Postkarten oder Zeitungsartikel, die mit einer komplexeren 
Schrift-Setzung einen höheren Aufwand in der Transkription benötigen. 
\noindent
\end{minipage}%
\hfill%
  % Rechte Minipage = Bild
  \begin{minipage}[t]{0.43\textwidth}
    \centering
    \vspace*{0cm} % ► schiebt alles um 0.3cm nach unten
  \includegraphics[width=0.8\textwidth]{./assets/Images/Akte_076_S001.jpg}
  \captionof{figure}{Beispiel für handschriftlichen Text in Akte\_076}\label{fig:Carl-handschrift}
\end{minipage}
\vspace{1em}
\footnotetext{Baselines = Schriftausrichtung}

Ausgangspunkt für dieses Projekt ist das generischen Modell \textit{The German Giant I}, 
das mit einer CER\footnote{\textbf{CER} (Character Error Rate): Kennzahl für die Anzahl falsch erkannter Zeichen.} von 8,30\% zunächst auf 70 Akten angewendet wird. 
Sie umfassen 158 Seiten mit insgesamt 22.155 Wörtern. Die Ergebnisse
sind dabei jedoch sehr unpräzise, wie Abbildung \nameref{fig:Carl-Transkribus} veranschaulicht. 
In insgesamt vier Durchläufen über diese Selektion wird daher manuell eine Groundtruth für ein eigenes Modell erstellt und gleichzeitig regelbasiert und strukturiert Personen, Orte, Daten und
Organisationen getaggt. Zwar erweist sich ChatGPT in der direkten Texterkennung aus Bilddateien (OCR) als nicht ausreichend zuverlässig. 




\begin{minipage}[t]{0.52\textwidth}
  \setstretch{1.5}
  \justifying
  \noindent
Wie oben ausgeführt wird zur 
Erstellung des Groundthruth-Korpus bei der manuellen Korrektur OpenAIs ChatGPT-4o-Modell für die Rechtschreibprüfung verwendet. Die vermeintliche Schwäche 
bei der Transkription, passende Begriffe zu halluzinieren, stellen sich als besonders hilfreich heraus. Insbesondere in der Rekonstruktion fehlender Worte oder 
Satzteile aus dem semantischen Kontext heraus Wörter oder Satzteile. 
In Kombination mit der philologischen Expertise bei der Entzifferung einzelner Buchstaben entsteht so ein kollaborativer Transkriptionsprozess, 
bei dem Maschine und Mensch sich wechselseitig ergänzen. Die automatische Transkription wird in \autoref{fig:Carl-Transkribus} dargestellt, die überarbeitete LLM-Version folgt in \autoref{fig:Carl-LLM}.
\end{minipage}
\vspace{1em}
\begin{minipage}[t]{0.5\textwidth}
    \centering
    \vspace*{0cm}
  \begin{tcolorbox}[colback=oldLetter, colframe=black, sharp corners, width=0.8\textwidth]
    \tiny{Murg. 15. Aug 41 
    
    Mein lieber Alfons! 

    Sehen lunge Lreitt es mich dem Männer- 

    chor wieder einmal ein Liedehen zu stehten. 

    und kam mir die gestege Gelegenheit gussend. 

    Männechor Venstad um den Title das Liedchen 

    zu erhalten, wo sie zum Abschied am Aute 

    sängen ``auf Wiederschen Owohl ich Frei! 

    märke beifügte, keine Aentwarb. Vielleicht 

    gelingt es Dir diesen Iitel zu erhalten. 
    
    Weiterhin sänge ich fal Lied nur 
    
    ``Bas alte Lied'' von being. Rerohl 

    Es wurde 1928 am 10. Dachub. Sängerb. Frst 

    von Begrüssungsabend in Dien gesungen. 

    und erntete überaus grossen Reifall. 

    Es ich schwer das Richtige zu finden. 

    Aler Alfon, werst das Vemsladler Liedchen. 

    alsdann das Biener Lidchen und wenn 

    Leides unmöglich, dann freu Nall. 

    Mit herzl. Grüsse 

    Dein 

    Carl} 
  \end{tcolorbox}
  \captionof{figure}{LLM-Version von \autoref{fig:Carl-handschrift}\label{fig:Carl-Transkribus}}
\end{minipage}
\hfill%



\begin{minipage}[t]{0.52\textwidth}
  \setstretch{1.5}
  \justifying
  \noindent
Die so entstehnde Groundtruth wird für das Training des Modells
(\href{https://app.transkribus.org/models/public/287793}{ModelID: 287793})\footnotemark{} verwendet.
Dieses trainierte Modell erreicht eine CER von 6,58\% und kommt anschliessend für die automatische Transkription der übrigen Dokumente zum Einsatz. Auch hier ist eine manuelle Überprüfung der durch das eigens trainierte Modell erstellten
Transkription unabdingbar, die knapp 1.7\% geringere CER macht sich jedoch beim Korrekturaufwand bereits bemerkbar. Gleichzeitig wird diese Korrektur für das Taggen benutzt, das folgend beschrieben werden soll.

\end{minipage}
\footnotetext{\cite[vgl.][]{burkhardt_transkribus_2024}}
\begin{minipage}[t]{0.5\textwidth}
    \centering
    \vspace*{0cm}
  \begin{tcolorbox}[colback=oldLetter, colframe=black, sharp corners, width=0.8\textwidth]
\tiny{Murg, 15. Aug. 41

Mein lieber Alfons!

Schon lange treibt es mich, dem Männerchor wieder einmal ein Liedchen zu stiften, und kam mir die günstige Gelegenheit gelegen.

Ich schrieb vergangenes Jahr an den Männerchor Venstad, um den Titel des Liedchens zu erhalten, das sie zum Abschied am Auto sangen: \enquote{Auf Wiedersehen, o wohl ich frei!}

Ich fügte eine Frankierung bei, erhielt jedoch keine Antwort. Vielleicht gelingt es Dir, diesen Titel zu erhalten.

Weiterhin sang ich das Lied nur \enquote{Das alte Lied von Wien}. Obwohl es am 10. Dezember 1928 beim Sängerbund-Fest von Begrüssungsabend in Wien gesungen wurde und überaus grossen Beifall erntete, ist es schwer, das Richtige zu finden.

Aber Alfons, zuerst das Venstadler Liedchen, dann das Wiener Liedchen und wenn beides unmöglich, dann Fröhlichsein.

Mit herzlichen Grüssen

Dein

Carl}

\end{tcolorbox}
  \captionof{figure}{Transkription durch ChatGPT von \autoref{fig:Carl-Transkribus}\label{fig:Carl-LLM}}
\end{minipage}
\hfill%




\subsubsection{Tagging mit Transkribus und LLM}

Während der manuellen Korrektur der Transkriptionen erfolgt parallel die Annotation zentraler Entitäten. 
Transkribus bietet hierfür ein flexibles Tagging-System, mit dem sowohl strukturelle als auch semantische Informationen direkt im Dokument markiert werden können. 
Im Zentrum stehen dabei Tags für Personen, Orte, Organisationen und Datumsangaben. Diese Kategorien sind für die spätere Analyse besonders relevant, 
etwa für die Modellierung historischer Netzwerke oder die Kontextualisierung von Ereignissen.

Ein Mixed-Method-Verfahren kommt dort zum Tragen, wo die Transkription an ihre Grenzen stösst: 
Fehlende Buchstaben, fehlerhafte Worttrennungen oder unleserliche Handschriften lassen sich durch die Kombination aus Modellwissen 
und menschlichem Quellenverständnis rekonstruieren. ChatGPT liefert hier auf Basis des Kontexts plausible Vorschläge, 
die von einer historisch geschulten Bearbeitung geprüft und übernommen oder verworfen werden. Dieser kollaborative Vorgang verbessert 
nicht nur die Lesbarkeit, sondern erhöht auch die semantische Genauigkeit der rekonstruierten Passagen.

Ein Beispiel zeigt die schrittweise Entwicklung einer Transkription: Ausgehend von einem gescannten Originalbrief (\autoref{fig:Carl-handschrift}) 
wird zunächst eine maschinelle Transkription erstellt (\autoref{fig:Carl-Transkribus}), die anschliessend durch ein LLM geglättet und lesbarer gemacht wird 
(\autoref{fig:Carl-LLM}).

In einem letzten Schritt erfolgt die manuelle Annotation mit Transkribus-Tags (\autoref{fig:Tagging-Carl-LLM}): 
Hier werden etwa \texttt{Murg} und \texttt{Wien} als Orte, \texttt{Alfons}, \texttt{Carl} und \texttt{Kirchl} als Personen sowie das 
\texttt{Deutsch. Sängerb. Fest} als Ereignis markiert. Auch der Liedtitel \enquote{Das alte Lied von Wien} wird in seinen Bestandteilen 
zwischen Person, Ort und kulturellem Kontext aufgeschlüsselt. Für unklare oder unleserliche Textstellen, wie etwa das Fragment \texttt{auf Wiederschen}, 
kommt das Tag \texttt{unclear} zum Einsatz – häufig auf Grundlage einer Vorschlagsformulierung durch ChatGPT.

Zusätzlich zur semantischen Markierung ermöglicht Transkribus auch die Kennzeichnung struktureller Eigenschaften. 
So wird beispielsweise die Abkürzung \texttt{V.D.A.} – für \textit{Verein für das Deutschtum im Ausland} – mit dem Tag \texttt{abbrev} versehen, 
auch wenn diese Tags in der XML-Exportstruktur teilweise nicht vollständig erhalten bleiben (vgl. Kapitel~\ref{section:Transkriptionen_Methoden}).

Für die spezifischen Anforderungen dieses Korpus wird das Tagging-Schema gezielt erweitert, etwa um den benutzerdefinierten Tag \texttt{signature}, 
der handschriftliche Unterschriften maschinenlesbar ausweist. Das zweite Beispiel – ein poetischer Brief an Otto (\autoref{fig:Tagging-Carl-LLM}, unten) – 
zeigt die Anwendung dieses Verfahrens in lyrischer Sprache. Auch hier werden alle erwähnten Personen (u.\,a. \texttt{Otto}, \texttt{Lina Fingerdick}, 
\texttt{Otto Bollinger}, \texttt{Alfons Zimmermann}), Orte (\texttt{Murg}, \texttt{Laufenburg (Baden)}, \texttt{Rhina}) sowie Organisationen 
(\texttt{Männerchor}) mit den entsprechenden Tags versehen. Die adressierte Funktion \texttt{Vereinsführer des Männerchor} wird dabei 
als Organisationseinheit erfasst und semantisch vom Personenbezug getrennt.

Fehlerhafte oder fehleranfällige Passagen – insbesondere historisch bedingte Schreibungen oder Transkriptionsunschärfen – 
werden mit dem Tag \texttt{sic} versehen. In diesen Fällen folgt die standardisierte Lesart unmittelbar auf das markierte Original, 
wodurch ein differenzierter Umgang mit dem Quelltext sichergestellt ist.

Alle Tags werden während des Transkriptionsprozesses konsistent dokumentiert und in einem projektspezifischen Regelwerk festgehalten. 
Dieses dient nicht nur der internen Nachvollziehbarkeit, sondern auch als Grundlage für die spätere Verarbeitung durch Sprachmodelle, 
die auf die gleichen semantischen Kategorien angewiesen sind. Das strukturierte Tagging bildet somit die Brücke zwischen manueller 
Quellenarbeit und automatisierter Weiterverarbeitung.

% % \begin{itemize}
% %   \item Ausgangslage und technische Bedingungen
% % \item Zeitlicher Verlauf des Digitalisierungsprojekts
% % \item Geräte, Software (iPad, Apple Scan, ökonomische Gründe)
% % \item Mangelnde DH-Vorkenntnisse bei Beginn
% % \item Qualität und OCR-Auswirkungen
% % \item Erste Transkription und Groundtruth-Erstellung
% % \item Nutzung von German Giant I
% % \item Erste Fehleranalyse, CER = 8,3 %
% % \item Auswahl von 70 Akten für manuelle Korrektur
% % \item Iterative Verbesserung in 4 Schritten
% % \item Tagging-Strategie
% % \item Manuelles Tagging von Personen, Orten, Daten, Organisationen
% % \item Ergänzung um Custom-Tags wie \texttt{signature}
% % \item Verwendung von \texttt{sic} für fehlerhafte Vorlagen
% % \item Dokumentation der Regeln zur Weitergabe an LLMs
% % \item Modelltraining und finale Transkription
% % \item Aufbauendes Modell mit CER 6,58 %
% % \item Nutzung des Modells für 80 restliche Akten
% % \item Vorteile im Workflow, aber weiterhin manuelle Kontrolle nötig
% % \item Beispielhafte Ergebnisse (Referenz auf Boxen + Abbildungen)
% % \item Bildquelle → Transkription → korrigierte, getaggte Version
% % \item Beobachtungen: LLM transformiert Inhalt lesbar, aber semantische Fehler (z.B. „Venstad“ vs. „Neustadt“)

% % \end{itemize}

% % Für die Transkrition der Daten wurde ein best-practise Ansatz gewählt. Nach Tests mit dem Python-Modul \textit{\enquote{Tesseract}} 
% % und unterschiedlichen LLMs wurde auf Transkribus zurückgegriffen. Eine Gegenüberstellung der drei erwähnten Tools findet 
% % sich im Kapitel~\nameref{section:Transkriptionen_Methoden}


% \subsection{Tagging}
% blabla
% \subsubsection{Tagging mit Transkribus}
% blabla
% \subsubsection{Tagging mit LLM}
% blabla
% \subsection{Export}
% blabla

%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––           Forschungsstand        ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––% 


%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––           Methodisches Vorgehen        ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––% 
\section{Methodisches Vorgehen}

Digitale Methoden spielen für die Durchführung dieser Arbeit eine zentrale Rolle. Von der Digitalisierung der Quellen
über die Transkription bis hin zur Auswertung durchlaufen die Daten zahlreiche Prozessschritte, die mithilfe von Large 
Language Models, Deep-Learning-Modellen und anderen digitalen Werkzeugen verarbeitet und visualisiert werden. 
Die Auswahl der Tools orientierte sich dabei an Kriterien wie Verfügbarkeit (Open~Source vs.\ proprietär), 
Kompatibilität, Community-Support, erforderlichem Arbeitsaufwand und selbstverständlich dem konkreten Mehrwert für 
die Forschungsfragen.

In diesem Kapitel werden sowohl Werkzeuge vorgestellt, die tatsächlich eingesetzt wurden, als auch solche, die sich 
im Verlauf des Projekts als ungeeignet erwiesen. Transparenz ist hierbei ein wesentlicher Aspekt: Ein grosser Teil der 
Methodik entwickelte sich erst im Forschungsprozess selbst. Da sich Large Language Models rasant weiterentwickeln, 
ist nicht immer von Beginn an klar, ob ein Tool für den eigenen Anwendungsfall geeignet ist.
Um diese Unsicherheiten zu dokumentieren, werden hier auch gescheiterte Versuche dargestellt.

\subsection{LOD – Linked Open Data}\label{subsec:LOD}

Linked Open Data (LOD) bezeichnet einen dezentral organisierten Ansatz zur Veröffentlichung und 
Verknüpfung strukturierter Daten im Web. Ziel ist es, Datensätze verschiedener Institutionen und Akteure 
maschinenlesbar zugänglich zu machen und über standardisierte Formate wie RDF und SPARQL miteinander zu 
verbinden\cite[vgl.][S.~VI und 13f]{garoufallou_metadata_2020}.
Wesentliches Merkmal der LOD-Cloud ist dabei die Nutzung semantischer Beziehungen, insbesondere Äquivalenzen einzelner Daten. 
Hierfür wird häufig das Prädikat \texttt{owl:sameAs} genutzt, um z.B. mit \colorbox{VeryLightGray}{\textit{:Choir owl:sameAs wd:Q131186}} eine eigene 
Instanz als identisch mit der Wikidata-Entität für einen Chor zu deklarieren.
Klassen oder Instanzen können so aus unterschiedlichen Datenquellen eindeutig identifiziert und zusammengeführt werden.

Die OWL Web Ontology Language, entwickelt vom World Wide Web Consortium (W3C), ist damit ein zentrales Werkzeug für die Realisierung von 
LOD.\cite[ vgl.][]{smith_owl_2004} 
Mit ihr lassen sich Ontologien definieren, die Domänen über Klassen, Individuen und deren Relationen formal beschreiben. 
Sie ermöglichen, logische Schlussfolgerungen zu ziehen, um verteilte Datenbestände zu verknüpfen und maschinenlesbar auszuwerten.
Besonders relevant ist dabei \texttt{owl:sameAs}, das als Identitätsrelation fungiert: 
Es deklariert Instanzen, die in unterschiedlichen Quellen unter verschiedenen URIs\footnote{Abk. \textbf{URI}\: Uniform Resource Identifier} geführt werden, 
als dasselbe reale Objekt\cite[ vgl.][2.3. Data Aggregation and Privacy]{smith_owl_2004}
und ermöglicht so eine präzise Zusammenführung von Informationen — ein Grundpfeiler für die Interoperabilität im Semantic Web.
Die OWL-Spezifikation baut auf RDF\footnote{Abk. \textbf{RDF}\; Resource Description Framework} auf und erweitert es um zusätzliche Konzepte.
Die RDF-Daten werden häufig im Turtle-Format (TTL) serialisiert, einer textbasierten Notation für RDF, die eine kompakte, leicht lesbare Schreibweise bietet.
Dieses Format eignet sich besonders für den Austausch und die manuelle Bearbeitung von RDF-Tripeln.
Die Sprache liegt in drei Varianten vor\footnote{OWL Lite, OWL DL und OWL Full}, die sich im Grad ihrer Ausdrucksstärke 
unterscheiden.\cite[ vgl.][1.1. The Species of OWL.]{smith_owl_2004}
Insbesondere OWL DL bietet einen praktikablen Mittelweg zwischen hoher Ausdruckskraft und vollständigem, entscheidbarem Schliessen (Reasoning) 
und ist daher für viele LOD-Anwendungsfälle geeignet.

Trotz ihres Potenzials wird diese Form der Datenverknüpfung bislang jedoch nicht von allen Websites konsequent 
umgesetzt.\cite[ vgl.][S. 14]{garoufallou_metadata_2020}. Für die technische 
Umsetzung für diese Arbeit werden zwei zentrale Werkzeuge genutzt: Protégé zur Modellierung der Ontologie und GraphDB für deren Verwaltung und Abfrage.

\subsubsection{Protégé} Zur praktischen Modellierung der Ontologie kam \textit{Protégé} zum Einsatz. 
Protégé ist eine weit verbreitete Open-Source-Software zur Erstellung, Visualisierung und Verwaltung von Ontologien.
Die grafische Oberfläche unterstützt eine intuitive Klassendefinition, 
Relationserstellung und Instanzverwaltung. 
Mit Hilfe von Plugins können darüber hinaus logische Konsistenzprüfungen durchgeführt und 
Ontologien direkt im OWL-Format exportiert werden, um sie in LOD-Workflows einzubinden.
Die initiale Version der Ontologie für dieses Projekt entstand zuerst im Codeeditor \textit{Visual Studio Code} wurde aber schnell vollständig in Protégé überarbeitet.
Damit bildet das Programm die Grundlage für erste Experimente mit Abfragen in SPARQL.%

\subsubsection{GraphDB} Für die Speicherung und Abfrage der Ontologie wurde \textit{GraphDB} verwendet. 
GraphDB ist eine spezialisierte RDF-Triplestore-Datenbank, die es ermöglicht, 
grosse Mengen an semantisch verknüpften Daten effizient zu verwalten. 
Mit der integrierten SPARQL-Schnittstelle können Benutzer gezielt nach Instanzen, Klassen und Relationen suchen 
und komplexe Muster in den Datenbeständen erkennen. 
Im Rahmen dieser Arbeit diente GraphDB als Backend, um die in Protégé entwickelte Ontologie zu testen 
und mit realen Entitäten aus den untersuchten Quellen abzugleichen.



\noindent
\begin{minipage}[t]{0.52\textwidth}
  \setstretch{1.5}
  \justifying%
\subsubsection{LOD-Ontologie}
Ein wichtiger Aspekt dieser Arbeit ist die Unstrukturiertheit relevanter Informationen. 
Aus diesem Grund wurde auf der Basis der Oben beschrieben Semantik begonnen, eine eigene Ontologie zu entwickeln, die die identifizierten Entitäten systematisch erfasst.
Beim Schreiben dieser initialen Ontologie aus rund 2000 Zeilen Code erweist sich schnell ein neues Problem. Die Datengrundlage aus den geschilderten Vorprojekten (vgl.~\hyperref[subsec:forschungsstand]{Forschungsstand und Forschungslücke}) ist 
zu klein, um daraus eine aussagekräftige Netzwerkanalyse zu machen. Hierfür erweisen sich die Unterschiede der Daten zusätzlich als zu gross tische Grundlage des Globalund damit aufwendig. Der Fokus der Arbeit verschiebt sich dementsprechend von der Ontologieentwicklung 
auf die Extraktion von Entitäten.
\end{minipage}%
\hfill%
  % Rechte Minipage = Bild
  \begin{minipage}[t]{0.43\textwidth}
    \centering
    \vspace*{0.3cm} % ► schiebt alles um 0.3cm nach unten
    \includegraphics[width=\linewidth]{assets/Images/Bildschirmfoto_ttl_ontologie_Ausschnitt.png}
    \captionof{figure}{Ausschnitt der TTL-Ontologie.}\label{fig:ttl-ontologie}
\end{minipage}

\vspace{1em}

Der bestehende Datensatz ist zu klein, um eine umfangreiche Ontologie lohnend zu machen. Hinzu kommen externe Quellen, und deren Zugänglichkeit. Zuverlässige Quellen für Informationen 
über militärische Einheiten und deren Feldpostnummern sind das \enquote{Forum der Wehrmacht}\cite[ vgl.][]{altenburger_lexikon_nodate} 
und der \enquote{Suchdienst des DRK}\cite[ vgl.][]{reuter_drk_2025}. In beiden Fällen liegen die Daten jedoch nicht als LOD vor, sondern im Forum als einfache Strings 
und beim Deutschen Roten Kreuz als OCR-PDF\footnote{OCR = Optical Character Recognition} historischer Suchlisten aus der Nachkriegszeit. Ein manuelles Recherchieren dieser Daten scheint zu diesem Zeitpunkt
den Rahmen der Arbeit zu sprengen. Die in diesem Schritt geleistete Vorarbeit beim Sortieren und Klassifizieren von Entitäten, besonders in Verknüpfung mit selbst erstellten Wikidata-Klassen 
wird in späteren Prozessschritten wieder aufgegriffen\footnote{siehe Abschnitt~\hyperref[subsec:Nodegoat_chapter]{Nodegoat}}.

\subsection{Wikidata}

\textit{Wikidata}\cite[vgl.][]{noauthor_wikidata_nodate} ist eines der zentralen Repositorien für Linked Open Data, und bietet eine hohe Interoperabilität durch standardisierte
URIs, SPARQL-Endpunkte und offene APIs zu den Entitäten.
Jede Entität erhält dabei eine eindeutige, persistente URI 
(z.B. \colorbox{VeryLightGray}{\textit{wd:Q131186}} für einen Chor), die in LOD-Szenarien 
als stabiler Referenzpunkt dient.
Neben anderen betonen Martinez \& Pereyra Metnik (2024) beispielsweise: \\
\textit{\enquote{Wikidata stands out for its great potential in interoperability and its ability to connect data from various domains.}}\cite{martinez_comparative_nodate}

Wikidata entspricht, ebenso wie das nachfolgend beschriebene GeoNames, den FAIR-Prinzipien: Die Daten sind 
\textbf{F}\textit{indable} und \textbf{A}\textit{ccessible}, \textbf{I}\textit{nteroperable} und \textbf{R}\textit{eusable}\cite[ vgl.][S. 2]{wilkinson_fair_2016}.

Im Rahmen dieser Arbeit dient Wikidata als zentrale externe Referenz, um lokal erhobene Entitäten mit international etablierten Datenobjekten 
zu verknüpfen und so ihre Interoperabilität sicherzustellen. Die Plattform ermöglicht eine eindeutige Identifizierung sowie die maschinenlesbare 
Anreicherung um zusätzliche Informationen.

Die praktische Umsetzung zeigt jedoch eine strukturelle Einschränkung. Für diese Arbeiteigens angelegter Einträge auf Wikidata werden
trotz systematischer Verknüpfung mit anderen dort verwalteten Entitäten, etwa mit Armeen, Militäreinheiten, Orten und Personen, entfernt die Community–Moderation etwa 70\% dieser Einträge. 
Das zeigt einerseits hohe internen Qualitätsanforderungen auf, andererseits werden diese jedoch nicht klar kommuniziert. Mit regidem Löschen neuer Einträge wird die Verlässlichkeit und den Nutzen der 
geleisteten Arbeit erheblich begrenzt. 
Aufwand und Unsicherheit über die Persistenz der Einträge machen den ursprünglich vorgesehenen LOD–Ansatz in dieser Form nicht praktikabel.

\subsection{GeoNames}\label{subsec:geonames}

Ebenso wie Wikidata bietet \textit{GeoNames}\cite[vgl.][]{noauthor_geonames_nodate} eine Open-Source-Plattform für interoperable Daten. GeoNames fokussiert sich hierbei auf geografische Informationen 
und stellt eine umfassende Datenbank mit über 25 Millionen Ortsnamen und rund 12 Millionen eindeutigen geografischen Objekten bereit.  
Alle Einträge sind in neun Feature–Klassen und über 600 spezifische Feature–Codes kategorisiert. Die Plattform integriert Daten zu 
Ortsnamen in verschiedenen Sprachen, Höhenlagen, Bevölkerungszahlen und weiteren Attributen aus unterschiedlichen nationalen und internationalen Quellen.  
Sämtliche Geokoordinaten basieren auf dem WGS84–System\cite[\textit{WGS84: geodätische Grundlage des Global Positioning System (GPS)} ;vgl.][]{noauthor_wgs84_nodate}
und können über frei zugängliche Webservices oder eine API abgerufen werden.  
Darüber hinaus erlaubt GeoNames registrierten Nutzenden, bestehende Datensätze über eine Wiki–Oberfläche zu bearbeiten oder zu ergänzen, wodurch eine 
kollaborative Qualitätssicherung gewährleistet wird.

GeoNames wird in dieser Arbeit intensiv zur Referenzierung von Ortsnamen verwendet und bildet die Basis für die Groundtruth, wie sie in den Kapiteln \nameref{subsec:Nodegoat_chapter} und 
\nameref{subsec:place_matcher_chapter} beschrieben ist. Im Gegensatz zu Wikidata wurde hier von Beginn an darauf verzichtet, eigene Ortsdatensätze zu ergänzen. Dies liegt einerseits an 
den klar kommunizierten Community–Guidelines und andererseits daran, dass der Datensatz bis auf wenige, sehr lokale Flurnamen als nahezu vollständig gelten kann.

Historische Gebäude wie Gaststätten oder Spitäler fehlen folgerichtig in der GeoNames–Datenbank. Diese Lücke ist erwartbar, aber erwähnenswert, da GeoNames 
ansonsten eine nahezu vollständige und ausgesprochen detaillierte Datengrundlage bietet.

\subsection{Transkriptionen (Methodenvergleich)}\label{section:Transkriptionen_Methoden}
\subsubsection{Tesseract}\label{subsubsection:Tesseract}

Da bereits zu Beginn des Projekts klar ist, dass ein Grossteil der Datenverarbeitung mit Python-Code erfolgen soll, wird gezielt 
nach Werkzeugen gesucht, die eine automatische Transkription von gescannten Dokumenten ermöglichen. 
Als besonders etabliert erweist sich ein Open-Source-Projekt zur Texterkennung in Bilddateien, die OCR-Engine Tesseract. 
Tesseract wird seit den 1980er-Jahren entwickelt, zunächst von Hewlett-Packard, später von Google weitergeführt, 
und ist über GitHub öffentlich zugänglich.\cite[vgl.][]{weil_tesseract-ocrtesseract_2025}

Die Software basiert seit Version 4 auf einem LSTM-basierten\cite[Abk.: \textbf{LSTM} steht für \textit{Long Short-Term Memory};  Architektur der frühen Generation rekurrenter neuronaler Netzwerke \textit{RNNs}. LSTMs wurden entwickelt, um Sequenzdaten zu verarbeiten und dabei sowohl kurzfristige als auch langfristige Abhängigkeiten in der Datenfolge zu erfassen – ein typisches Beispiel sind Texte, Sprache, Zeitreihen oder Handschrift. ;vgl.][p.1-2]{beck_review_2020} neuronalen Netzwerk, das besonders bei der Erkennung von zusammenhängenden Textzeilen eine 
hohe Genauigkeit bietet. Tesseract unterstützt neben modernen Schrifttypen auch historische Schriftsätze wie Fraktur, was es besonders geeignet 
für den Einsatz in digitalisierten Archiven macht.\cite[vgl.][]{weil_tesseract-ocrtesseract_2025}

Vorbereitend für den Einsatz von Tesseract müssen alle gescannten PDF in das JPEG Format umgewandelt werden, wofür ein kurzes Python-Script verwendet wird\cite[vgl.][]{burkhardt_githubpdf_to_jpegpy_2025}. 
Im praktischen Einsatz scheiterte die Integration von TEsseract jedoch an der Heterogenität des Korpus: uneinheitliche Layouts, wechselnde 
Schrifttypen, maschinen- und handschriftliche Texte sowie komplexe Textverläufe. Beispielhaft sollen hier überlagerte oder mehrspaltig angeordnete 
Passagen auf Postkarten und Zeitungsartikeln genannt werden, die zu massiven Erkennungsfehlern führten. Auch mit angepassten Segmentierungsparametern (--psm) 
konnte keine zufriedenstellende Texterkennung erzielt werden.

Tesseract wurde daher nicht weiterverwendet.

\subsubsection{LLM}\label{subsubsec:LLM_transcript}
Analog zum Einsatz des OCR-Systems Tesseract\footnote{siehe Abschnitt \nameref{subsubsec:Tesseract}} stellt die Integration von Large Language Models (LLMs) von Beginn an einen zentralen Bestandteil der Projektkonzeption dar.
Aus diesem Grund wird in einer frühen Phase auch der Einsatz von LLMs, spezifisch ChatGPT, bei der Transkription der Unterlagen erprobt.

Der Einsatz von LLMs wie ChatGPT für die Transkription historischer Quellen erweist sich als ambivalent. Während die Modelle nach gezielter 
Anleitung eine erstaunlich präzise Rekonstruktion von Layoutstrukturen und maschinell erfassten Textdaten leisten, bestehen erhebliche Einschränkungen. Im Detail sind das
erhebliche Probleme bei der semantischen Genauigkeit. Das LLM beginnt sehr schnell mit sinnverändernden Haluzinationen, die unklare Textpassagen aus dem gelernten Kontext stimmig auffüllt.
Eine genaue Transkription, mit forcierter Notation von unklaren Stellen\footnote{Beispielsweise durch das Einfügen von \enquote{[\dots]}} gelingt in der Regel nicht. 
Die Verarbeitung handschriftlicher Dokumente scheitert weitgehend und führt zu stark spekulativen oder fehlerhaften Inhalten.

Hinzu kommen zu Projektbeginn technische Begrenzungen: Da ein API-Zugang zu OpenAI noch nicht verfügbar ist, erfolgt der Zugriff 
über die Weboberfläche. Diese stösst bei umfangreichen Eingaben rasch an Kapazitätsgrenzen; 
Sitzungen brechen häufig ab oder lassen sich nicht zuverlässig fortsetzen. Das kann zu inpersistenzen in der Promptstrukturierung und
dem Kontext des LLMs führen, was wiederum direkten Einfluss auf die Verarbeitung der Unterlagen hat.

Zum Zeitpunkt der exploratien Nutzun stelt das zentrales Hindernis der integrierte Content-Filter der Modelle dar. Inhalte mit Bezug zum Nationalsozialismus
führen zu einem sofortigen Abbruch der Verarbeitung. Beispielhaft sollen etwa Grussformeln wie \enquote{\textit{Heil Hitler}} genannt sein. Auffällig ist jedoch, dass
sich diese Filtermechanismen durch alternative Schreibweisen in den Quellen umgehen lassen. Die schreibweise \enquote{\textit{Heil -- Hitler}} umgeht den Filter komplett 
und wird ohne Einschränkung transkribiert.

Ohne den Zugang zur API und dem damit notwendigen Umweg über den Webclient zeigt sich 
zudem, dass LLMs ohne persistente Promptstrukturierung dazu neigen, wichtige Hintergrundinformationen zu vergessen. 
Eine Kombination aus Ground-Truth-gestützter Anleitung und manuellem Review ist daher notwendig, um eine verlässliche Transkription zu gewährleisten. Sie wird in dem Abschnitt \nameref{subsubsec:LLM_use} näher ausgeführt.

Aus den genannten Gründen kommt auch eine Transkription mittels generativem LLM nicht zum Einsatz.

\subsubsection{Transkribus}\label{subsubsec:Transkribus}
Transkribus ist eine webbasierte Plattform zur automatisierten Handschrifterkennung (HTR) und Texterkennung (OCR), die sich seit ihrer Entwicklung im EU-Projekt READ 
(Recognition and Enrichment of Archival Documents)\cite[vgl.][]{noauthor_recognition_nodate} als Standardwerkzeug in den digitalen Geschichtswissenschaften etabliert 
hat\cite[vgl.][]{muhlberger_transkribus_2019}. Betrieben wird Transkribus durch die READ-COOP SCE, einer europäischen Genossenschaft.

Die Plattform bietet zwei zentrale Zugriffsmöglichkeiten: einerseits die schlanke Webanwendung \textit{Transkribus Lite}, andererseits den \textit{Expert Client}, 
eine umfangreiche Desktopsoftware zur Bearbeitung und Verwaltung grosser Dokumentenkorpora. Beide Varianten ermöglichen die Transkription von gescannten Dokumenten, 
die Annotation von strukturellen und semantischen Einheiten sowie den Export in verschiedenen Dateiformaten. 

Die Nutzung des Expert Clients erlaubt darüber hinaus eine detaillierte Kontrolle über Transkriptionsprozesse und das zugrunde liegende Datenmanagement. 
Über integrierte Schnittstellen lassen sich grosse Datenmengen effizient verwalten. Auch externe FTP-Clients können zur Anbindung an das interne Dateisystem verwendet werden, 
um beispielsweise umfangreiche Digitalisate in strukturierter Form einzubinden.

Ein zentrales Merkmal von Transkribus ist die Möglichkeit, \textit{Tags} zu vergeben. Diese umfassen sowohl strukturelle Merkmale wie Abkürzungen, Unklarheiten oder Layout-Elemente, 
als auch semantische Einheiten wie Personen, Orte, Organisationen und Daten. Tags können individuell erweitert oder angepasst werden und werden im XML-Export maschinenlesbar dargestellt.

Die Exportfunktion von Transkribus erlaubt den Download der Transkriptionen im standardisierten PageXML-Format. Dieses Format ist auf die langfristige Nachnutzung 
struktureller Informationen ausgelegt und bildet die Grundlage für weiterführende Auswertungsschritte etwa in Digital Humanities-Projekten.

In der praktischen Handhabung zeigt sich jedoch eine teils deutliche Diskrepanz zwischen den im Interface sichtbaren Informationen und der tatsächlichen XML-Ausgabe. 
So werden beispielsweise benutzerdefinierte Abkürzungsauflösungen oder Listenstrukturen nicht zuverlässig im XML ausgegeben. 
Informationen, die manuell innerhalb der Transkriptionsumgebung gepflegt wurden, gehen im strukturierten Export unter Umständen verloren. 
Insbesondere bei Listenobjekten, etwa für Personenverzeichnisse oder Inventare, bleibt die XML-Struktur häufig leer. Eine Möglichkeit zur systematischen Nachbearbeitung 
oder maschinellen Extraktion steht bislang nicht bereit.

Diese Einschränkungen wurden auch in aktuellen Studien festgestellt. So verweisen Capurro et al.\cite[][]{capurro_experimenting_2023} im Rahmen ihrer Analyse mehrsprachiger Handschriftenkorpora 
auf signifikante Herausforderungen bei der automatisierten Layoutanalyse sowie bei der Verarbeitung komplexer Dokumentstrukturen. 
Sowohl beim Tagging als auch bei der Postcorrection sei weiterhin eine umfangreiche manuelle Nachbearbeitung notwendig, um konsistente und weiterverwendbare Datenformate zu erzeugen.

Trotz dieser Limitierungen liegt der methodische Mehrwert von Transkribus insbesondere in der Möglichkeit, ein eigenes HTR-Modell auf Basis einer spezifischen Groundtruth zu trainieren. 
Dies erlaubt es, auf charakteristische Eigenschaften eines konkreten Korpus einzugehen und so die Character Error Rate (CER) gegenüber generischen Modellen deutlich zu reduzieren. 
Darüber hinaus kann durch strukturierte Annotation eine Grundlage für die spätere Modellbewertung oder den Vergleich mit LLM-basierten Verfahren geschaffen werden.

Insgesamt stellt Transkribus eine leistungsfähige Plattform zur initialen Bearbeitung und Annotation historischer Quellen dar. 
Die automatisierte Erkennung unterstützt den Einstieg in umfangreiche Korpora, ersetzt jedoch nicht die editorische Kontrolle und Nachbearbeitung. 
Gerade für forschungsorientierte Projekte mit Fokus auf strukturierte, semantisch angereicherte Daten bleibt eine kritische Auseinandersetzung mit den technischen Grenzen unerlässlich.

\subsection{Large Language Models} \label{subsubsec:LLM_use}
Ein zentrales Werkzeug bei der Verarbeitung der historischen Quellen ist die weiter unten näher beschriebene Python-Pipeline, die auf der Verarbeitung von XML-Dateien basiert. Vorgreifend sei erwähnt, dass diese XML-Verarbeitung ein Large Language Model (LLM) zum Custom-Tagging nutzt. Nebst dem Tagging stellt das Programmieren dieser Pipeline eine der Kernherausforderungen dieses Forschungsprojekts dar. 
Für das Tagging und die Entwicklung der Pipeline werden verschiedene Large Language Models intensiv getestet und eingesetzt.
\subsection{Msty}
Um ein dafür geeignetes LLM zu evaluieren, werden zu Beginn des Projektes beispielhafte Prompts erstellt und deren Ergebnisse systematisch verglichen. Um diesen Vergleich zu erleichtern, wird die Desktop-Anwendung Msty\cite[vgl][]{noauthor_msty_nodate} eingesetzt. Zu den zentralen Funktionen gehören parallele Chatinterfaces (\enquote{Parallel Multiverse Chats}),
eine flexible Verwaltung lokaler Wissensbestände (\enquote{Knowledge Stacks})\cite[vgl][]{noauthor_msty_nodate}, sowie eine vollständige Offline-Nutzung ohne externe Datenübertragung. Msty dient dazu, verschiedene Modelle zu testen, durch die Parallel Multiverse Chats Antworten zu vergleichen und Konversationen strukturiert zu verzweigen und auszuwerten.

Wichtig ist, dass dies kein klassisches Benchmarking auf Basis vergleichbarer Resultate ist. Es wird zu diesem frühen Projektzeitpunkt weder systematisch überpüft, welche Qualität der jeweilge Codeteil hat, noch wird gemessen, wie viel Prozent der Named Entities jeweils richtig erkannt werden. Der direkte Vergleich der getesteten LLMs liefert jedoch schnell ein klares Bild, welches Modell sich am besten eignet. Beprobt werden die Folgenden Anbieter und Modelle:



\subsection{Alphabet – Gemini}
\subsection{Anthopic – Claude}
\subsection{OpenAI – ChatGPT}


\subsection{Nodegoat}\label{subsec:Nodegoat_chapter}

Nachdem sich die Implementierung von Linked Open Data (LOD) für das vorliegende Projekt aufgrund des hohen zeitlichen 
Aufwands als nicht realisierbar erwiesen hat, wird mit \textbf{Nodegoat}\cite{kessels_nodegoat_2013}
eine praktikable und zugleich forschungsnahe Alternative eingeführt. Im Folgenden soll das Tool näher beschrieben, und ihre Anwendung für das Projekt erläutert werden.

Nodegoat ist eine webbasierte Plattform, die sich besonders in den Digital Humanities etabliert hat. Es unterstützt Forschende in der Modellierung, 
Verwaltung, Analyse und Visualisierung komplexer Datenbestände. Ein zentrales Merkmal von Nodegoat ist die grafische Benutzeroberfläche, die eine vergleichsweise niedrige Einstiegshürde bietet. 
Auch Forschenden ohne tiefgehende Programmierkenntnisse wird so die Möglichkeit  eröffnet, eigene Datenmodelle zu definieren, zu pflegen und weiterzuentwickeln. 
Die Plattform folgt einem modularen Prinzip: Über das UI\footnote{Abk.:\textbf{UI} User-Interface} können beliebig viele Datenmodelle 
erstellt werden, die sich flexibel an die spezifischen Forschungsfragen anpassen lassen. Diese hohe Individualisierbarkeit der Datenstrukturen erlaubt es, innerhalb kürzester Zeit projektspezifische 
Datenbanken zu konzipieren und fortlaufend zu erweitern oder an sich ändernde Bedürfnisse anzupassen.

Die Grundstruktur eines Modells unterscheidet in Nodegoat zwischen sogenannten \code{Object Descriptions} und \code{Sub-Objects}. 
Erstere legen die grundlegenden Merkmale eines Objekts fest, etwa Zeichenketten, Zahlen oder Verweise auf andere Einträge. 
So kann eine \code{Person} in diesem Projekt neben einem Feld für Vor- und Nachname auch eine Referenz auf ein \code{Gender}-Objekt enthalten, 
das eine eindeutige Geschlechtszuordnung ermöglicht.

Für das hier behandelte Forschungsvorhaben übernimmt Nodegoat die zentrale Verwaltung der Groundtruthdaten, die später als CSV-Export in die Pipeline integriert werden. 
Für die Groundtruth werden folgende Entitäten modelliert:

\begin{table}[h]
  \renewcommand{\arraystretch}{1.5}
  \centering
  \rowcolors{1}{VeryLightGray}{VeryLightGray}
  \begin{tabular}{|p{0.45\textwidth}|p{0.45\textwidth}|}
    \hline
    Personen & Orte \\ \hline
    Organisationen & Ereignisse \\ \hline
    Dokumente & Rollen \\ \hline
    Gender & \\ \hline
  \end{tabular}
  \caption{\small Übersicht der erfassten Entitäten}
\end{table}

Die Auflistung ist dabei so geordnet, dass die Entitäten mit der grössten Anzahl oder Vielfalt an zugehörigen Sub-Objects zuerst genannt werden. \code{Sub-Objects} erlauben eine
noch vielfältigere Abbildung abhängiger Informationen. Das können in dieser Arbeit zum Beispiel Quellenbelege, sowie temporale oder lokale Attribute, die einem Hauptobjekt zugeordnet werden. Lokale Attribute 
werden in Verknüpfung mit \hyperref[subsec:geonames]{GeoNames} und für Länder, Flurnamen oder Gewässer im Geojson-Format\cite[Weiterführend:][]{thomson_geographic_2017}. So lassen sich beispielsweise für\code{Persons}
in den Sub-Objekten \code{Geburt} oder \code{Tod} das Datum und der Ort modelieren, sowie die Todesursache notieren.

Die in den Kapiteln \hyperref[section:Transkriptionen_Methoden]{Transkriptionen und Methoden}, \hyperref[subsec:forschungsstand]{Forschungsstand und Forschungslücke} 
sowie \hyperref[subsec:unmatched_logger]{Unmatched Logger} beschriebenen Verarbeitungsschritte bilden die Grundlage für die in Nodegoat hinterlegten Entitäten.
Die strukturierte Erfassung dient dabei nicht nur der internen Qualitätssicherung, sondern ermöglicht auch eine nachhaltige Referenzierbarkeit durch eindeutig 
vergebene Identifikatoren, die beispielsweise beim Abgleich von Personendaten genutzt werden, um Textstellen präzise mit den zugehörigen Datenbankeinträgen zu verknüpfen.

Darüber hinaus wird Nodegoat über seine Programmierschnittstelle (API) mit einer eigens entwickelten Webanwendung verknüpft. Diese Webanwendung fungiert als publikumsorientierte 
Such- und Präsentationsplattform für die erarbeiteten Quellen. Die in den strukturierten JSON-Daten enthaltenen Nodegoat-IDs verknüpfen hier ebenfalls jede identifizierte Named Entity eindeutig mit dem 
zugehörigen Objekt in der Nodegoat-Datenbank. Dies soll nachfolgend erläutert werden.

Kritisiert werden muss an Nodegoat die fehlende Doukumentation. Viele Informationen finden sich nur über Drittparteien\cite[beispielsweise durch Schulungsunterlagen von Universitäten, 
hier besonders:\\][]{gubler_nodegoat_nodate} oder, wie unten geschildert, auf konkrete Nachfrage bei den Entwicklern. Der dann geleistete Support 
ist jedoch ausgesprochen zeitnah und umfassend. 



\subsection{\colorbox{red}{Webtool}}\label{subsec:webtool_chapter}
Die Planung sieht vor, oben beschriebene Verknüpfung zu nutzen, um aus der Webanwendung heraus bei Bedarf Detailinformationen zu den einzelnen Entitäten abzurufen. 
Dazu wird über spezifische API-Requests die jeweilige Objektbeschreibung geladen. In einem exemplarischen Anwendungsfall wird etwa eine Abfrage an eine URL der Form
\begin{figure}
  \hspace*{-0.6cm}%
  \centering\code{https://api.nodegoat.dasch.swiss/data/type/\colorbox{MediumGray}{11680}object/ngEL9c68pELQqGVuoFN49t/}
\end{figure}
gesendet. Hierbei steht die Type-ID \textit{11680} im Beispiel für den jeweiligen Modelltyp \enquote{Organisation} und die Zeichenkette am Ende für die eindeutige Nodegoat-ID 
des Objekts\footnote{im Beispiel eine gekürzte Nodegoat-ID}.

Die dabei zurückgelieferte JSON-Antwort enthält die gespeicherten Metadaten (z.B. Namensvarianten, Zugehörigkeiten, Quellenbelege). Um diese Informationen benutzerfreundlich darzustellen, 
wird angestrebt, den aus Nodegoat bekannten \enquote{Object Detail View} in Form eines iFrames direkt in die Webanwendung einzubetten. Dabei kann über gezielte CSS-Regeln gesteuert werden, dass 
lediglich der gewünschte Objektbereich angezeigt wird, ohne die übrigen Bestandteile der öffentlichen Nodegoat-Oberfläche zu übernehmen.

In der technischen Umsetzung wurde zudem erörtert, ob die interne Object-ID oder die plattformübergreifende Nodegoat-ID als Referenz verwendet werden sollte. Die Rückmeldung bei den
Nodegoat-Entwicklern (Kessels und van Bree) legt auf Anfrage nahe, dass beide ID-Typen im Prinzip austauschbar sind: Die Object-ID gewährleistet eine eindeutige Identifikation 
innerhalb einer spezifischen Nodegoat-Instanz, 
während die Nodegoat-ID eine konsistente Referenz über mehrere Installationen hinweg ermöglicht — auch im Hinblick auf LOD-Kompatibilität.

Für die Integration in die eigene Webanwendung wird daher ein hybrides Modell verfolgt: In den JSON-Daten der Quelltexte werden Nodegoat-IDs gespeichert, um langfristig eine offene Verknüpfbarkeit sicherzustellen. 
Gleichzeitig wird über die API der jeweilige Objekttyp (z.B.\enquote{Person}, \enquote{Organisation}) abgefragt, um die semantischen Eigenschaften der Entität zu laden. Diese werden mit dem 
Model-Endpunkt kombiniert (z.B. \code{https://api.nodegoat.dasch.swiss/model/type/11680}), um etwa Labelstrukturen oder benutzerdefinierte Felder korrekt abzubilden.

Auf diese Weise kann die Webanwendung den Nutzenden nicht nur eine reine Objektliste liefern, sondern auch kontextreiche Detailansichten generieren. Sie sind als iFrame eingebettet oder dynamisch 
gerendert und visualisieren alle relevanten Informationen direkt aus Nodegoat. Um die Serverlast zu minimieren, wird hierbei eine Caching-Strategie empfohlen, sodass wiederholte API-Abfragen 
effizient verarbeitet werden können.

Zusammengefasst fungiert Nodegoat somit als zentrales Bindeglied zwischen der internen Datenhaltung und der öffentlichen Präsentation: Es vereinfacht die Pflege konsistenter Groundtruth-Daten, 
unterstützt deren Ausspielung über standardisierte Schnittstellen und ermöglicht eine modular erweiterbare Verknüpfung mit Webportalen und Suchsystemen.



% \subsection{Netzwerkanalyse als Methode}
%   \subsubsection{Theoretischer Hintergrund der Netzwerkanalyse}
%   \subsubsection{Ziele der Netzwerkanalyse im Kontext der Quellen}
%   \subsubsection{Technische Umsetzung (Tools, Datenbankstruktur)}



%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––           Pipeline        ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––% 
\section{Pipeline}

\subsubsection{Übersichtsgrafik der Pipeline}

Die untenstehende Grafik soll im Folgenden als visuelle Orientierung dienen. 
Sie bildet die logische Gliederung der Pipeline ab, wie sie im weiteren Verlauf des 
Kapitels analysiert wird. Zum Einsatz kommt sie als Referenz für nachfolgende Module in Form schematischer Zeichnungen bei der Einordnung der verschiedenen Verarbeitungsschritte und Modulabhängigkeiten. 
Als zentrale Übersicht gliedert sich diese Darstellung in drei farblich differenzierte Ebenen, die im Folgenden erläutert werden.

Grün markiert sind externe Ressourcen, die die Grundlage der Verarbeitung bilden. Dazu zählen 
zum einen XML-Dateien, die aus dem Transkriptionsprogramm Transkribus stammen, und zum anderen 
strukturierte Groundtruth-Daten im CSV-Format, die zuvor aus Nodegoat exportiert 
wurden. Dieser Prozess ist im Kapitel \nameref{section:Transkriptionen_Methoden} und \nameref{subsec:Nodegoat_chapter} dargestellt.

Orange steht für die zentralen Verarbeitungsschritte der Pipeline. Dazu gehört insbesondere das 
Preprocessing durch das LLM, wie im Abschnitt \nameref{subsec:preprocessing} 
ausgeführt, sowie das Hauptmodul \code{transkribus\_to\_base.py}. Dieses Modul koordiniert den 
gesamten Verarbeitungsablauf und wird im nachfolgenden Kapitel \nameref{sec:transkribus_to_base} ausführlich behandelt.

Blau gekennzeichnet sind die einzelnen Funktionsmodule, in die sich \code{transkribus\_to\_base.py} 
unterteilt. Diese übernehmen spezialisierte Aufgaben, etwa die Erkennung und Anreicherung von Personen, 
Orten, Organisationen oder Ereignissen. Eine detaillierte Beschreibung der jeweiligen Module erfolgt im 
Abschnitt \nameref{subsec:Module_im_Detail}.


\vspace{0.9\baselineskip}
  \centering
  \resizebox{0.99\textwidth}{!}{
    \begin{tikzpicture}[
  module/.style={rectangle, draw=black, fill=blue!10, thick, minimum width=4.5cm, minimum height=1.2cm, align=center},
  process/.style={rectangle, draw=black, fill=orange!20, thick, minimum width=4.8cm, minimum height=1.2cm, align=center},
  source/.style={rectangle, draw=black, fill=green!30!gray!20, thick, minimum width=4.2cm, minimum height=0.7cm, align=center},
  group/.style={draw=gray, dashed, rounded corners, inner sep=0.5cm},
  arrow/.style={-Latex, thick}, arrowboth/.style={<Latex>-<Latex>, thick},
  node distance=0.8cm and 1.6cm
]


% --- Hauptquellen oben ---
\node[source] (transkribus) at (-14.5, 0) {\textbf{app.transkribus.org} \\ Transkription};
\node[process] (dir) at (0, -3.2) {\textbf{Dateiverzeichnis} \\ mit XML-Dateien};
\node[process] (llmpre) at (6, -3.2) {\textbf{llm\_XML\_preprocessing.py}};
\node[process, below=7.5cm of dir] (main) {\textbf{Transkribus\_to\_base.py} \\ Hauptverarbeitung};

% ------------------- MODULE (FLOWCHART) ------------------------
\node[module, below=2cm of main] (init) {Initialisierung \\ (CSV-Import, Matcher)};
\node[module, below=of init]    (xml)  {XML \& Custom-Tags \\ Parsen, extract\_metadata};

% Referenzpunkt zentriert unter xml
\coordinate (modcenter) at ($(xml.south) + (0,-1.8)$);

\node[module] (roles)   at ($(modcenter) + (-8cm, -0.5)$) {Rollen \\ zuweisen, anreichern};
\node[module] (persons) at ($(modcenter) + (-2.7cm, -0.5)$) {Personen \\ match, split, enrich};
\node[module] (orgs)    at ($(modcenter) + (2.7cm, -0.5)$)  {Organisationen \\ extrahieren, deduplizieren};
\node[module] (places)  at ($(modcenter) + (8cm, -0.5)$)  {Orte \\ Kontext + Kombination};
\node[module] (dates)  at ($(modcenter) + (13cm, -0.5)$) {Daten \\ combine\_dates(), count};
\node[module] (events)  at ($(modcenter) + (-13.5cm, -0.5)$)  {Events \\ extract\_events\_from\_xml()};
\node[module, below=1.2cm of persons] (authors) {Autoren \& Empfänger \\ infer, dedup, score};
\node[module] (unmatched) at ($(modcenter) + (0cm, -7.5cm)$) {unmatched\_logger.py};
\node[module] (json)  at ($(modcenter) + (0cm, -9.5cm)$)  {BaseDocument Build \\ + Rollenpostprocessing};
\node[coordinate] (joinpoint) at ($(unmatched.north) + (0, 1.5cm)$) {};
\node[coordinate] (joinpointxml) at ($(xml.south) + (0, -0.5cm)$) {};
\node[source, below=of json] (save) {Speicherung \\ JSON pro Seite + total\_json};
\node[source, right=of save] (unmatchedjson) {unmatched.json};

% CSV-Quellen
\node[source] (csv1) at ($(transkribus.south) + (0.5, -4.5)$) {\textbf{export-person.csv}};
\node[source, below=0.6cm of csv1] (csv2) {\textbf{export-place.csv}};
\node[source, below=0.6cm of csv2] (csv3) {\textbf{export-roles.csv}};
\node[source, below=0.6cm of csv3] (csv4) {\textbf{export-organisationen.csv}};

\node[group, fit=(csv1)(csv2)(csv3)(csv4), name=nodegoatbox,
  label={[anchor=north west, xshift=-0.5cm, yshift=0.7cm]north west:\texttt{Vorverarbeitung: Nodegoat-Exporte}}] {};

\node[group, fit=(transkribus), name=pretranskribusbox,
  label={[anchor=north west, xshift=-0.5cm, yshift=0.7cm]north west:\texttt{Vorverarbeitung: Transkribus}}] {};

 %--- Gruppenrahmen für Modulblock ---
\node[group, fit=
  (persons)(authors)(roles)
  (orgs)(dates)(events)
  (places)(joinpoint), name=flowchartbox] {};

% --- Gruppenrahmen um Hauptverarbeitung + Flowchart ---
\node[group, fit=(main)(flowchartbox)(save),
  label={[anchor=north west, xshift=12.5cm, yshift=+0.7cm]north west:\texttt{Hauptverarbeitung der Transkribus\_II-Pipeline}}] {};


% Verbindungen
\draw[arrow] (transkribus.east) -- ++(1.2,0)-- ++(0,-3.2)-- node[pos=0.6, left, font=\small, xshift=-2cm, yshift=1.3cm]{\textit{Export als XML}} (dir.west);  
\draw[arrow] (dir.east) -- (llmpre.west);
\draw[arrow] (llmpre.north) -- ++(0,1.2) -| node[pos=0.5, above, font=\small, xshift=3cm] {\textit{Custom-Tags}} (dir.north);
\draw[arrow] (dir.south) -- (main.north);
\draw[arrow] (main.south) -- (init.north);
\draw[arrow] (init.south) -- (xml.north);

\draw[arrow] (xml.south) -- (joinpointxml.south);
\draw[arrow] (joinpointxml) -- ($(roles.north |- joinpointxml)$) -- (roles.north);
\draw[arrow] (joinpointxml) -- ($(persons.north |- joinpointxml)$) -- (persons.north);
\draw[arrow] (joinpointxml) -- ($(orgs.north |- joinpointxml)$) -- (orgs.north);
\draw[arrow] (joinpointxml) -- ($(places.north |- joinpointxml)$) -- (places.north);
\draw[arrow] (joinpointxml) -- ($(dates.north |- joinpointxml)$) -- (dates.north);
\draw[arrow] (joinpointxml) -- ($(events.north |- joinpointxml)$) -- (events.north);
\draw[arrow] (unmatched.east) -- ($(unmatchedjson.north |- unmatched.east)$) -- (unmatchedjson.north);

\draw[arrow, dashed] (persons.south) -- (authors.north);
\draw[arrow] (persons.west) -- (roles.east);
\draw[arrow] (roles.east) -- (persons.west);   
\draw[arrow] (persons.east) -- (orgs.west);
\draw[arrow] (orgs.west) -- (persons.east); 
\draw[arrow] (persons.east) -- (orgs.west);
\draw[arrow] (orgs.west) -- (persons.east);      
\draw[arrow] (orgs.east) -- (places.west);  
\draw[arrow] (places.west) -- (orgs.east);  

             
% Doppelte Pfeile mit definiertem Stil
\draw[arrow] (events.south) -- ($(events.south |- joinpoint)$);
\draw[arrow] ($(events.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (roles.south) -- ($(roles.south |- joinpoint)$);
\draw[arrow] ($(roles.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (authors.south) -- ($(authors.south |- joinpoint)$);
\draw[arrow] ($(authors.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (orgs.south) -- ($(orgs.south |- joinpoint)$);
\draw[arrow] ($(orgs.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (places.south) -- ($(places.south |- joinpoint)$);
\draw[arrow] ($(places.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (dates.south) -- ($(dates.south |- joinpoint)$);
\draw[arrow] ($(dates.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (joinpoint.south) -- (unmatched.north);


\draw[arrow] (json.south) -- (save.north);
\draw[arrow] 
  (nodegoatbox.south) |- ([xshift=-0.4cm]init.west)
  node[pos=0.6, left, font=\small, xshift=2.6cm, yshift=2.8cm]{\textit{liefert Groundtruth}};
\end{tikzpicture}
  }
  \captionof{figure}{Übersicht der gesamten XML-to-JSON-Pipeline}\label{fig:pipeline-uebersicht}
\vspace{\baselineskip}
\justifying. %Blocksatz nach Grafik

\subsection{Vorverarbeitung}\label{subsec:preprocessing}
Neben Transkribus, das bei der Transkription einen elementaren Schritt in der Vorverarbeitung aller Dokumente darstellt,
müssen die Seiten für bessere Ergebnisse weiter vorbereitet werden. Die Kernherausforderung der vorliegenden Arbeit ist die Named Entity Recognition (NER). Wie bereits im Abschnitt \nameref{subsubsec:Transkribus} ausgeführt, werden viele Inhalte bereits während des Transkriptionsprozesses manuell auf Basis von im Anhang erläuterten Regeln getaggt\footnote{vgl. Anhang \nameref{subsec:Taggingregeln_Anhang}}. Dieser Prozess ist zwar sehr genau, überschreitet aber auch den begrenzten zeitlichen Rahmen aufgrund der Menge an Unterlagen. Daher wird für das Projekt auch eine zweite Verarbeitungsform gewählt, eine \textbf{NER durch ein LLM}.

Im Zentrum des Vorverarbeitungsskripts steht eine strukturierte Anbindung an die OpenAI-API, um ausgewählte 
PAGE-XML-Dateien aus dem Transkribus-Export automatisiert mit Annotationen anzureichern. Das Skript ist modular
aufgebaut und folgt einer klar definierten Abfolge von Verarbeitungsschritten, die im Folgenden näher erläutert werden.

Die Vorverarbeitung beginnt mit der Funktion \code{get\_api\_client()}, die eine Verbindung zur OpenAI-Programmierschnittstelle 
aufbaut. Dabei wird der API-Schlüssel über eine Umgebungsvariable geladen und für spätere Anfragen bereitgestellt.
Die zentrale Annotation erfolgt in der Funktion \code{annotate\_with\_llm()}, die den vollständigen Unicode-Text der XML-Datei
verarbeitet und an das Modell GPT-4o übergibt. Grundlage ist ein präzise formulierter Prompt \footnote{vgl. Anhang \nameref{subsec:LLM-Promt}}, der die Struktur des zu erwartenden
Outputs definiert. Der Prompt spezifiziert, dass ausschliesslich \code{<TextLine>}-Elemente bearbeitet und mit einem \code{custom}-Attribut
versehen werden dürfen. Innerhalb dieses Attributs werden ausschliesslich tatsächlich erkannte Entitäten in standardisiertem Format codiert, 
darunter Personen (\code{person}), Rollen (\code{role}), Orte (\code{place}), Organisationen (\code{organization}), Daten (\code{date}) 
sowie autoren- und empfängerbezogene Markierungen (\code{author}, \code{recipient}, \code{creation\_place}). Für jedes einzelne Tag gibt es genaue Anweisungen
an das Modell. Auch ein Beispielresultat der Annotation wird jedes Mal mitgeliefert, um möglichst wenig Varianz in den Antworten zu erhalten. Gleichzeitig liefert
das Beispielresultat auch Informationen über Abkürzungen, die nicht von dem Modell als Organisationen erkannt werden. Das \enquote{WhW - das Winterhilfswerk} ist eine
Abkürzung, die offenbar nicht in den Trainingsdaten des Modells vorkommt. Dieses Vorgehen stellt den Versuch dar, ein nicht domänenspezifisch trainiertes Modell 
auf eine historische Quellenlage zu adaptieren. Es ist jedoch davon auszugehen, dass ein speziell auf den historischen Korpus abgestimmtes Sprachmodell 
deutlich präzisere Ergebnisse liefern würde.

Die Antwort des Sprachmodels ist eine vollständige XML-Datei, die sämtliche bestehenden Strukturinformationen beibehalten soll. 
Der Rückgabewert wird zunächst in der Funktion \code{clean\_llm\_output()} auf mögliche Formatierungen überprüft. Diese Funktion 
extrahiert den tatsächlichen XML-Inhalt aus dem Rückgabestring, etwa wenn dieser durch Markdown-Wrapper 
wie \code{\textasciigrave\textasciigrave\textasciigrave xml-content\textquotesingle\textquotesingle\textquotesingle} eingefasst wurde.

Die konkrete Verarbeitung einzelner Dateien erfolgt über die Funktion \code{process\_file()}, die eine originale Transkribus-XML-Datei einliest,
an das Modell übergibt, das Ergebnis prüft und anschliessend unter verändertem Dateinamen (Suffix \code{\_preprocessed}) abspeichert. Vor dem 
Schreiben erfolgt eine strukturelle Validierung mittels XML-Parser, um syntaktische Fehler oder unvollständige Rückgaben zu 
erkennen. Fehlermeldungen werden protokolliert, fehlerhafte Dateien übersprungen.

Die eigentliche Ausführung der Batch-Verarbeitung wird durch die \code{main()}-Funktion gesteuert. Diese durchläuft das in 
\code{TRANSKRIBUS\_DIR} konfigurierte Arbeitsverzeichnis, wobei alle Unterordner der Form \code{<7-stelliger Ordner>/Akte\_<Nummer>/page/} 
rekursiv analysiert werden. XML-Dateien, die bereits mit \code{\_preprocessed} enden, werden übersprungen. Zusätzlich wird ein Verzeichnis 
ignoriert, wenn bereits mehr als fünfzig Prozent der Dateien annotiert wurden. Die verbleibenden Dateien werden schrittweise mit dem Modell 
verarbeitet. Hintergrund ist eine kosteneffiziente Verarbeitung, die verhindern soll, dass Dateien mehrfach durch die API bearbeitet werden. Die Funktion 
\code{annotate\_with\_llm()} berechnet darüber hinaus die Anzahl der vom Modell verarbeiteten Eingabe- und Ausgabetokens. Auf Basis dieser Werte wird für 
jede Datei eine Schätzung der anfallenden API-Kosten vorgenommen. Diese Informationen werden für jede Anfrage protokolliert, um eine transparente 
Kostenkontrolle sicherzustellen.

Am Ende dieses Prozesses entsteht pro annotierter Seite eine neue, syntaktisch validierte XML-Datei, 
die alle Annotationen als \code{custom}-Attribute enthält. Diese dienen in den nachfolgenden Modulen der 
Pipeline (insbesondere \code{transkribus\_to\_base.py}) als Grundlage für die strukturierte Extraktion und Validierung von Entitäten.


\subsection{Hauptmodul -- Transkribus\_to\_base}\label{sec:transkribus_to_base}
\subsubsection*{1. Initialisierung und Pfadlogik}

Die Initialisierungsphase des Skripts\footnote{bis ca. Line 250} dient der Einrichtung sämtlicher Systempfade, Datenquellen, 
Modulabhängigkeiten und Ressourcen. Im Zentrum steht dabei die Festlegung der Projektstruktur sowie die dynamische Integration aller untergeordneten Module und Datenbestände.

Zu Beginn werden die Python-Standardbibliotheken sowie externe Abhängigkeiten geladen, darunter \code{pandas} für 
Tabellenverarbeitung, \code{spacy} für die linguistische Analyse und \code{rapidfuzz} für den Vergleich von ähnlichen Strings. Zusätzlich 
wird das aktuelle Datum mit Zeitstempel generiert und als Konsolenoutput ausgegeben. Das unterstützt die Nachvollziehbarkeit von
Verarbeitungsläufen und das Debugging in grösseren Verarbeitungsszenarien.

Das Verzeichnis der Skriptdatei wird mithilfe des \code{pathlib}-Moduls identifiziert. Anschliessend wird von dort ausgehend das 
Projektwurzelverzeichnis nachvollzogen. Diese dynamisch bestimmte Wurzel fungiert als zentraler Referenzpunkt für alle nachfolgenden Verzeichnispfade, absolute Pfadangaben werden vollständig vermieden.  
Das Projekt ist so organisiert, dass sämtliche zentralen Datenverzeichnisse, Ressourcen und Modulstrukturen relativ
zur Wurzel definiert und im Code programmatisch zugänglich gemacht sind. Dazu gehören beispielsweise die 
Unterverzeichnisse \code{Data} für CSV-basierte Groundtruth-Informationen, \code{Module} für modulare 
Verarbeitungsfunktionen sowie \code{Transkribus\_test\_In} und \code{Transkribus\_test\_Out} als standardisierte 
Ein- und Ausgabeverzeichnisse für XML-Dateien und strukturierte JSON-Outputs.

Zur Gewährleistung der Modulverfügbarkeit wird dem globalen Importpfad das Verzeichnis \code{Module} über \code{sys.path.insert} hinzugefügt. 
Dadurch lassen sich alle enthaltenen Funktions- und Klassendefinitionen zentral importieren. Die einzelnen Komponenten sind in modularisierten Unterdateien wie \nameref{subsec:person_matcher.py}, \nameref{subsec:place_matcher_chapter} und \nameref{subsec:document_schema} abgelegt. 
Diese Struktur fördert die klare Trennung thematischer Verantwortlichkeiten innerhalb des Codes.

Darüber hinaus erfolgt die Definition aller relevanten Ein- und Ausgabepfade. Der 
Pfad \code{TRANSKRIBUS\_DIR} verweist auf das Eingabeverzeichnis für XML-Dateien, während 
\code{OUTPUT\_DIR} die Ausgabe der produzierten Ergebnisse in JSON-Dateien 
sowie untergeordnete Strukturen für ungematchte Entitäten, Logdateien und CSV-Dumps aufnimmt.

Die Initialisierungslogik umfasst ferner das standardisierte Einlesen der Groundtruth-Dateien aus den 
jeweiligen Nodegoat-Exporten, die zentral im Ordner \code{/Data/Nodegoat\_Export} gespeichert sind. 
Dabei werden die dort hinterlegten Informationen aus den 
CSV-Dateien geladen, in \code{pandas.DataFrames} überführt und durch Fehlerbehandlung in Form von
\code{try- \& exept}-Blöcken abgesichert. 
Eine Konsolenausgabe informiert über Anzahl und Status der geladenen Einträge.

Ob das deutsche Sprachmodell 
\code{de\_core\_news\_sm} von \code{spaCy} verfügbar ist, wird geprüft und gegebenenfalls auf ein Fehlen hingewiesen. 
Essentiell ist die Abfrage eines gültigen 
API-Schlüssels aus der Umgebungsvariable für das OpenAI-Modul. Ist dies nicht der Fall, wird der am Ende folgende Enrichment-Prozess automatisch deaktiviert.

Die standardisierte Datenstruktur für Personen wird unmittelbar nach dem Einlesen der 
Groundtruth-Dateien erzeugt. Hierzu werden die relevanten Felder aus der CSV-Datei 
\code{export-person.csv} extrahiert und als strukturierte Python-Dictionaries gespeichert. 
Diese beinhalten Attribute wie \code{forename}, \code{familyname}, \code{alternate\_name}, 
\code{title} sowie die eindeutige \code{nodegoat\_id}. Die Struktur orientiert sich an der in 
\code{document\_schemas.py} definierten Klasse \code{Person}, bildet jedoch in dieser Phase noch 
keine Instanzen davon.\footnote{Dies ist ein Relikt des dynamisch entstandenen Codes, das aus Zeitgründen noch nicht aufgelöst werden konnte} Sie dient als Referenz für alle nachfolgenden Matching- und 
Deduplikationsvorgänge.

Abschliessend stehen zwei Hilfsfunktionen bereit, um neu erkannte Personen in die bestehende 
CSV-Struktur zu überführen. Auf Basis des \enquote{fuzzy stringmatchings} mit der sogenannten Levensthein-Distanz\cite[\textbf{Levensthein-Distanz:} Die minimale Anzahl von elementaren Editieroperationen (Einfügen, Löschen, Ersetzen), die notwendig sind, um ein Wort a in ein anderes Wort b zu überführen. vgl:][]{levenshtein_binary_nodate} wird eine Duplikatsprüfung 
durch \code{Rapidfuzz} realisiert und stellt sicher, dass nur bisher unbekannte Einträge ergänzt werden.

Die Initialisierungs- und Pfadlogik legt damit den Grundstein für eine skalierbare, 
reproduzierbare und systematisch strukturierte Weiterverarbeitung aller Eingabedaten.


\subsubsection*{2. Extraktion von Struktur und Fliesstext}

Die Verarbeitung einer einzelnen Transkribus-Seite beginnt mit der systematischen Analyse der zugrundeliegenden XML-Datei. 
Ziel dieses Abschnitts ist die strukturierte Extraktion zentraler Informationen, die als Grundlage für die weitere Anreicherung 
und Validierung dienen. Dabei wird zwischen technischen Metadaten, transkribiertem Fliesstext und semantisch annotierten Entitäten 
(in sogenannten \code{custom}-Tags) unterschieden. Die nachfolgend beschriebenen Funktionen sind für die Extraktion dieser drei 
Bereiche zuständig.

\paragraph{extract\_metadata\_from\_xml~(~)} Diese Funktion extrahiert maschinenlesbare Metadaten aus dem 
\code{<TranskribusMetadata>}-Block der XML-Datei. Dazu gehören die eindeutige Dokumenten-ID (\code{docId}), die Seiten-ID 
(\code{pageId}), die Transkriptions-ID (\code{tsid}), sowie die zugehörige Bildreferenz und die XML-Quelldatei. Diese Informationen 
werden zentral in einem Metadaten-Dictionary gespeichert, das später in das Attributfeld des JSON-Objekts übernommen wird. 
Sie bilden die technische Grundlage für die spätere Identifikation und Reproduzierbarkeit jeder Seite.

\paragraph{get\_document\_type~(~)} ist in \nameref{subsection:type_matcher} genau erklärt. Es liefert Informationen, ob ein Dokument beispielsweise ein Brief, eine Postkarte oder Protokoll ist.

\paragraph{extract\_text\_from\_xml~(~)} Der transkribierte Fliesstext wird aus den Elementen \code{<TextEquiv>} bzw. 
\code{<Unicode>} extrahiert. Dabei werden alle Textinhalte zeilenweise aus den \code{TextLine}-Knoten der XML-Datei gelesen und zu 
einem zusammenhängenden String zusammengefügt. Jede Zeile wird durch einen Zeilenumbruch (\code{\textbackslash n}) getrennt, um den 
späteren kontextuellen Zugriff auf Zeilenpositionen (z.B. bei der Erwähnung von Personen oder Rollen) zu ermöglichen. Der extrahierte 
Text dient als Grundlage für regelbasierte Erkennungsverfahren sowie für LLM-gestützte Analysen in späteren Verarbeitungsschritten.

\paragraph{extract\_custom\_attributes~(~)} Die semantische Annotation in den Transkribus-Dokumenten erfolgt über \code{custom}-
Attribute, die bestimmten Textbereichen zugeordnet sind (z.B. \code{person\{offset:42; length:14\}}). Die Funktion 
\code{extract\_custom\_attributes()} iteriert über alle \code{TextLine}-Elemente und prüft, ob ein solches Attribut vorhanden ist. 
Ist dies der Fall, wird der betroffene Textabschnitt anhand der Offset-Angaben extrahiert und weiterverarbeitet. Die Extraktion 
erfolgt kategoriebezogen, wobei für jede Entitätstyp eigene Subfunktionen aufgerufen werden:

\begin{itemize}
    \item \textbf{extract\_person\_from\_custom~(~)}: Diese Funktion extrahiert Personennamen, gegebenenfalls Titel (z.B. \enquote{Herr}, \enquote{Frau}) und mögliche Rollenbezeichnungen aus den XML-Tags. Sie bereitet die Einträge für ein späteres Matching gegen die bekannte Personenliste vor. Hier werden Vor- und Nachnamen aus den Tags abgeleitet. Unvollständige Angaben erhalten das Attribut \code{needs\_review=true}, wenn kein eindeutiger Abgleich möglich ist. Die Extraktion erfolgt entweder direkt über im Tag hinterlegte Informationen (\code{firstname}, \code{lastname}) oder – falls diese fehlen – anhand der im Attribut gespeicherten \code{offset}- und \code{length}-Werte im zugehörigen Textabschnitt. Zusätzlich werden bei Bedarf das Geschlecht sowie ein möglicher Titel heuristisch aus dem Textkontext abgeleitet. Die Funktion gibt eine Liste von Dictionaries zurück, die jeweils eine potenziell relevante Person beschreiben. Es handelt sich hierbei um einen vorbereitenden Zwischenschritt; die Daten sind noch nicht dedupliziert oder strukturell validiert.
  
  \item \textbf{extract\_organization\_from\_custom~(~)}: Diese Funktion erkennt Organisationen (z.B. \enquote{Männerchor Murg}) in den XML-Anmerkungen. Sie produziert die Variable \code{raw-string}, die in späteren Verarbeitungsschritten mit bekannten Organisationen abgeglichen wird. 

  \item \textbf{extract\_place\_from\_custom~(~)}: Analog zur Personenerkennung werden Ortsnamen anhand der Offset-Daten aus dem Text extrahiert. Die Funktion übergibt die Einträge an eine vorbereitete \code{PlaceMatcher}-Instanz, die auf Basis von Groundtruth-Daten sowie ggf. externer APIs (GeoNames, Wikidata) einen Abgleich durchführt. Unsichere Orte werden ebenfalls mit \code{needs\_review} versehen.
  
  \item \textbf{extract\_date\_from\_custom~(~)}: Die Funktion erkennt Datumsangaben (z.B. \enquote{1. Januar 1943}) und normalisiert sie bei Bedarf. In einem späteren Schritt werden identische Daten zusammengeführt und gezählt (\code{combine\_dates()}), um Mehrfachnennungen zu erfassen.
\end{itemize}

Alle extrahierten Custom-Tags werden in einem strukturierten Dictionary gesammelt, das unter anderem folgende Schlüssel enthält: \code{"persons"}, \code{"roles"}, \code{"places"}, \code{"organizations"}, \code{"dates"}. Diese Struktur dient als zentrale Datenquelle für die nachfolgende Entitätenanreicherung, Deduplikation und Netzwerkanalyse.



Zweck: Parsen der XML-Datei und strukturierte Extraktion von Text, Metadaten und Custom-Tags.
\begin{itemize}
\item \texttt{extract\_metadata\_from\_xml~(~)}
\item \texttt{get\_document\_type~(~)}
\item \texttt{extract\_text\_from\_xml~(~)}
\item \texttt{extract\_custom\_attributes~(~)} mit:
  \begin{itemize}
  \item \textbf{extract\_and\_prepare\_persons~(~)}: Diese Funktion bildet das zentrale Anreicherungsmodul für alle zuvor extrahierten Personeninformationen. Sie übernimmt die von \code{extract\_person\_from\_custom()} zurückgegebenen Rohdaten (\code{raw\_persons}) und führt eine mehrstufige Weiterverarbeitung durch. Zunächst werden mit Hilfe von \code{split\_and\_enrich\_persons()} die Namen tokenisiert, normalisiert und durch Fuzzy-Matching mit den bekannten Personen aus der Groundtruth abgeglichen. In einem zweiten Schritt ergänzt die Funktion \code{assign\_roles\_to\_known\_persons()} passende Rolleninformationen aus dem Fließtextkontext. Anschliessend werden diese Informationen zusammengeführt, Dubletten entfernt und gleichnamige Personen gruppiert. Die bereinigte Personenliste wird dann auf potenzielle Autor:innen und Empfänger:innen hin analysiert. Dabei erfolgt die Zuweisung auf Basis textueller Heuristiken und Rollenkennzeichnungen. Das Ergebnis dieser Funktion ist eine dreiteilige Rückgabe: \code{mentioned\_persons}, \code{authors} und \code{recipients} – jeweils als strukturierte Liste von validierten Personenobjekten. Diese bilden die Grundlage für die finale JSON-Ausgabe.

  \item \texttt{extract\_place\_from\_custom~(~)}
  \item \texttt{extract\_organization\_from\_custom~(~)}
  \item \texttt{extract\_date\_from\_custom~(~)}
  \end{itemize}
\end{itemize}

\subsubsection*{3. Named Entity Recognition}
Zweck: Erkennung, Anreicherung und Zuordnung von Personen, Rollen, Orten, Organisationen und Ereignissen.
\begin{itemize}
\item \texttt{mentioned\_places\_from\_custom\_data~(~)}
\item \texttt{extract\_and\_prepare\_persons~(~)}
\item \texttt{assign\_roles\_to\_known\_persons~(~)}
\item \texttt{match\_organization\_entities~(~)}
\item \texttt{extract\_events\_from\_xml~(~)}
\item \texttt{combine\_dates~(~)}
\item \texttt{assign\_sender\_and\_recipient\_place~(~)}
\end{itemize}


\subsubsection*{4. Deduplikation und Validierung}
Zweck: Zusammenführung mehrfach erkannter Entitäten und finale Konsistenzprüfung.
\begin{itemize}
\item \texttt{deduplicate\_and\_group\_persons~(~)}
\item \texttt{ensure\_author\_recipient\_in\_mentions~(~)}
\item \texttt{count\_mentions\_in\_transcript\_contextual~(~)}
\item \texttt{postprocess\_roles~(~)}
\item \texttt{mark\_unmatched\_persons~(~)}
\item \texttt{validate\_extended~(~)}
\end{itemize}

\subsubsection*{5. JSON-Export und Logging}
Zweck: Erstellung der finalen JSON-Dateien im gewünschten Basisschema und Protokollierung von problematischen Einträgen.
\begin{itemize}
\item Erstellung von \texttt{BaseDocument~(~)}
\item \texttt{doc.to\_json~(~)}
\item \texttt{update\_total\_json~(~)}
\item \texttt{log\_unmatched\_entities~(~)}
\item Terminalausgabe bei Validierungsfehlern
\end{itemize}

JSon Export weil menschenlesbar und leichte Abwandelbarkeit in andere Formate.

\subsubsection*{6. Review-Prozess}
Zweck: Markierung und Protokollierung unsicherer, unvollständiger oder nicht eindeutig gematchter Entitäten für eine spätere manuelle Überprüfung.

\begin{itemize}
\item \texttt{mark\_unmatched\_persons~(~)} – Kennzeichnung von Personen ohne ID, mit niedrigem Score, unklarem Namen
\item \texttt{needs\_review = true} bei allen problematischen Einträgen
\item \texttt{review\_reason} zur Beschreibung der Ursache (z.B. „nur Vorname“, „nicht in Groundtruth“)
\item \texttt{log\_unmatched\_entities~(~)} – Protokollierung in den Dateien:
  \begin{itemize}
  \item \texttt{unmatched\_persons.json}
  \item \texttt{unmatched\_places.json}
  \item \texttt{unmatched\_roles.json}
  \item \texttt{unmatched\_events.json}
  \end{itemize}
\item Kontextbasierte Filterung durch Zeilenumfeld (z.B. keine Dopplung bei Rolle+Name in direkter Nachbarschaft)
\end{itemize}


\vspace{1em}\textbf{DAS HIER IST EIN ALTER ABSCHNITT, DER FÜR DIE OBEN ZU ERGÄNZENDEN KAPITEL VERWENDET WERDEN SOLL}\\
------------------------------------\\
Die wesentliche Verarbeitung der durch ChatGPT verarbeiteten XML-Files für jede einzelne Seite wird im 
Hauptmodul \code{Transkribus\_to\_base.py} gesteuert. Es ist das umfangreichste
Modul für dieses Projekt, dessen Funktionsweise im Folgenden beschrieben werden soll.

Nach Abschluss der Vorverarbeitung und der Anreicherung mit Annotationen werden die XML-Dateien mithilfe des 
Moduls \texttt{transkribus\_to\_base.py} in eine strukturierte JSON-Repräsentation überführt. Diese stellt das 
im Projekt definierte Basisschema dar und dient als Grundlage für die nachfolgenden Analyseschritte. Ziel ist es, 
aus dem strukturierten und annotierten XML-Dokument ein validiertes JSON-Objekt zu erzeugen, das alle im Dokument 
erkannten Entitäten eindeutig, formal konsistent und datenmodellkonform beschreibt.

Das Modul \code{transkribus\_to\_base.py} ist als zentraler Verarbeitungsknoten konzipiert. Es verarbeitet die Inhalte 
der zuvor erzeugten XML-Dateien schrittweise, prüft und transformiert sie und strukturiert sie in einer einheitlichen Objektklasse 
(\code{BaseDocument}). Die Verarbeitung beginnt mit dem Einlesen der XML-Datei. In einem ersten Schritt werden aus dem XML-Header 
Informationen wie \code{docId}, \code{pageId}, \code{tsid} sowie Referenzen zu Bild- und Quelldateien extrahiert. Diese Metadaten 
bilden die Basis für die eindeutige Identifikation jeder Seite. Zusätzlich wird aus dem Dateinamen das Dokumentformat (z.B. Brief, 
Postkarte, Protokoll) abgeleitet. Dieses wird später im Feld \code{document\_type} gespeichert und dient der Klassifikation innerhalb 
der Datenstruktur.

Parallel dazu wird der vollständige Transkriptionstext aus den \code{<TextEquiv>} bzw. \code{<Unicode>}-Blöcken extrahiert. Dieser 
Text bildet die Grundlage für alle heuristischen, regelbasierten und modellgestützten Erkennungsverfahren. Die im vorherigen Schritt von 
ChatGPT annotierten \code{custom}-Attribute werden nun systematisch ausgelesen und auf ihre Struktur analysiert. Dabei werden Personen, 
Rollen, Orte, Organisationen und Daten extrahiert. Jede dieser Kategorien wird durch eine eigene Funktionsgruppe behandelt, die intern auf 
vorab geladene Groundtruth-Daten zurückgreift. Die Groundtruth-Dateien stammen aus \textit{Nodegoat} und werden projektweit als 
CSV-Dateien verwaltet.

Die Extraktion von Personen erfolgt über die Funktion \code{extract\_person\_from\_custom()}, die für jeden in der XML-Datei 
annotierten \code{person}-Tag eine initiale Zerlegung vornimmt. In einem mehrstufigen Matchingverfahren wird versucht, die 
extrahierten Namen mit bekannten Personen zu verknüpfen. Dabei kommen Fuzzy-Matching-Techniken zum Einsatz, die über die Funktion 
\code{match\_person()} gesteuert werden. Zusätzlich werden Titel wie \enquote{Herr}, \enquote{Frau}, \enquote{Sängerbruder} oder 
\enquote{Witwe} als Geschlechtsindikatoren erkannt und gespeichert. Für jede identifizierte Person wird ein Eintrag erzeugt, der sowohl 
die extrahierten als auch die gematchten Informationen enthält. Bei fehlender Übereinstimmung wird der Eintrag mit dem Vermerk 
\code{needs\_review} gekennzeichnet.

Die Ortsverarbeitung basiert auf einem spezialisierten \code{PlaceMatcher}-Objekt, das die extrahierten Ortsnamen mit bekannten 
Ortsbezeichnungen aus der Groundtruth sowie mit externen Ressourcen wie Geonames oder Wikidata abgleicht. Bei unklaren oder 
mehrdeutigen Ortsangaben kann der Matcher mehrere Kandidaten zurückgeben. In diesem Fall erfolgt eine Gewichtung anhand von 
Konfidenz- und Ähnlichkeitswerten. Die Funktion \code{extract\_place\_from\_custom()} ist dabei für die Initialextraktion 
zuständig, während die Funktion \code{deduplicate\_places()} eine Zusammenführung ähnlicher Ortsangaben durchführt.

Zusätzlich zu den durch das Sprachmodell erzeugten Custom-Tags werden weitere Entitäten heuristisch aus dem Fliesstext erkannt. 
Besonders betrifft dies Rollenbezeichnungen, die in unmittelbarer Nähe zu Personennamen vorkommen. Eine regelbasierte Extraktion 
dieser Kontexte wird durch die Funktion \code{assign\_roles\_to\_known\_persons()} realisiert. Auch hier wird das Ergebnis validiert 
und – sofern die Rolle einer standardisierten Ontologie entspricht – in das Feld \code{role\_schema} überführt.

Die Kombination der verschiedenen Erkennungsmethoden kann zu Duplikaten führen. Um konsistente und eindeutige Entitäten zu erzeugen, 
erfolgt ein deduplizierender Abgleich über die Funktion \code{deduplicate\_and\_group\_persons()}. Diese vergleicht alle Personen 
aus den Kategorien \code{authors}, \code{recipients} und \code{mentioned\_persons} untereinander. Dabei werden vorhandene Scores 
(wie \code{match\_score}, \code{recipient\_score} und \code{confidence}) zusammengeführt und priorisiert.

Das so angereicherte Dokument wird in ein Objekt der Klasse \code{BaseDocument} überführt. Dieses enthält strukturierte Felder für 
Metadaten, Volltext, Autoren, Empfänger, erwähnte Personen, Orte, Organisationen, Datumsangaben und Ereignisse. Jede Entität wird gemäss 
den Typdefinitionen in \code{document\_schemas.py} validiert. Ein abschliessender Validierungsschritt erfolgt über 
\code{validate\_extended()}, das auf Fehler in der Struktur, Inkonsistenzen oder fehlende Pflichtfelder prüft.

Abschliessend wird das Dokument im JSON-Format gespeichert. Neben der Einzelseite wird auch eine aggregierte Datei 
\code{total\_json.json} fortgeschrieben, in der alle Seiten einer Akte gesammelt werden. Zusätzlich wird für jede Seite 
eine Prüfung auf nicht zuordenbare Entitäten durchgeführt. Diese werden in der Datei \code{unmatched.json} gespeichert, 
um eine spätere manuelle Nachbearbeitung zu ermöglichen. Das Ergebnis dieser Konvertierung bildet die Grundlage für die weitere 
Verarbeitung in \code{Nodegoat} sowie für die explorative Analyse der Akteursnetzwerke.

------------------------------------


\subsection{Module im Detail}\label{subsec:Module_im_Detail}
\subsubsection{document\_schemas.py}\label{subsec:document_schema}
Das Modul \code{document\_schemas.py} definiert die zentrale Schema- und Datenstruktur 
zur Modellierung aller extrahierten Inhalte aus den Transkribus-Dokumenten des Projekts. Es gewährleistet die einheitliche Repräsentation, 
Serialisierung und Validierung der im Projekt verarbeiteten Entitäten und ihrer Relationen. Die definierten Klassen bilden die Grundlage für die JSON-Ausgabe der angereicherten Dokumente und dienen zugleich der Nachvollziehbarkeit und strukturierten Weiterverarbeitung in externen Anwendungen wie Nodegoat.

\paragraph{Person}\\
Die Klasse \code{Person} bildet Einzelpersonen ab, wie sie in den Briefen, Postkarten oder Protokollen erscheinen. 
Neben klassischen Namensfeldern (\code{forename}, \code{familyname}, \code{title}) unterstützt die Klasse auch 
alternative Namen (\code{alternate\_name}) und Rolleninhalte. Die Rollenverarbeitung erfolgt in einem zweistufigen Verfahren:
Zum einen wird die Freitextrolle (\code{role}) als Originaleintrag gespeichert, zum anderen erfolgt eine normierte 
Zuordnung über \code{role\_schema}, basierend auf dem internen Mapping in \code{Assigned\_Roles\_Module.py}. 

Titel wie \code{"Herr"} oder \code{"Frau"} dienen in \code{person\_matcher.py} zugleich als Grundlage zur 
Ableitung des Geschlechts der Person. Das Feld \code{gender} erlaubt in \code{document\_schemas.py} eine 
geschlechtsspezifische Zuordnung, basierend auf Titelbezeichnungen oder Groundtruth-Matching. 

Ergänzt wird die Person um Kontexte wie \code{associated\_place} und \code{associated\_organisation}, sowie um Bewertungsparameter 
wie \code{match\_score}, \code{recipient\_score}, \code{confidence} und \code{mentioned\_count}.\footnote{Detailliert wird auf diese Logiken in 
\nameref{subsec:person_matcher.py} eingegangen, sie sollen hier nur umrissen werden.} 
Letzterer gibt an, wie oft 
die Person im Transkript erwähnt wurde, unter Berücksichtigung einer kontextbasierten Zähllogik.
\code{match\_score} beschreibt einen Wert der Wahrscheinlichkeit, dass es sich bei der im Dokument beschriebenen Person auch 
um die Person handelt, die schlussendlich gematcht wurde. Vor- und Nachnamen erhöhen beispielsweise den Score, während das
Fehlen solch einzelner Informationen den Wert senken. 

Das Feld \code{confidence} beschreibt die Modell-Sicherheit der Zuordnung, z.B.beim LLM-basierten Matching. 
\code{recipient\_score} bewertet die Wahrscheinlichkeit, dass es sich bei der Person um den tatsächlichen Empfänger des Dokuments handelt. 

Ein optionales \code{needs\_review}-Flag sowie ein \code{review\_reason} ermöglichen die gezielte Markierung unsicherer oder 
maschinell nur teilweise auflösbarer Fälle.

\paragraph{Organization}\\
Die Klasse \code{Organization} dient der Abbildung aller im Text genannten Vereine, Gruppen oder Institutionen, 
darunter z.B. Gesangsvereine, NS-Organisationen. Neben dem Namen, Typ und eventuellen 
Alternativnamen (\code{alternate\_names}) wird insbesondere die eindeutige \code{nodegoat\_id} gespeichert. Für den Spezialfall 
militärischer Einheiten enthält die Klasse ein separates Feld \code{feldpostnummer}. Da sie in der Pipeline jedoch nicht seperat getaggt und verarbeitet sind,
werden diese aktuell ausschliesslich über die Groundtruth abgerufen. Über \code{match\_score} und \code{confidence} 
werden auch in der Klasse \code{Organization} Qualität und Unsicherheiten der Zuordnung nachvollziehbar gemacht. 
Organisationen können zusätzlich über das Feld \code{associated\_place} mit einem geographischen Ort verknüpft werden, z.B. \enquote{Männerchor Murg} mit \enquote{Murg}.

\paragraph{Place}\\
Die Klasse \code{Place} strukturiert geographische Orte, wie sie z.B. als Absender-, Empfänger- oder 
Veranstaltungsorte im Dokument auftreten. Unterstützt werden neben der Hauptbezeichnung (\code{name}) 
auch alternative Formen (\code{alternate\_place\_name}), sowie standardisierte Identifikatoren aus Geonames, 
Wikidata und Nodegoat. Diese Orte sind über eigene Felder sowohl im Metadatenblock (\code{creation\_place}, 
\code{recipient\_place}) als auch in der Liste \code{mentioned\_places} verortet, falls erstere nicht genau zugeordnet werden können. 
Wie bei Personen, kann auch bei Orten 
über das Flag \code{needs\_review} eine manuelle Überprüfung unsicherer Matches angestossen werden.

\paragraph{Event}\\
Ereignisse werden über die Klasse \code{Event} abgebildet. Diese enthält einen Namen, eine optionale Beschreibung, 
Datumsangaben und Referenzen auf beteiligte Personen, Orte und Organisationen. Über das Feld \code{inferred} wird angegeben, 
ob das Ereignis direkt im Text genannt oder aus dem Kontext erschlossen wurde. Die genaue Event-Extraktion findet sich 
in \nameref{subsection:event_matcher}.

\paragraph{BaseDocument}\\
Alle genannten Entitäten werden im zentralen Objekttyp \code{BaseDocument} zusammengeführt. Diese Klasse bildet 
die Grundlage für die strukturierte Speicherung jedes einzelnen Dokuments und enthält unter anderem:
\begin{itemize}
\item einen Attributblock mit \code{document\_type}, \code{object\_type}, \code{creation\_date}, \code{creation\_place}, \code{recipient\_place}
\item Listen der \code{authors}, \code{recipients} und \code{mentioned\_persons}
\item Listen von \code{mentioned\_organizations}, \code{mentioned\_places}, \code{mentioned\_events} und \code{mentioned\_dates}
\item die Originaltranskription (\code{content\_transcription}) sowie inhaltliche Tagging-Kategorien (\code{content\_tags\_in\_german})
\item optionale Zusatzinformationen im Feld \code{custom\_data}, z.B.Zwischenoutputs oder Debug-Daten
\end{itemize}

Zur automatisierten Konvertierung vom oder ins JSON-Format stehen die Methoden \code{to\_dict()}, \code{from\_dict()}, \code{to\_json()} und \code{from\_json()} zur Verfügung. Darüber hinaus erlaubt die Methode \code{validate()} eine strukturierte Prüfung der Einträge auf Konsistenz und Vollständigkeit, etwa im Hinblick auf unvollständige Daten oder ungültige Formate.

\paragraph{Documenttype}\\
Für die wichtigsten Dokumenttypen existieren spezialisierte Unterklassen wie \code{Brief}, 
\code{Postkarte} oder \code{Protokoll}, die zusätzliche Felder (z.B.\code{greeting}, \code{postmark}, \code{meeting\_type}) aufnehmen. 
Diese können über die zentrale sogenannte \enquote{factory function} \code{create\_document()} automatisch erzeugt werden, wenn ein Dokumenttyp erkannt wurde. 
Diese spezialisierten Erkennungstypen stehen aktuell lediglich für eine spätere Verwendung bereit, und kommen in der aktuellen Anwendung nur in der Benennung des jeweiligen Typs zum Einsatz.

Alle Ergebnisse werden anschliessend in strukturierter Form in eine JSON-Datei geschrieben, wobei jeder Eintrag dem oben definierten Schema 
entspricht. 
\subsubsection{\_\_init\_\_.py}\label{subsec:init_module}
Das Modul \code{\_\_init\_\_.py} fungiert als zentrale Importschnittstelle für alle Funktionalitäten der Projektpipeline. Es aggregiert sämtliche zentralen Komponenten aus den verschiedenen Teilmodulen (z.B. \code{person\_matcher.py}, \code{document\_schemas.py}, \code{place\_matcher.py}) und stellt sie über das \code{\_\_all\_\_}-Array einheitlich zur Verfügung. Dadurch wird eine übersichtliche, modulübergreifende Nutzung der wichtigsten Funktionen und Klassen in anderen Programmteilen (z.B. in \code{transkribus\_to\_base.py}) ermöglicht.
Auf diese Weise kann das Hauptprogramm \code{transkribus\_to\_base.py} auf alle notwendigen Komponenten mit einem einzigen Importbefehl zugreifen, ohne die internen Modulpfade kennen zu müssen. Die Datei übernimmt damit die Funktion einer projektinternen API und gewährleistet eine saubere Trennung zwischen interner Modulstruktur und externer Nutzung. 

\newpage
\begin{minipage}[t]{0.35\textwidth}
   \setstretch{1.5}
   \justifying%

  \subsubsection{Person\_matcher.py}\label{subsec:person_matcher.py}

  Hier steht eine menge schöner Text

  \end{minipage}%
%================================
%Person PROZESSDIAGRAMM
%================================
  \hfill
\begin{minipage}[t]{0.6\textwidth}
  \vspace*{0.7cm} %► schiebt alles um 0.7cm nach unten
\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}


%TikZ styles
\tikzset{
  module/.style={rectangle, draw=black, fill=blue!10, minimum width=0.4cm, minimum height=0.2cm,align=center},
  source/.style={rectangle, draw=black, fill=green!20, minimum width=0.4cm, minimum height=0.2cm,align=center},
  highlight/.style={rectangle, draw=black, fill=blue!40, minimum width=0.4cm, minimum height=0.2cm,align=center},
  arrow/.style={-stealth, line width=0.15mm},
  arrowboth/.style={stealth-stealth, line width=0.15mm},
  process/.style={rectangle, draw=black, fill=orange!20, thick, minimum width=6cm, minimum height=1cm, align=center},
  large/.style={rectangle, draw=black, fill=blue!10, thick, minimum width=4.5cm, minimum height=1.2cm, align=center}
};
%==========================
% Mini-Diagramm oben rechts
%==========================

\begin{scope}[shift={(10.1,0.45)}, node distance=0.3cm]

\node[module] (init) at (0,0) {};
\node[module] (xml) at (0, -0.5) {};
\node[module]    (events)  at (-2.0, -1.3) {};
\node[module]    (roles)   at (-1.2, -1.3) {};
\node[highlight] (persons) at (-0.4, -1.3) {};
\node[module]    (orgs)    at ( 0.4, -1.3) {};
\node[module]    (places)  at ( 1.2, -1.3) {};
\node[module]    (dates)   at ( 2.0, -1.3) {};
\node[module] (authors)    at (-0.4, -2  ) {};

\node[coordinate] (joinpoint) at ($(xml.south) + (0, -1.8cm)$) {};
\node[coordinate] (joinpointxml) at ($(xml.south) + (0, -0.3cm)$) {};

\node[module, below=of joinpoint] (unmatched) {};
\node[module, below=of unmatched] (json) {};
\node[source, below=of json] (save) {};
\node[source, right=0.5cm of save] (unmatchedjson) {};
\draw[arrow, dashed] (persons.south) -- (authors.north);
\draw[arrow] (unmatched.east) -| (unmatchedjson.north);

\draw[arrow] (json.south) -- (save.north);
\draw[arrow] (init.south) -- (xml.north);
\draw[arrowboth] (roles.east) -- (persons.west);
\draw[arrowboth] (persons.east) -- (orgs.west);
\draw[arrowboth] (orgs.east) -- (places.west);
%Pfeile oben
\draw[arrow] (xml.south) -- (joinpointxml.south);
\draw[arrow] (joinpointxml) -- ($(roles.north |- joinpointxml)$) -- (roles.north);
\draw[arrow] (joinpointxml) -- ($(persons.north |- joinpointxml)$) -- (persons.north);
\draw[arrow] (joinpointxml) -- ($(orgs.north |- joinpointxml)$) -- (orgs.north);
\draw[arrow] (joinpointxml) -- ($(places.north |- joinpointxml)$) -- (places.north);
\draw[arrow] (joinpointxml) -- ($(dates.north |- joinpointxml)$) -- (dates.north);
\draw[arrow] (joinpointxml) -- ($(events.north |- joinpointxml)$) -- (events.north);

%Pfeile unten Doppelte Pfeile mit definiertem Stil
\draw[arrow] (events.south) -- ($(events.south |- joinpoint)$);
\draw[arrow] ($(events.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (roles.south) -- ($(roles.south |- joinpoint)$);
\draw[arrow] ($(roles.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (authors.south) -- ($(authors.south |- joinpoint)$);
\draw[arrow] ($(authors.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (orgs.south) -- ($(orgs.south |- joinpoint)$);
\draw[arrow] ($(orgs.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (places.south) -- ($(places.south |- joinpoint)$);
\draw[arrow] ($(places.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (dates.south) -- ($(dates.south |- joinpoint)$);
\draw[arrow] ($(dates.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (joinpoint.south) -- (unmatched.north);
\draw[arrow] (unmatched.south) -- (json.north);
\draw[arrow] (json.south) -- (save.north);
\end{scope}

%==========================
% Haupt-Diagramm
%==========================

\begin{scope}

\node[large] (personsmod) at (0,0) {\textbf{Personen} \\ match, split, enrich};

\node[process, below=1.2cm of personsmod] (split)  {\textbf{def split\_and\_enrich\_persons} \\ \textit{(Extrahiere, verbinde Titel, Kontext-Rolle, Match)}};

\node[process, below=2.5cm of joinpoint] (LoadPers) {\textbf{load\_known\_persons\_from\_csv} \\ Lädt CSV, erzeugt KNOWN\_PERSONS};

\node[source, below=of LoadPers] (GTPers)  {\textbf{export-person.csv} \\ \textit{(Groundtruth CSV)}};

\node[process, below=of split] (parse)  {\textbf{def extract\_person\_data} \\ (Zerlege Namen, erkenne Titel \& Rollen)};

\node[process, below=of parse] (swapped)  {\textbf{def correct\_swapped\_name} \\ (Korrigiere Vor-/Nachname vertauscht)};

\node[process, below=of swapped] (exrawrole)  {\textbf{def extract\_role\_from\_raw\_name} \\ (Trenne Rolle aus Namensstring)};

\node[process, below=of exrawrole] (match)  {\textbf{def match\_person} \\ (Vergleiche mit Groundtruth, OCR/Fuzzy)};

\node[process, below=of match] (inferrole)  {\textbf{def infer\_role\_and\_organisation} \\ (Zuweisung Rolle + Organisation aus Kontext)};

\node[large, left=2.5cm of inferrole] (orgmatch) {\textbf{Organization\_Matcher.py} \\ (Normalisiert Organisation)};

\node[large, above= of orgmatch] (rolematch) {\textbf{Assigned\_Roles\_Module.py} \\ (Normalisiert Rollen)};

\node[process, below=of inferrole] (score)  {\textit{recipient\_score-Berechnung} \\ (intern: Score-Kontextlogik)};

\node[process, below=of score] (dedup)  {\textbf{def deduplicate\_and\_group\_persons} \\ (Gruppiere, merge Scores, Rollen)};

\node[process, below=of dedup] (count)  {\textbf{def count\_mentions\_in\_transcript\_contextual} \\ (Kontext-Zählung der Nennungen)};

% ================
% Mini-Helfer seitlich
% ================
\node[process, left=2.3cm of split] (merge) {\textbf{def merge\_title\_tokens} \\ (Titel + Name zusammenführen)};
\node[process, left=3.1cm of parse] (norm) {\textbf{def normalize\_name\_string} \\ (Namen säubern, Titel trennen)};
\node[process, right=3.8cm of match] (ocr) {\textbf{def ocr\_error\_match} \\ (OCR-Heuristik für Matches)};
\node[process, right=3.5cm of inferrole] (fuzzy) {\textbf{def fuzzy\_match\_name} \\ (Rapidfuzz-Suche)};
\draw[arrow] (GTPers) -- (LoadPers);
%\draw[arrow] (LoadPers) -- (split.east);
\draw[arrow] (LoadPers.west) -- ++(-1.1,0) |- (split.east);



% ================
% Hauptpfeile
% ================
\draw[arrow] (personsmod.south) -- (split.north);
\draw[arrow] (split.south) -- (parse.north);
\draw[arrow] (parse.south) -- (swapped.north);
\draw[arrow] (swapped.south) -- (exrawrole.north);
\draw[arrow] (exrawrole.south) -- (match.north);
\draw[arrow] (match.south) -- (inferrole.north);

\draw[arrow] (exrawrole.west) -| (rolematch.north);
\draw[arrow] (inferrole.west) -- (orgmatch.east);

\draw[arrow] (inferrole.south) -- (score.north);
\draw[arrow] (score.south) -- (dedup.north);
\draw[arrow] (dedup.south) -- (count.north);

\draw[arrow] (rolematch.east) -- ++(1.5,0) |- (inferrole.west);

% ================
% Helferpfeile
% ================
\draw[arrow] (merge.east) -- (split.west);
\draw[arrow] (norm.east) -- (parse.west);
\draw[arrow] (ocr.west) -- (match.east);
\draw[arrow] (match.east) -- ++(1.0,0) |- (fuzzy.west);
%\draw[arrow] (fuzzy.west) -- (match.east);

% ================
% Gruppenbox
% ================
\node[draw=gray, dashed, rounded corners, inner sep=0.5cm, fit=(split)(count)] (group) {};
\end{scope}
\end{tikzpicture}
  }
  \captionof{figure}{\\\small Oben links: \small Prozessdiagramm für \code{Personen\_matcher.py},\\{\small Oben rechts: Pipelineübersicht}}\label{fig:person_matcher.py}
\end{minipage}










\newpage
\begin{minipage}[t]{0.38\textwidth}
  \setstretch{1.5}
  \justifying%

  \subsubsection{Assigned\_Roles\_Module.py}\label{subsection:assigned_roles_module}

  Hier steht eine menge schöner Text

  \end{minipage}%
% ================================
% Roles PROZESSDIAGRAMM
% ================================
  \hfill
\begin{minipage}[t]{0.55\textwidth}
  \vspace*{0.7cm} % ► schiebt alles um 0.7cm nach unten
\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}
% TikZ styles
\tikzset{
  module/.style={rectangle, draw=black, fill=blue!10, minimum width=0.4cm, minimum height=0.2cm},
  source/.style={rectangle, draw=black, fill=green!20, minimum width=0.4cm, minimum height=0.2cm,align=center},
  highlight/.style={rectangle, draw=black, fill=blue!40, minimum width=0.4cm, minimum height=0.2cm},
  arrow/.style={-stealth, line width=0.15mm},
  arrowboth/.style={stealth-stealth, line width=0.15mm},
  process/.style={rectangle, draw=black, fill=orange!20, thick, minimum width=6cm, minimum height=1cm, align=center},
  large/.style={rectangle, draw=black, fill=blue!10, thick, minimum width=4.5cm, minimum height=1.2cm, align=center}
};
%==========================
% Mini-Diagramm oben rechts
%==========================

\begin{scope}[shift={(9,0.45)}, node distance=0.3cm]
\node[module] (init) at (0,0) {};
\node[module] (xml) at (0, -0.5) {};
\node[module]    (events)  at (-2.0, -1.3) {};
\node[highlight]    (roles)   at (-1.2, -1.3) {};
\node[module] (persons) at (-0.4, -1.3) {};
\node[module]    (orgs)    at ( 0.4, -1.3) {};
\node[module]    (places)  at ( 1.2, -1.3) {};
\node[module]    (dates)   at ( 2.0, -1.3) {};
\node[module] (authors)    at (-0.4, -2  ) {};

\node[coordinate] (joinpoint) at ($(xml.south) + (0, -1.8cm)$) {};
\node[coordinate] (joinpointxml) at ($(xml.south) + (0, -0.3cm)$) {};

\node[module, below=of joinpoint] (unmatched) {};
\node[module, below=of unmatched] (json) {};
\node[source, below=of json] (save) {};
\node[source, right=0.5cm of save] (unmatchedjson) {};
\draw[arrow, dashed] (persons.south) -- (authors.north);
\draw[arrow] (unmatched.east) -| (unmatchedjson.north);

\draw[arrow] (json.south) -- (save.north);
\draw[arrow] (init.south) -- (xml.north);
\draw[arrowboth] (roles.east) -- (persons.west);
\draw[arrowboth] (persons.east) -- (orgs.west);
\draw[arrowboth] (orgs.east) -- (places.west);
%Pfeile oben
\draw[arrow] (xml.south) -- (joinpointxml.south);
\draw[arrow] (joinpointxml) -- ($(roles.north |- joinpointxml)$) -- (roles.north);
\draw[arrow] (joinpointxml) -- ($(persons.north |- joinpointxml)$) -- (persons.north);
\draw[arrow] (joinpointxml) -- ($(orgs.north |- joinpointxml)$) -- (orgs.north);
\draw[arrow] (joinpointxml) -- ($(places.north |- joinpointxml)$) -- (places.north);
\draw[arrow] (joinpointxml) -- ($(dates.north |- joinpointxml)$) -- (dates.north);
\draw[arrow] (joinpointxml) -- ($(events.north |- joinpointxml)$) -- (events.north);

%Pfeile unten Doppelte Pfeile mit definiertem Stil
\draw[arrow] (events.south) -- ($(events.south |- joinpoint)$);
\draw[arrow] ($(events.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (roles.south) -- ($(roles.south |- joinpoint)$);
\draw[arrow] ($(roles.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (authors.south) -- ($(authors.south |- joinpoint)$);
\draw[arrow] ($(authors.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (orgs.south) -- ($(orgs.south |- joinpoint)$);
\draw[arrow] ($(orgs.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (places.south) -- ($(places.south |- joinpoint)$);
\draw[arrow] ($(places.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (dates.south) -- ($(dates.south |- joinpoint)$);
\draw[arrow] ($(dates.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (joinpoint.south) -- (unmatched.north);
\draw[arrow] (unmatched.south) -- (json.north);
\draw[arrow] (json.south) -- (save.north);
\end{scope}


%==========================
% Hauptprogramm Assigned_roles_module
%==========================
\begin{scope}
\node[large] (rolesmod) at (0,0) {\textbf{Assigned\_Roles\_Module.py} \\ match, split, enrich};

\node[process, below=1.2cm of rolesmod] (normalize)  {\textbf{def normalize\_and\_match\_role} \\ (Normalisiert Rollen mit Role\_Mappings\_DE)};

\node[source, below= 3.6cm of joinpointxml] (ROLE)  {\textbf{Role\_Mappings\_DE} \\ (interne Lookup-Tabelle)};

\node[process, below=of normalize] (loadknown)  {\textbf{def load\_known\_persons} \\ (Lädt bekannte Personen aus GT-CSV)};

\node[source, below= of ROLE] (PERSONGT)  {\textbf{export-person.csv} \\ (Groundtruth Personen)};

\node[process, below=of loadknown] (maprole)  {\textbf{def map\_role\_to\_schema\_entry} \\ (Wandelt normierte Rolle in Dict um)};

\node[process, below=of maprole] (assign)  {\textbf{def assign\_roles\_to\_known\_persons} \\ (Weist Rollen zu bekannten Personen zu)};

\node[process, below=of assign] (findindx)  {\textbf{def find\_line\_index\_for\_person} \\ (Findet Person im Text)};

\node[process, below=of findindx] (search)  {\textbf{def search\_roles\_nearby} \\ (Sucht Rollen im Umkreis)};

\node[source, right=3.5cm of search] (ROLEGTSOURCE)  {\textbf{export-roles.csv} \\ (Groundtruth Rollen)};

\node[process, below=of search] (standalone)  {\textbf{def extract\_standalone\_roles} \\ (Findet neue Personen+Rollen-Kombis)};

\node[process, below=of standalone] (intoken)  {\textbf{def extract\_role\_in\_token} \\ (Token-Analyse Rolle+Name)};

% === Pfeile ===
\draw[arrow] (rolesmod.south) -- (normalize.north);
\draw[arrow] (normalize.south) -- (loadknown.north);
\draw[arrow] (loadknown.south) -- (maprole.north);
\draw[arrow] (maprole.south) -- (assign.north);
\draw[arrow] (assign.south) -- (findindx.north);
\draw[arrow] (findindx.south) -- (search.north);
\draw[arrow] (search.south) -- (standalone.north);
\draw[arrow] (standalone.south) -- (intoken.north);

% === Ressource-Pfeile ===
\draw[arrow] (ROLE.west) -- ++(-1.4,0) |- (normalize.east);
\draw[arrow] (PERSONGT.west) -- ++(-1.5,0) |-  (loadknown.east);
\draw[arrow] (ROLEGTSOURCE.west) -- (search.east);

% === Umrandung ===
\node[draw=gray, dashed, rounded corners, inner sep=0.5cm, fit=(normalize)(intoken)] (group) {};
\end{scope}

\end{tikzpicture}
}
\captionof{figure}{\\\small Links: \small Prozessdiagramm für \code{Assigned\_Roles\_Module.py},\\{\small Rechts: Pipelineübersicht}}\label{fig:assigned_roles_module}
\end{minipage}





\newpage
\begin{minipage}[t]{0.38\textwidth}
  \setstretch{1.5}
  \justifying%

  \subsubsection{place\_matcher.py}\label{subsec:place_matcher_chapter}

  Hier steht eine menge schöner Text

  \end{minipage}%
% ================================
% PlaceMatcher PROZESSDIAGRAMM
% ================================
  \hfill
\begin{minipage}[t]{0.55\textwidth}
  \vspace*{0.7cm} % ► schiebt alles um 0.7cm nach unten
\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}


% =============================
% TikZ Styles
% =============================
\tikzset{
  module/.style={rectangle, draw=black, fill=blue!10, minimum width=0.4cm, minimum height=0.2cm},
  source/.style={rectangle, draw=black, fill=green!20, minimum width=0.4cm, minimum height=0.2cm, align=center},
  highlight/.style={rectangle, draw=black, fill=blue!40, minimum width=0.4cm, minimum height=0.2cm},
  process/.style={rectangle, draw=black, fill=orange!20, thick, minimum width=6cm, minimum height=1cm, align=center},
  large/.style={rectangle, draw=black, fill=blue!10, thick, minimum width=4.5cm, minimum height=1.2cm, align=center},
  arrow/.style={-stealth, line width=0.15mm},
  arrowboth/.style={stealth-stealth, line width=0.15mm}
};
%==========================
% Mini-Diagramm Placematcher
%==========================

\begin{scope}[shift={(9,0.45)}, node distance=0.3cm]

\node[module] (init) at (0,0) {};
\node[module] (xml) at (0, -0.5) {};
\node[module]    (events)  at (-2.0, -1.3) {};
\node[module]    (roles)   at (-1.2, -1.3) {};
\node[module] (persons) at (-0.4, -1.3) {};
\node[module]    (orgs)    at ( 0.4, -1.3) {};
\node[module]    (places)  at ( 1.2, -1.3) {};
\node[highlight]    (dates)   at ( 2.0, -1.3) {};
\node[module] (authors)    at (-0.4, -2  ) {};

\node[coordinate] (joinpoint) at ($(xml.south) + (0, -1.8cm)$) {};
\node[coordinate] (joinpointxml) at ($(xml.south) + (0, -0.3cm)$) {};

\node[module, below=of joinpoint] (unmatched) {};
\node[module, below=of unmatched] (json) {};
\node[source, below=of json] (save) {};
\node[source, right=0.5cm of save] (unmatchedjson) {};
\draw[arrow, dashed] (persons.south) -- (authors.north);
\draw[arrow] (unmatched.east) -| (unmatchedjson.north);

\draw[arrow] (json.south) -- (save.north);
\draw[arrow] (init.south) -- (xml.north);
\draw[arrowboth] (roles.east) -- (persons.west);
\draw[arrowboth] (persons.east) -- (orgs.west);
\draw[arrowboth] (orgs.east) -- (places.west);
%Pfeile oben
\draw[arrow] (xml.south) -- (joinpointxml.south);
\draw[arrow] (joinpointxml) -- ($(roles.north |- joinpointxml)$) -- (roles.north);
\draw[arrow] (joinpointxml) -- ($(persons.north |- joinpointxml)$) -- (persons.north);
\draw[arrow] (joinpointxml) -- ($(orgs.north |- joinpointxml)$) -- (orgs.north);
\draw[arrow] (joinpointxml) -- ($(places.north |- joinpointxml)$) -- (places.north);
\draw[arrow] (joinpointxml) -- ($(dates.north |- joinpointxml)$) -- (dates.north);
\draw[arrow] (joinpointxml) -- ($(events.north |- joinpointxml)$) -- (events.north);

%Pfeile unten Doppelte Pfeile mit definiertem Stil
\draw[arrow] (events.south) -- ($(events.south |- joinpoint)$);
\draw[arrow] ($(events.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (roles.south) -- ($(roles.south |- joinpoint)$);
\draw[arrow] ($(roles.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (authors.south) -- ($(authors.south |- joinpoint)$);
\draw[arrow] ($(authors.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (orgs.south) -- ($(orgs.south |- joinpoint)$);
\draw[arrow] ($(orgs.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (places.south) -- ($(places.south |- joinpoint)$);
\draw[arrow] ($(places.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (dates.south) -- ($(dates.south |- joinpoint)$);
\draw[arrow] ($(dates.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (joinpoint.south) -- (unmatched.north);
\draw[arrow] (unmatched.south) -- (json.north);
\draw[arrow] (json.south) -- (save.north);
\end{scope}


% =============================
% Haupt-Block: PlaceMatcher
% =============================
\begin{scope}
\node[large] (placematcher) at (0,0) {\textbf{PlaceMatcher.py} \\ load, match, deduplicate};

% CSV links von init
\node[source, below=2.5cm of joinpoint] (CSV) {\textbf{export-place.csv} \\ known places};

% Hauptfluss vertikal
\node[process, below=1.2cm of placematcher] (initp) {\textbf{\_\_init\_\_ + sanitize\_id} \\ (Lädt CSV, alt\_name\_map, known\_name\_map)};

\node[process, below=of initp] (normalize) {\textbf{\_normalize\_place\_name} \\ (Normiert Ortsnamen)};

\node[process, below=of normalize] (match) {\textbf{match\_place} \\ (Exact, Fuzzy, Kontext, API)};

\node[process, below=of match] (buildres) {\textbf{\_build\_match\_result} \\ (APIs ergänzen, Wikidata enrich)};

\node[process, below=of buildres] (dedup) {\textbf{deduplicate\_places} \\ (Gruppiert nach ID)};

\node[process, below=of dedup] (pipeline) {\textbf{enrich\_and\_deduplicate} \\ (Pipeline Wrapper)};

\node[process, below=of pipeline] (output) {\textbf{mentioned\_places\_from\_custom\_data} \\ (Erzeugt Place-Objekte)};

% Abzweige rechts
\node[process, left=1.5cm of match] (context) {\textbf{\_generate\_combined\_place\_names} \\ (Kontext-Kombis)};

\node[source, below=4.8cm of joinpoint] (geoapi) {\textbf{Geonames API}};
\node[source, below=0.8cm of geoapi] (wikiapi) {\textbf{Wikidata SPARQL}};

\node[process, left=1.5cm of buildres] (log) {\textbf{log\_unmatched\_place} \\ (Speichert unmatched)};

% Verbindungen Hauptfluss
\draw[arrow] (placematcher.south) -- (initp.north);
\draw[arrow] (initp.south) -- (normalize.north);
\draw[arrow] (normalize.south) -- (match.north);
\draw[arrow] (match.south) -- (buildres.north);
\draw[arrow] (buildres.south) -- (dedup.north);
\draw[arrow] (dedup.south) -- (pipeline.north);
\draw[arrow] (pipeline.south) -- (output.north);

% CSV-Input
\draw[arrow] 
  (CSV.west)
  -- ++(-1,0)       % 1 cm rechts
  |- (initp.east); % dann runter/hoch bis initp.west

% Kontext-Pfeil
\draw[arrowboth] (match.west) -- (context.east);

% APIs

\draw[arrow]
  % 1) Verschobener Start um 0,3 cm nach oben
  ($(buildres.east)+(0cm,0.3cm)$)
  % 2) von dort 1 cm nach rechts
  -- ++(1cm,0)
  % 3) und dann im L-Bogen zur GeoAPI
  |- (geoapi.west);

% Pfeil zu Wiki
\coordinate (pivot) at ($(buildres.east)+(0cm,-0.3)$);
\draw[arrow] 
  (wikiapi.south)    % wieder am buildres
  -- ++(0,0)       % dieselbe Hilfsstrecke
  |- (pivot);

\draw[arrow] (geoapi.south) -- (wikiapi.north);


% Logging
\draw[arrow] (buildres.west) -- ++(0,0) |- (log.east);

% Optional: normalize nutzt context auch
% \draw[arrow, dashed] (normalize.east) -- ++(0.5,0) |- (context.west);

% Umrandung
\node[draw=gray, dashed, rounded corners, inner sep=0.5cm, fit=(initp)(output)(log)(context)] {};
\end{scope}

\end{tikzpicture}
}
\captionof{figure}{\\\small Links: \small Prozessdiagramm für \code{place\_matcher.py},\\{\small Rechts: Pipelineübersicht}}\label{fig:placematcher_final}
\end{minipage}


\begin{minipage}[t]{0.38\textwidth}
  \setstretch{1.5}
  \justifying%

\subsubsection{organization\_matcher.py}\label{subsection:organization_matcher}

  \code{organization\_matcher.py} dient der Erkennung und Normalisierung von Organisationen in historischen Transkripten. Es kombiniert reguläre Ausdrücke mit unscharfem Vergleich (fuzzy matching), um eingegebene Strings mit bekannten Organisationen aus der Groundtruth-Datei \texttt{export-organisationen.csv} abzugleichen.

  \noindent \code{extract\_organization(\{\})} bereinigt im nächsten Schritt die Eingabestrings zunächst, indem 
\end{minipage}%
\hfill
\begin{minipage}[t]{0.55\textwidth}
  \vspace*{0.7cm} % ► schiebt alles um 0.7cm nach unten
\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}
% =============================
% TikZ Styles
% =============================
\tikzset{
  module/.style={rectangle, draw=black, fill=blue!10, minimum width=0.4cm, minimum height=0.2cm},
  source/.style={rectangle, draw=black, fill=green!20, minimum width=0.4cm, minimum height=0.2cm, align=center},
  highlight/.style={rectangle, draw=black, fill=blue!40, minimum width=0.4cm, minimum height=0.2cm},
  process/.style={rectangle, draw=black, fill=orange!20, thick, minimum width=6cm, minimum height=1cm, align=center},
  large/.style={rectangle, draw=black, fill=blue!10, thick, minimum width=4.5cm, minimum height=1.2cm, align=center},
  arrow/.style={-stealth, line width=0.15mm},
  arrowboth/.style={stealth-stealth, line width=0.15mm}
};

% =============================
% Mini-Diagramm oben rechts
% =============================
\begin{scope}[shift={(9,0.45)}, node distance=0.3cm]
\node[module] (init) at (0,0) {};
\node[module] (xml) at (0, -0.5) {};
\node[module] (events)  at (-2.0, -1.3) {};
\node[module] (roles)   at (-1.2, -1.3) {};
\node[module] (persons) at (-0.4, -1.3) {};
\node[highlight] (orgs) at ( 0.4, -1.3) {}; % Highlight für Org-Matcher
\node[module] (places)  at ( 1.2, -1.3) {};
\node[module] (dates)   at ( 2.0, -1.3) {};
\node[module] (authors) at (-0.4, -2  ) {};

\node[coordinate] (joinpoint) at ($(xml.south) + (0, -1.8cm)$) {};
\node[coordinate] (joinpointxml) at ($(xml.south) + (0, -0.3cm)$) {};

\node[module, below=of joinpoint] (unmatched) {};
\node[module, below=of unmatched] (json) {};
\node[source, below=of json] (save) {};
\node[source, right=0.5cm of save] (unmatchedjson) {};

\draw[arrow, dashed] (persons.south) -- (authors.north);
\draw[arrow] (unmatched.east) -| (unmatchedjson.north);
\draw[arrow] (json.south) -- (save.north);
\draw[arrow] (init.south) -- (xml.north);
\draw[arrowboth] (roles.east) -- (persons.west);
\draw[arrowboth] (persons.east) -- (orgs.west);
\draw[arrowboth] (orgs.east) -- (places.west);
\draw[arrowboth] (places.east) -- (dates.west);

\draw[arrow] (xml.south) -- (joinpointxml.south);
\draw[arrow] (joinpointxml) -- ($(roles.north |- joinpointxml)$) -- (roles.north);
\draw[arrow] (joinpointxml) -- ($(persons.north |- joinpointxml)$) -- (persons.north);
\draw[arrow] (joinpointxml) -- ($(orgs.north |- joinpointxml)$) -- (orgs.north);
\draw[arrow] (joinpointxml) -- ($(places.north |- joinpointxml)$) -- (places.north);
\draw[arrow] (joinpointxml) -- ($(dates.north |- joinpointxml)$) -- (dates.north);
\draw[arrow] (joinpointxml) -- ($(events.north |- joinpointxml)$) -- (events.north);

\draw[arrow] (events.south) -- ($(events.south |- joinpoint)$);
\draw[arrow] ($(events.south |- joinpoint)$) -- (joinpoint);
\draw[arrow] (roles.south) -- ($(roles.south |- joinpoint)$);
\draw[arrow] ($(roles.south |- joinpoint)$) -- (joinpoint);
\draw[arrow] (authors.south) -- ($(authors.south |- joinpoint)$);
\draw[arrow] ($(authors.south |- joinpoint)$) -- (joinpoint);
\draw[arrow] (orgs.south) -- ($(orgs.south |- joinpoint)$);
\draw[arrow] ($(orgs.south |- joinpoint)$) -- (joinpoint);
\draw[arrow] (places.south) -- ($(places.south |- joinpoint)$);
\draw[arrow] ($(places.south |- joinpoint)$) -- (joinpoint);
\draw[arrow] (dates.south) -- ($(dates.south |- joinpoint)$);
\draw[arrow] ($(dates.south |- joinpoint)$) -- (joinpoint);
\draw[arrow] (joinpoint.south) -- (unmatched.north);
\draw[arrow] (unmatched.south) -- (json.north);
\draw[arrow] (json.south) -- (save.north);
\end{scope}

% =============================
% Haupt-Block: Organization Matcher
% =============================

\begin{scope}
\node[large] (orgmatcher) at (0,0) {\textbf{Organization\_matcher.py}\\clean, match, deduplicate};

% Groundtruth CSV links
\node[source, below= 3.6cm of joinpointxml] (CSV) {\textbf{export-organisationen.csv}\\export-organisationen.csv};

% Load
\node[process, below=1.2cm of orgmatcher] (load) {\textbf{load\_organizations\_from\_csv} \\ (Lädt Orgs + Aliase)};

% Global List
\node[process, below=of load] (known) {\textbf{KNOWN\_ORGS} \\ (Globale Liste aller Orgs)};

% Cleaning
\node[process, below=of known] (extract) {\textbf{extract\_organization} \\ (Säubert Name, entfernt Müll)};

% Normalize
\node[process, below=of extract] (normalize) {\textbf{normalize\_org\_name} \\ (Normiert Name für Fuzzy)};

% Special Case
\node[process, left=1.5cm of normalize] (acro) {\textbf{match\_acronym} \\ (Akronym-Fall)};

% Main Matcher
\node[process, below=of normalize] (match) {\textbf{match\_organization} \\ (Exact + Fuzzy Vergleich)};

% Entities Matcher
\node[process, below=of match] (entities) {\textbf{match\_organization\_entities} \\ (Mehrere Tokens + Kombis)};

% Fulltext
\node[process, below=of entities] (fulltext) {\textbf{match\_organization\_from\_text} \\ (Ganzer Text auf Orgs prüfen)};

% Direct Lookup
\node[process, below=of fulltext] (byname) {\textbf{match\_organization\_by\_name} \\ (Direkter Einzelaufruf)};

% Wikidata
\node[process, left=1.5cm of extract] (wikidata) {\textbf{extract\_wikidata\_id} \\ (Wikidata-ID extrahieren)};

% === Pfeile ===
\draw[arrow] (orgmatcher.south) -- (load.north);
\draw[arrow] (load.south) -- (known.north);
\draw[arrow] (known.south) -- (extract.north);
\draw[arrow] (extract.south) -- (normalize.north);
\draw[arrow] (normalize.south) -- (match.north);
\draw[arrow] (match.south) -- (entities.north);
\draw[arrow] (entities.south) -- (fulltext.north);
\draw[arrow] (fulltext.south) -- (byname.north);

% CSV Verbindung
\draw[arrow]
  (load.east)       % Start am load-Knoten
  -- ++(1cm,0)      % 1 cm nach rechts (spiegelt das ursprüngliche -- ++(-1,0))
  |- ($(CSV.west)+(0cm,0.3cm)$);    % dann im L-Bogen zurück zum CSV-Knoten

\draw[arrow]
  % Start 0,3 cm unter CSV.west
  ($(CSV.west)+(0cm,-0.3cm)$)
  % dann gerade zur known.east
  -- (known.east);

% Branches
\draw[arrow] (normalize.west) -- (acro.east);
\draw[arrow] (extract.west) -- (wikidata.east);

% Acronym-Verbindung zu Main Matcher
\draw[arrow] (acro.south) |- (match.east);

% Umrandung
\node[draw=gray, dashed, rounded corners, inner sep=0.5cm, fit=(load)(byname)(wikidata)(acro)] {};
\end{scope}

\end{tikzpicture}
}
\captionof{figure}{
  \small
  Links: Prozessdiagramm für \texttt{Organization\_matcher.py},\\
  Rechts: Pipelineübersicht
}
\label{fig:organization_matcher}
\end{minipage}
\vspace{1em}
\justifying

sie umschliessende Klammern entfernt, innere Satzzeichen säubert und führende bzw. abschliessende Interpunktion streicht. Darüber hinaus wird eine Blacklist abgeglichen, um etwa Einträge wie \enquote{Verein} als alleinstehende Organisation auszuschliessen - ein klares Match kann hier algorithmisch vorerst nicht gewährleistet werden.

Die bekannten Organisationen werden über die Funktion \code{load\_organizations\_from\_csv()} eingelesen. Dabei entsteht eine strukturierte Liste von Dictionaries, die mit den Anforderungen aus \code{document\_schemas.py} kompatibel ist und Hauptnamen, alternative Bezeichnungen sowie Identifier enthält.

Um die Organisationen vergleichbar zu machen wird in \code{def normalize\_org\_name ( )} in drei schritten normalisiert.

Zunächst werden mithilfe einer RegEx\footnote{\textbf{Abk.:} Regular Expression} sämtliche Zeichen entfernt, die nicht alphanumerisch sind. Erlaubt bleiben dabei Gross- und Kleinbuchstaben (einschliesslich deutscher Umlaute), Ziffern sowie Leerzeichen. So wird beispielsweise aus dem Ausdruck \enquote{Männerchor Murg e.V.} der bereinigte String \enquote{Männerchor Murg eV}.

Im zweiten Schritt wird der bereinigte String durch \code{.lower()} vollständig in Kleinbuchstaben transformiert.
Abschliessend werden mit \code{.strip()} etwaige führende oder nachfolgende Leerzeichen entfernt, sodass der Rückgabewert für weitere Vergleichsoperationen normiert vorliegt. Verdeutlichend wird aus:
\begin{verbatim}
Input:  "  (Männerchor Murg e.V.)  "
Output: "männerchor murg ev"
\end{verbatim}
Um nicht nur vollständige Namen zu prüfen, sondern auch Akronyme\footnote{Abkürzungen}, wird gezielt nach solchen in den \texttt{alternate\_names} bekannter Organisationen gesucht.
 
Das Matching der Organisationen erfolgt im Anschluss in den drei zentralen Funktionen \code{match\_organization()}, \code{match\_organization\_from\_text()} und \code{match\_organization\_entities()}.

Die Funktion \code{match\_organization()} bildet dabei den Kern des Vergleichsprozesses. Sie prüft zunächst, ob es sich bei der Eingabe um ein kurzes Akronym handelt und nutzt hierfür die Funktion \code{match\_acronym()}. Falls kein solcher Sonderfall vorliegt, wird der Eingabewert mithilfe von Fuzzy Matching mit \code{fuzz.ratio} gegen alle bekannten Organisationsnamen sowie deren alternative Bezeichnungen geprüft. Das Ergebnis ist das bestbewertete Match mit einem Score oberhalb eines standardmässig auf vordefinierten Schwellenwerts .

Die Funktion \code{match\_organization\_from\_text()} erweitert diesen Vergleich auf vollständige Transkriptionstexte. Dabei wird zunächst überprüft, ob ein bekannter Organisationsname direkt im Text vorkommt. Falls dies nicht der Fall ist, wird ein fuzzy-Teilvergleich durchgeführt (\code{fuzz.partial\_ratio}), um auch unvollständige oder abweichende Schreibweisen zu erkennen.

\code{match\_organization\_entities()} verarbeitet eine Liste roher Organisationseinträge aus der Extraktion und bereinigt sie mit \code{extract\_organization()}. Zudem werden aufeinanderfolgende Namen wie \enquote{Männerchor} und \enquote{Murg} zu einem kombinierten Eintrag zusammengeführt. Anschliessend wird für jeden dieser bereinigten Namen ein fuzzy-Matching gegen die Groundtruth durchgeführt, wobei Treffer mit Score und \code{confidence = "fuzzy"} zurückgegeben werden.

\subsubsection{letter\_metadata\_matcher.py}\label{subsection:letter_metadata_matcher}

Der letter\_metadata\_matcher widmet sich auf der inhaltlichen Metadaten Briefen und Postkarten. Da diese Dokumententypen sehr ähnlich strukturiert sind, liegt der Fokus dieses Moduls auf der Extraktion ihrer Daten. Vornehmlich hier Informationen über die Autorenschaft, den Empfängern und Empfängerinnen sowie deren Rollen. Darüber hinaus sollen auch Daten und Orte nachvollzogen werden, mit einem Fokus auf den Absende- und Empfangsort. 

\paragraph{Pattern Matching in Briefen und Postkarten}
Die zentrale Annahme ist, dass diese Metadaten meist im Textkörper selbst enthalten sind und anhand wiederkehrender sprachlicher Muster bzw. Indikatoren erkannt werden können. Ein Autor oder eine Autorin beendet einen Brief beispielsweise in der Regel mit einer Grussformel. Die Pipeline macht sich das zunutze, indem diese Patterns als Marker in einem String dienen, und zusätzlich zu den Custom-Tags im XML File auf eine Autorenschaft hinweisen können. Diese Marker sind in fünf Listen auf RegEx-Basis notiert.
\begin{itemize}
    \item \code{GREETING\_PATTERNS} als Schlussformel in einem Brief zeigt die Autorenschaft an. \\Beispiel: \texttt{mit freundlichen Grüßen} oder \texttt{Heil Hitler}.
    \item \code{RECIPIENT\_PATTERNS} Erkennt direkte Anreden von Empfänger:innen im Text. \\Beispiel: \texttt{An Herrn/Frau} oder \texttt{Lieber Fritz}
    \item \code{direct\_patterns} zeigen die unmittelbaren Empfänger eines Briefes an.
    \item \code{INDIRECT\_RECIPIENT\_PATTERNS} zeigen an, dass ein Brief oder Postkarte an einen Zwischenempfänger geht. \\Beispiel: \texttt{zu Händen von}
    \item \code{SIGNATURE\_PATTERNS} ist eine auf das nachfolgende ROLE\_PATTERNS stützende RegEx, die reine Rollenzeilen am Ende eines Briefes erkennt. Beispiel:\\
    \texttt{der Vereinsführer\\Alfons Zimmermann}. \\ 
    \item \code{ROLE\_PATTERNS} ist eine dynamisch generierte RegEx, die auf der Rolle basierende Signaturen überprüft. Grundlage ist eine aus \code{export-roles.csv} geladene Liste bekannter Rollenbegriffe.
\end{itemize}

\paragraph{Extraktion von Autorinnen und Autoren}
Die konkrete Anwendung dieser Patterns erfolgt in den jeweiligen Funktionen, deren Grundlage eine Verarbeitung der raw strings ist. Die Funktion \code{extract\_authors\_raw} analysiert hierfür zu Beginn alle Textzeilen innerhalb des Transkription, ohne dabei auf strukturierte XML-Tags angewiesen zu sein. Stattdessen orientiert sich die Funktion an den oben erläuterten typischen linguistischen Mustern. Somit kann die Funktion auch dann Ergebnisse liefern, wenn strukturierte XML-Tags wie \code{<author>} fehlen oder unvollständig sind, was als Backup-Lösung die LLM-Annotation komplettiert. Ziel ist es, möglichst frühzeitig Hinweise auf Autor:innen zu erkennen und diese mit einem einfachen Scoring-System zu bewerten. Das erkannte Scoring wird in einem separaten Feld \code{author\_score} festgehalten und dient als Bewertungsgrundlage für die spätere Auswahl der wahrscheinlichsten Autor:innen.

Da Autoren in der Regel am Briefende unterschreiben, werden die Zeilen des Transkripts von unten nach oben analysiert. Dabei wird zunächst geprüft, ob eine typische Rollensignatur wie \texttt{Der Vereinsführer} alleine am Dokumentende steht. Wird eine solche erkannt, wird unmittelbar eine entsprechende Rolle extrahiert, ohne dass ein Name erforderlich ist.

Findet sich keine Rollenzeile, durchsucht die Funktion den Text nach einer Schlussformel basierend auf den in \code{GREETING\_PATTERNS} definierten Mustern. Der Textabschnitt, der auf diese Grussformel folgt, wird im nächsten Schritt nach Signaturen durchsucht. Die Auswertung erfolgt dabei in mehreren aufeinanderfolgenden Prüfungen.

Zunächst wird versucht, eine Kombination aus Rolle und Nachname zu identifizieren\footnote{z.B. \texttt{Der Vereinsführer Zimmermann}}.
Alternativ wird geprüft, ob Rolle und Name in zwei aufeinanderfolgenden Zeilen vorkommen.
Weitere Varianten umfassen typische Signaturformate wie \texttt{\enquote{Max Ganter, Schriftführer}} oder Namensinitialen wie \texttt{\endquote{M. Ganter}}.
Auch vollständige Namen (Vorname + Nachname) oder einzelne Namen (z.B. nur \texttt{\enquote{Fritz}}) werden erkannt und mit Platzhaltern übernommen.

Wird kein valides Format erkannt, liefert die Funktion ein leeres Dictionary mit einem Eintrag für \code{closing}, der die gefundene Grussformel dokumentiert.

Neben der heuristischen Pattern-Suche durch \code{extract\_authors\_raw()} stützt sich die Pipeline auch auf strukturierte Annotationen aus dem XML. Die Funktion \code{extract\_author\_from\_custom\_tag()} sucht gezielt nach \code{<author>}-Tags und übernimmt diese, sofern sie vollständig sind. Im Anschluss wird jeder Kandidateneintrag durch \code{resolve\_llm\_custom\_authors\_recipients()} validiert und bewertet. Insbesondere dann, wenn er von einem LLM vorgeschlagen oder aus unstrukturiertem Text extrahiert wurde kommt die Bewertung zum tragen. Abschliessend übernimmt \code{letter\_match\_and\_enrich()} die Anreicherung durch bekannte Personen aus der Groundtruth-Liste. Hierbei werden unvollständige Einträge (z.B. nur ein Vorname) durch vollständigere bereits in \code{mentioned\_persons} aufgenommene Personen ersetzt, sofern möglich. 

Die Pipeline deckt jedoch nicht nur einzelne Autoren ab. \code{match\_authors~(~)} kombiniert die Roh-Extraktion der Autor:in (\code{extract\_authors\_raw}) mit der Anreicherung durch \code{letter\_match\_and\_enrich}. Ist das Ergebnis leer, prüft sie, ob zumindest eine Rolle ohne Namensangabe vorliegt. In diesem Sonderfall wird ein Eintrag mit leerem Namen, aber übernommener Rolle erstellt. Die Rolle wird normalisiert (\code{normalize\_and\_match\_role}) und in ein \code{role\_schema} überführt. Der Rückgabewert enthält \code{needs\_review = True} sowie eine niedrige \code{match\_score} (10), um auf die Unsicherheit der reinen Rollenangabe hinzuweisen.


\paragraph{Extraktion von Empfängerinnen und Empfängern}

Die Extraktion potenzieller Empfänger:innen erfolgt analog zur Autorenerkennung über eine Kombination aus regulären Ausdrücken, strukturierten XML-Tags und nachgelagerter Anreicherung. Im Gegensatz zur Autorenschaft, die typischerweise am Ende eines Dokuments steht, werden Empfänger:innen meist im Briefkopf genannt. Die Funktion \code{extract\_recipients\_raw()} konzentriert sich daher auf die ersten fünf Zeilen des Transkripts. Dieser Abschnitt enthält häufig Adressierungen wie \texttt{\enquote{An Herrn Otto Bollinger}} oder zeilenweise strukturierte Formate, die auf eine Empfängeradresse hindeuten. Die Funktion arbeitet unabhängig von expliziten XML-Tags wie \code{<recipient>}, wodurch sie auch bei unvollständig annotierten Dokumenten Ergebnisse liefern kann.

Die Erkennung erfolgt in drei heuristischen Stufen, abhängig von der Länge und Struktur der jeweiligen Adressierung. Einzeilige Formulierungen wie \texttt{\enquote{An Herrn Otto Bollinger}} werden als besonders zuverlässig bewertet (\code{recipient\_score = 90}, \code{confidence = "header-inline"}). Dreizeilige Strukturen wie 
\\ \begin{center}
\texttt{An\\Herrn\\Otto Bollinger}
\end{center}
\justifying werden als semistrukturiert interpretiert und mit einem Score von 80 markiert (\code{confidence = "header-3line"}). Bei zweizeiligen Konstruktionen – etwa \texttt{Herrn} in einer Zeile und ein Name in der nächsten – wird aus methodischer Vorsicht ein niedrigerer Score (70) vergeben, da diese auch als Fliesstextnennung interpretiert werden könnten (\code{confidence = "header-2line"}). Diese Liste von Empfängerkandidaten wird anschliessend in mehreren Schritten angereichert und validiert.

In einem weiteren Schritt werden \code{<recipient>}-Tags aus dem XML durch \code{extract\_recipient\_from\_custom\_tag()} übernommen, sofern vorhanden. Wie bei den Authoren durchlaufen Rezipienten danach die Funktion \code{resolve\_llm\_custom\_authors\_recipients()}. So werden die Rohdaten zusammengeführt, und anschliessen eine Validierung vorgenommen. Insbesondere unvollständige oder generische Einträge wie \texttt{\enquote{Herrn}} oder häufig vorkommende Namen wie \texttt{\enquote{Otto}}\footnote{die Groundtruth kennt 11 verschiedene Otto} werden entweder mit dem Vermerk \code{needs\_review = true} versehen oder durch bereits \code{mentioned\_persons} ersetzt. Diese Ersetzung erfolgt abschliessend in \code{letter\_match\_and\_enrich()}, wo wie bei den Autor:innen unvollständige Einträge – etwa nur der Vorname – durch vollständigere Einträge aus \code{mentioned\_persons} ersetzt werden, sofern diese zuvor im Text erkannt wurden. Mit \code{extract\_multiple\_recipients\_raw()} können darüber hinaus potenzielle Empfänger:innen aus dem Transkripttext anhand direkter und indirekter Anredeformen aus den jeweiligen Patternlisten extrahiert werden. Direkte Formulierungen wie \texttt{Lieber Otto} werden mit \code{recipient\_score = 100} als besonders zuverlässig bewertet. Indirekte Anreden wie \texttt{zu Händen des Herrn Alfons Zimmermann} werden mit Score 70 erfasst. Die Ergebnisse enthalten Vorname, Nachname (falls vorhanden), Rolle (leer), Score und Confidence-Label. Durch die unterschiedlichen Ratings im Recipient-Scores können so Staffelungen dargestellt werden.


\paragraph{Extraktion des Absendeort und Empfangsort}

Die Extraktion des Absendeorts (\code{creation\_place}) und des Empfangsorts (\code{recipient\_place}) erfolgt in einem mehrstufigen Verfahren, das strukturierte XML-Informationen, heuristische Pattern-Erkennung und ein darauf aufbauendes Groundtruth-Matching kombiniert. Ziel ist es, auch in unvollständig annotierten oder uneinheitlich formulierten Briefen möglichst präzise Ortsinformationen zu identifizieren und anzureichern. Die Verarbeitung ist dabei modular aufgebaut und erfolgt durch vier aufeinander abgestimmte Funktionen.

Die Funktion \code{extract\_places\_and\_date()} bildet den Einstiegspunkt für die Ortserkennung. Sie durchsucht alle Zeilen im XML-Dokument (\code{<TextLine>}) nach Attributen vom Typ \texttt{custom}, in denen strukturierte Angaben zu \code{creation\_place}, \code{recipient\_place} und \code{creation\_date} hinterlegt sein können. Wird ein solches Attribut gefunden, wird der zugehörige Ort mittels RegEx aus dem Text extrahiert. Grundlage hierfür sind die LLM-ergänzten, im Transkribus-Export enthaltenen Offset- und Length-Parameter, die den Ortstext innerhalb der Zeile markieren. Die Funktion liefert drei \code{raw-strings} zurück: den vermuteten Absendeort (\code{raw\_creation\_place}), den Empfangsort (\code{raw\_recipient\_place}) sowie das Datum der Entstehung (\code{creation\_date})\footnote{dazu mehr im nächsten Abschnitt}. Sind keine Custom-Tags vorhanden, aktiviert die Funktion ein Fallback-Verfahren, das den Briefkopf anhand typischer Formate mit regulären Ausdrücken analysiert. Erkannt werden insbesondere Muster wie \texttt{Ort, den dd.mm.yyyy} oder \texttt{An ... in Ort}.

Die so ermittelten Raw-Strings werden in \code{assign\_sender\_and\_recipient\_place(...)} weiterverarbeitet. Diese Hauptfunktion ruft zunächst \code{extract\_places\_and\_date()} auf, um die Rohdaten zu extrahieren, und analysiert dann alle Zeilen des Transkripts, um auch Kontexte für zusammengesetzte Ortsbezeichnungen\footnote{z.B. Laufenburg-Rhina} zu erfassen. Anschliessend wird das Orts-Matching über die Hilfsfunktion \code{enrich\_place\_candidate()} durchgeführt. Diese Funktion gleicht den erkannten Komposit-Ortsnamen mit einer Groundtruth-Liste ab, in der alle bekannten Orte und ihre Varianten hinterlegt sind. Neben exakten Treffern erlaubt das Matching auch Fuzzy Matching über die Levenshtein-Distanz. Ergibt sich dabei ein Treffer, wird ein Score berechnet und mit weiteren Metadaten \footnote{z.B. \code{nodegoat\_id}} angereichert. Im Kontext von Empfängeradressen wird zusätzlich geprüft, ob im unmittelbaren Umfeld ein Vereins- oder Organisationsname vorkommt. In diesem Fall wird ein Score-Malus vergeben, da  das ein Hinweis auf eine mögliche Verwechslung zwischen Ort und Organisation sein kann.

Eine besondere Stärke der Pipeline liegt in der Erkennung zusammengesetzter Ortsnamen. Die Funktion \code{find\_combined\_place()} durchsucht benachbarte Zeilen nach Kombinationen wie \texttt{Laufenburg} und \texttt{Rhina}, die zusammen in einem Eintrag der Groundtruth-Liste vorkommen (\texttt{Laufenburg-Rhina}). Wird eine solche Kombination erkannt, wird sie bevorzugt, erhält einen Bonus im Matching-Score und ersetzt die Einzeltreffer. Weil so auch Teilorte oder Stadtteile abgebildet werden können, wird die Ortserkennung deutlich präziser.

Abschliessend wird durch \code{finalize\_recipient\_places()} der am besten bewertete Empfängerort ausgewählt. Die Funktion sortiert alle erkannten Ortskandidaten nach Score und übernimmt den besten Eintrag als \code{recipient\_place}. Weitere valide, aber weniger eindeutige Orte werden ebenfalls gespeichert, jedoch mit dem Attribut \code{needs\_review = true} gekennzeichnet, um eine manuelle Prüfung zu ermöglichen.

Die finale Ausgabe enthält für beide Ortsangaben strukturierte Einträge mit Namen, Score, falls vorhanden externe IDs aus der Groundtruth, Kontextinformationen und ggf. die Flag \code{needs\_review}. Durch das modulare Zusammenspiel der vier Funktionen können auch in schwierig strukturierten oder unvollständig annotierten Dokumenten präzise Ortsangaben extrahiert werden.

\paragraph{Extraktion des Erstellungsdatum}

Die Erkennung von Entstehungsdaten (\code{creation\_date}) erfolgt im Modul \code{letter\_metadata\_matcher.py} primär über die von oben bereits bekannte Funktion \code{extract\_places\_and\_date( )}. Im Zentrum steht die Auswertung strukturierter Custom-Tags im PAGE-XML sowie ein regulärer Fallback-Mechanismus zur heuristischen Analyse des Briefkopfs.

Im ersten Schritt durchsucht die Funktion alle \code{<TextLine>}-Elemente nach dem Attribut \code{custom}, das von Transkribus für semantische Annotationen verwendet wird. Wird ein Eintrag vom Typ \texttt{creation\_date} oder \texttt{date} erkannt, wird mittels RegEx nach einem \texttt{when}-Feld gesucht. Dieser Wert wird direkt übernommen und als \code{creation\_date} gespeichert. Da bei dem manuellen Tagging besonders auf die korrekten Daten geachtet, und diese händisch mit \code{when\=} immer ergänzt wurden, sind diese Angaben in jedem Fall vorhanden.

Wird jedoch kein Custom-Tag erkannt, greift ein Fallback-Verfahren: Die Funktion analysiert hierfür die ersten fünf Zeilen des Dokuments – typischerweise der Briefkopf – auf typische Orts-Datumskombinationen. Dazu zählt insbesondere das Muster

\begin{center}
\texttt{Ort, den dd.mm.yyyy}
\end{center}

\noindent Die RegEx erfasst dabei sowohl den Ort (als möglichen Absendeort) als auch das Datum. Sie sind daher auch gemeinsam in dieser Funktion aufgenommen. Wird ein solches Datum gefunden, wird es als \code{creation\_date} übernommen, selbst wenn kein zugehöriger Ort erkannt werden konnte. Die Erkennung erfolgt dabei tolerant gegenüber optionalem \enquote{den} und erlaubt sowohl zwei- als auch vierstellige Jahresangaben\footnote{erkannt werden also sowohl beispielsweise 1939, als auch nur 39}. Ist kein solcher Eintrag vorhanden, bleibt \code{creation\_date = None}.

In Kombination mit \code{assign\_sender\_and\_recipient\_place()} wird das erkannte Datum anschliessend gemeinsam mit den Ortseinträgen an die Hauptverarbeitung zurückgegeben und in den Metadaten des Dokuments gespeichert.


\subsubsection{type\_matcher.py}\label{subsection:type_matcher}
Dieses Modul besteht aus nur einer einzigen Funktion: \code{def get\_document\_type~(~)}. Diese zerlegt den Namen der zu verarbeitenden Datei und extrahiert daraus die siebenstellige Transkribus-ID und die Seitennummer. Es folgt ein Vergleich in der Akten\_Gesamtübersich.csv, die als Groundtruth alle Dateien im Korpus listet.

Da während der Transkription in dieser Tabelle auch die Dokumententypen festgehalten wurden, kann über einen einfachen Lookup für jede Seite der zugehörige Typ extrahiert werden. Mögliche Kategorien sind unter anderem \enquote{Brief}, \enquote{Postkarte}, \enquote{Protokoll}.\footnote{siehe Kapitel \nameref{Dokumententypen}} 

Falls kein passender Eintrag in der Groundtruth gefunden wird, bietet die Funktion ein optionales Fallback: Wird zusätzlich ein \code{xml\_path} übergeben, so wird versucht, den Dokumenttyp direkt aus einem \code{<Dokumententyp>}-Tag innerhalb der zugehörigen XML-Datei auszulesen. Auf diese Weise wird sichergestellt, dass jede Seite im JSON-Output einen Typ zugewiesen erhält – entweder verlässlich aus der CSV oder über das XML-Fallback.

Die extrahierte Typinformation wird im Attributblock des finalen JSON unter \code{attributes["document\_type"]} gespeichert. So kann später seitengenau nach Dokumententypen gefiltert und eine differenzierte Verteilung der Typen innerhalb einer Akte nachvollzogen werden.


\subsubsection*{event\_matcher.py}\label{subsection:event_matcher}

Das Modul \texttt{event\_matcher.py} dient der Erkennung, Strukturierung und Anreicherung von 
Ereignistags aus den Transkribus-Dokumenten. Die zugrundeliegende Funktion \texttt{extract\_events\_from\_xml()} 
verarbeitet die gesamte XML-Datei zeilenweise und extrahiert jene Textstellen, die als Ereignis markiert 
wurden oder im unmittelbaren Zusammenhang mit einem Ereignisblock stehen. Grundlage dafür ist die Auswertung 
des \texttt{custom}-Attributs einzelner \texttt{<TextLine>}-Elemente, insbesondere wenn dieses explizit mit 
\enquote{event} gekennzeichnet ist.

Die Funktion arbeitet blockbasiert: Ereignisblöcke bestehen in der Regel aus mehreren aufeinanderfolgenden 
Zeilen, die durch inhaltliche Merkmale wie Bindestriche, kleingeschriebene Fortsetzungszeilen oder fehlende 
Datumsmarkierungen zusammenhängend interpretiert werden. Die Funktion \texttt{is\_continuation()} prüft dabei, 
ob eine Zeile an die vorhergehende angeschlossen werden kann, etwa durch typische Anschlusswörter oder formale Merkmale. 
Sobald ein abgeschlossener Block erkannt ist, wird dieser mit der Hilfsfunktion \texttt{build\_event()} zu einem 
vollständigen Ereignisobjekt zusammengeführt.

In \texttt{build\_event()} wird der aus mehreren Zeilen bestehende Textblock zunächst als Fliesstext zusammengesetzt 
und inhaltlich analysiert. Es werden mehrere Entitäten erkannt und dem Ereignis zugeordnet:

\begin{itemize}
  \item \textbf{Orte:} Über die Funktion \texttt{match\_place()} aus dem Modul \texttt{place\_matcher.py} werden potenzielle Ortsnamen im Text identifiziert und mit der Groundtruth abgeglichen. Dabei wird jeder erkannte Ort zusätzlich durch eine Plausibilitätsprüfung überprüft, bevor er als \texttt{Place}-Objekt übernommen wird.
  \item \textbf{Daten:} Die Funktion \texttt{extract\_custom\_date()} durchsucht die XML-Zeile nach Datumsangaben in den XML-Tags. Wenn kein strukturiertes Datum vorhanden ist, aber einfache numerische Formate wie \enquote{15.03} im Fliesstext erkannt werden, werden diese als Datum übernommen.
  \item \textbf{Organisationen:} Über \texttt{match\_organization\_from\_text()} wird der Textblock mit bekannten Organisationseinträgen abgeglichen. Bei Übereinstimmung werden entsprechende Organisationen als strukturierte Objekte ergänzt.
  \item \textbf{Personen:} Mögliche Namen werden durch reguläre Ausdrücke identifiziert und mit Hilfe der Funktion \texttt{extract\_name\_with\_spacy()} in Vor- und Nachnamen getrennt. Anschliessend erfolgt ein Abgleich mit der Personen-Groundtruth über \texttt{match\_person()}. Positive Treffer werden inklusive Match-Score und Herkunftskennzeichnung als \texttt{Person}-Objekte dem Ereignis zugeordnet.
\end{itemize}

Die fertigen Ereignisse bestehen jeweils aus einer Kurzbeschreibung (in der Regel der ersten Zeile), 
einer ausführlichen Beschreibung (bestehend aus allen zugehörigen Textzeilen), einem Datumsfeld, einer 
Ortsangabe und einer strukturierten Liste aller beteiligten Orte, Organisationen und Personen. Der vollständige 
Satz aller so erkannten Ereignisse wird am Ende als Liste von \texttt{Event}-Objekten zurückgegeben.

Das Modul arbeitet vollständig dateibasiert und benötigt als einzige Eingabe den Pfad zur Transkribus-XML-Datei 
sowie eine initialisierte Instanz des \texttt{PlaceMatcher}. Es greift auf zentrale Komponenten der Projektarchitektur 
zurück, darunter die Groundtruth-Listen für Orte, Personen und Organisationen. Die extrahierten Ereignisse werden 
im finalen JSON unter dem Attribut \texttt{events} gespeichert. Da ein Event ein eher abstraktes Konstrukt ist, liegt der
Fokus der Pipeline weniger auf diesem Modul, das zu einem späteren Zeitpunkt beispielsweise durch präziseres Prompten für das
Eventtagging optimiert werden soll. 

\subsubsection*{date\_matcher.py}\label{subsection:date_matcher}
Das Modul \texttt{date\_matcher.py} dient der systematischen Extraktion, Normalisierung und 
Zählung von Datumsangaben in den Transkribus-Dokumenten. Es basiert auf der 
Auswertung strukturierter Angaben, die im Rahmen der Transkription über XML-Custom-Tags im 
\texttt{custom}-Attribut einzelner \texttt{<TextLine>}-Elemente eingebettet wurden. Diese 
Daten gelten innerhalb des Korpus als zuverlässig, da sie während der manuellen Korrekturprozesse
in Transkribus einheitlich normiert und im Format \texttt{dd.mm.yyyy} ergänzt wurden.


Treten im historischen Text verkürzte Datumsangaben wie \textit{\enquote{1.~d.~Mts}} auf, so handelt
es sich um Abkürzungen, die bei der Transkription mit einem entsprechenden \texttt{abbreviation}-Tag
markiert werden. Die Funktionsweise dieser Markierungen sowie die heuristische Auflösung solcher 
verkürzter Angaben wird im Kapitel \ref{subsec:TaggingKapitel} erläutert. Lässt sich aus dem weiteren 
Kontext\footnote{Zum Beispiel durch Hinweise im Seiteninhalt oder durch übergeordnete Informationen im Umfeld der Akte} 
ein vollständiges Datum erschliessen, kann dieses anschliessend in strukturierter Form übernommen und im JSON als 
normiertes Datum gespeichert werden.

Innerhalb der Verarbeitungspipeline wird das Modul über die Funktionen \texttt{extract\_custom\_date()} und 
\texttt{combine\_dates()} aufgerufen. Zunächst durchläuft \texttt{extract\_custom\_date()} das XML-Dokument 
und extrahiert alle \texttt{custom}-Attribute, die ein \texttt{date\{…\}}-Muster enthalten. Die Inhalte dieser 
Attribute werden bereinigt und zur weiteren Analyse an die Funktion \texttt{extract\_date\_from\_custom()} übergeben.\\
Diese Funktion überprüft mithilfe RegEx, ob der String tatsächlich eine gültige Datumsangabe enthält. 
Dabei wird insbesondere nach einem \texttt{when}-Feld gesucht, das im Inneren des \texttt{date}-Blocks enthalten ist. 
Die in diesem Feld hinterlegten Daten werden anschliessend mit der Funktion \texttt{parse\_custom\_attributes()} als 
Key-Value-Paare interpretiert. Liegt ein gültiges Datum vor, wird dessen Format mit \texttt{normalize\_to\_ddmmyyyy()} 
überprüft und gegebenenfalls vereinheitlicht.
\\
Unterstützt werden mehrere Eingabeformate, darunter standardisierte Formen wie \texttt{dd.mm.yyyy}, ISO-Formate 
wie \texttt{yyyy-mm-dd} oder zweistellige Jahresangaben, die automatisch in vierstellige Jahre des 20.
Jahrhunderts umgewandelt werden. Zusätzlich erkennt die Funktion auch Intervallangaben wie \texttt{01/03.04.1944}, 
bei denen ein Datumsbereich über einen Schrägstrich kodiert ist. Solche Intervalle werden in strukturierter Form mit 
einem \texttt{from}- und \texttt{to}-Wert als \texttt{date\_range} gespeichert.

Die Funktion \texttt{combine\_dates()} führt schliesslich alle erkannten Einzel- und Intervallangaben zusammen, 
zählt deren Häufigkeit im Dokument und erstellt eine deduplizierte, sortierte Liste für den späteren Export. Dabei 
wird jede identifizierte Angabe – ob Einzeldatum oder Zeitspanne – um eine Zählung der Nennungen ergänzt. Bei 
Intervallen wird zusätzlich der Originalstring dokumentiert, aus dem die Angabe hervorging.

Das Ergebnis der Verarbeitung wird im Feld \texttt{mentioned\_dates} gespeichert. Jeder Eintrag enthält entweder 
ein einzelnes Datum oder einen Datumsbereich, ergänzt um die Häufigkeit und gegebenenfalls den ursprünglichen Wortlaut 
aus dem \texttt{custom}-Attribut.

Das Modul arbeitet unabhängig von externen Ressourcen und benötigt lediglich das XML-Baumobjekt des jeweiligen 
Dokuments. Die so gewonnenen Zeitangaben bilden die Grundlage für die chronologische Einordnung, 
Kontextualisierung und Auswertung der digitalen Quellenbasis.

\begin{minipage}[t]{0.52\textwidth}
  \setstretch{1.5}
  \justifying%

  \subsubsection{unmatched\_logger.py}\label{subsec:unmatched_logger}

Das Modul \texttt{unmatched\_logger.py} dient der systematischen Protokollierung von Entitäten, 
die in der aktuellen Version der Groundtruth noch nicht enthalten sind. 
Diese Protokolle bilden die Grundlage für weiterführende Recherchen, 
durch die die Groundtruth schrittweise ergänzt und verbessert werden kann.

\noindent Innerhalb der Verarbeitungs-Pipeline wird das Modul \texttt{unmatched\_logger.py} über die Funktion \texttt{process\_single\_xml()} im Hauptprogramm aufgerufen. 

\noindent Bereits in der Testphase kam das Modul mehrfach zum Einsatz, um die Erkennung und Zuordnung bislang nicht erfasster Entitäten zu überprüfen.
\end{minipage}%
\hfill
\begin{minipage}[t]{0.4\textwidth}
  \vspace*{0.7cm} % ► schiebt alles um 0.7cm nach unten
\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}


% TikZ styles
\tikzset{
  module/.style={rectangle, draw=black, fill=blue!10, minimum width=0.4cm, minimum height=0.2cm},
  source/.style={rectangle, draw=black, fill=green!20, minimum width=0.4cm, minimum height=0.2cm},
  highlight/.style={rectangle, draw=black, fill=blue!40, minimum width=0.4cm, minimum height=0.2cm},
  arrow/.style={-stealth, line width=0.15mm},
  arrowboth/.style={stealth-stealth, line width=0.15mm},
  process/.style={rectangle, draw=black, fill=orange!20, thick, minimum width=6cm, minimum height=1cm, align=center},
  large/.style={rectangle, draw=black, fill=blue!10, thick, minimum width=4.5cm, minimum height=1.2cm, align=center}
};
%==========================
% Mini-Diagramm oben rechts
%==========================

\begin{scope}[shift={(9,0.45)}, node distance=0.3cm]

\node[module] (init) at (0,0) {};
\node[module] (xml) at (0, -0.5) {};
\node[module]    (events)  at (-2.0, -1.3) {};
\node[module]    (roles)   at (-1.2, -1.3) {};
\node[module] (persons) at (-0.4, -1.3) {};
\node[module]    (orgs)    at ( 0.4, -1.3) {};
\node[module]    (places)  at ( 1.2, -1.3) {};
\node[module]    (dates)   at ( 2.0, -1.3) {};
\node[module] (authors)    at (-0.4, -2  ) {};

\node[coordinate] (joinpoint) at ($(xml.south) + (0, -1.8cm)$) {};
\node[coordinate] (joinpointxml) at ($(xml.south) + (0, -0.3cm)$) {};

\node[highlight, below=of joinpoint] (unmatched) {};
\node[module, below=of unmatched] (json) {};
\node[source, below=of json] (save) {};
\node[source, right=0.5cm of save] (unmatchedjson) {};
\draw[arrow, dashed] (persons.south) -- (authors.north);
\draw[arrow] (unmatched.east) -| (unmatchedjson.north);

\draw[arrow] (json.south) -- (save.north);
\draw[arrow] (init.south) -- (xml.north);
\draw[arrowboth] (roles.east) -- (persons.west);
\draw[arrowboth] (persons.east) -- (orgs.west);
\draw[arrowboth] (orgs.east) -- (places.west);
%Pfeile oben
\draw[arrow] (xml.south) -- (joinpointxml.south);
\draw[arrow] (joinpointxml) -- ($(roles.north |- joinpointxml)$) -- (roles.north);
\draw[arrow] (joinpointxml) -- ($(persons.north |- joinpointxml)$) -- (persons.north);
\draw[arrow] (joinpointxml) -- ($(orgs.north |- joinpointxml)$) -- (orgs.north);
\draw[arrow] (joinpointxml) -- ($(places.north |- joinpointxml)$) -- (places.north);
\draw[arrow] (joinpointxml) -- ($(dates.north |- joinpointxml)$) -- (dates.north);
\draw[arrow] (joinpointxml) -- ($(events.north |- joinpointxml)$) -- (events.north);

%Pfeile unten; Doppelte Pfeile mit definiertem Stil
\draw[arrow] (events.south) -- ($(events.south |- joinpoint)$);
\draw[arrow] ($(events.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (roles.south) -- ($(roles.south |- joinpoint)$);
\draw[arrow] ($(roles.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (authors.south) -- ($(authors.south |- joinpoint)$);
\draw[arrow] ($(authors.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (orgs.south) -- ($(orgs.south |- joinpoint)$);
\draw[arrow] ($(orgs.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (places.south) -- ($(places.south |- joinpoint)$);
\draw[arrow] ($(places.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (dates.south) -- ($(dates.south |- joinpoint)$);
\draw[arrow] ($(dates.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (joinpoint.south) -- (unmatched.north);
\draw[arrow] (unmatched.south) -- (json.north);
\draw[arrow] (json.south) -- (save.north);
\end{scope}

% Grosses Prozessdiagramm
\begin{scope}
% --- Kopf-Node ---
\node[large] (logger) at (1.5,0) {\textbf{log\_unmatched\_entities} \\ (Starte Logging aller Entitätstypen)};

% --- Schleifen ---
\node[process, below=1.2cm of logger] (persons)  {\textbf{Personen} \\ (Nicht in mentioned\_persons? → log)};
\node[process, below=of persons] (places)  {\textbf{Orte} \\ (Nicht in mentioned\_places? → log)};
\node[process, below=of places] (roles)  {\textbf{Rollen} \\ (Nicht in Personenrollen? → log)};
\node[process, below=of roles] (orgs)  {\textbf{Organisationen} \\ (Nicht als associated\_organisation? → log)};
\node[process, below=of orgs] (events)  {\textbf{Events} \\ (Nicht in mentioned\_events? → log)};
\node[process, below=of events] (done)  {\textbf{Speichern} \\ (save\_json, print)};

% --- Pfeile ---
\draw[arrow] (logger.south) -- (persons.north);
\draw[arrow] (persons.south) -- (places.north);
\draw[arrow] (places.south) -- (roles.north);
\draw[arrow] (roles.south) -- (orgs.north);
\draw[arrow] (orgs.south) -- (events.north);
\draw[arrow] (events.south) -- (done.north);


\node[draw=gray, dashed, rounded corners, inner sep=0.5cm, fit=(persons)(orgs)(done)] (group) {};
\end{scope}
\end{tikzpicture}} % ► Die Caption direkt darunter, blockweise formatiert
  \captionof{figure}{\\\small Oben links: \small Prozessdiagramm für \texttt{unmatched\_logger.py},\\{\small Oben rechts: Pipelineübersicht}}
  \label{fig:unmatched_logger.py}
\end{minipage}


Im Kern stellt das Modul die Funktion \texttt{log\_unmatched\_entities} bereit. 
Diese übernimmt die von den zuvor beschriebenen Matcher-Funktionen ermittelten Entitäten 
und prüft, ob sie in den entsprechenden Groundtruth-CSV-Dateien vorhanden sind. 

Die Suche erfolgt iterativ innerhalb der Listenstrukturen für Personen, Orte, Rollen, Organisationen und Ereignisse. 
Wird eine Entität über ein XML-Custom-Tag einer dieser Kategorien zugewiesen, ohne dass sie in der Groundtruth verzeichnet ist, 
wird sie in einer spezifischen JSON-Datei protokolliert.

Die folgenden Dateien werden dabei erzeugt:
\begin{itemize}
  \item unmatched\_persons.json
  \item unmatched\_places.json
  \item unmatched\_roles.json
  \item unmatched\_events.json
  \item unmatched\_organisations.json
\end{itemize}

Zusätzlich werden alle Einträge in einer zusammengeführten Datei \texttt{unmatched.json} gespeichert, 
um einen vollständigen Überblick nicht zugeordneter Entitäten zu gewährleisten.


Alle Ergebnisse werden zudem in einer Datei \code{unmatched.json} gespeichert, um einen Gesamtüberblick zu erhalten.



\subsection{KEINE AHNUNG WAS DIE HIER MACHEN}
\subsubsection{validation\_module.py}
\subsubsection{validation\_module.py}
\subsubsection{test\_role\_schema.py}
\subsubsection{llm\_enricher.py}
\subsubsection{enrich\_pipeline.py}


%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%---------------    Analyse & Diskussion der Ergebnisse   ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––% 
\section{Analyse \& Diskussion der Ergebnisse}
\subsection{Visualisierung auf der VM}

%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––           Fazit        ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––% 
\section{Fazit und Ausblick}
\subsection{Zusammenfassung der zentralen Erkenntnisse}
\subsection{Methodische Herausforderungen und Lösungen}
\subsection{Ausblick auf zukünftige Forschung und mögliche Erweiterungen der Datenbank}
\newpage

%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––           Bibliographie         –––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
\pagecolor{white}
\newpage
\begingroup
\small
\singlespacing%
\printbibliography[
heading=bibintoc,
title={Bibliographie}%title of the 'references' section, change this if necessary
]%
\endgroup

%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––                                        ––––––––––––––––––––––% 
%––––––––––––––––––––––––           Anhang                       ––––––––––––––––––––––%
%––––––––––––––––––––––––                                        ––––––––––––––––––––––% 
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––% 

\newpage
\appendix
\section{Anhang}
\begingroup
\small
\subsection{PDF\_to\_JPEG.py}\label{section:PDF_to_JPEG}
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=LightGray,
fontsize=\footnotesize,
linenos,
breaklines=true,  %Automatischer Zeilenumbruch
breakanywhere=true % Minted darf überall umbrechen (optional)
]{python}
import os
import fitz  # PyMuPDF

def convert_pdf_to_jpg(src_folder, dest_folder):
    # Überprüfen, ob der Zielordner existiert, und ihn ggf. erstellen
    if not os.path.exists(dest_folder):
        os.makedirs(dest_folder)

    # Durchgehen durch alle Dateien im Quellordner
    for root, dirs, files in os.walk(src_folder):
        for file in files:
            # Überprüfen, ob die Datei eine PDF-Datei ist
            if file.lower().endswith(".pdf"):
                # Vollständigen Pfad zur PDF-Datei erstellen
                pdf_path = os.path.join(root, file)
                # PDF-Datei öffnen
                doc = fitz.open(pdf_path)
                # Durch alle Seiten der PDF-Datei gehen
                for page_num in range(len(doc)):
                    page = doc[page_num]
                    # Seite in ein PixMap-Objekt umwandeln (für die Konvertierung in JPG)
                    pix = page.get_pixmap()
                    # Dateinamen ohne Dateiendung extrahieren
                    filename_without_extension = os.path.splitext(file)[0]
                    # Ausgabedateinamen erstellen mit führenden Nullen für die 
                    # Seitennummer
                    output_filename = f"{filename_without_extension}_S{page_num + 1:03d}.jpg"


                    # Vollständigen Pfad zur Ausgabedatei erstellen
                    output_path = os.path.join(dest_folder, output_filename)
                    # Bild speichern
                    pix.save(output_path)
                # PDF-Datei schliessen
                doc.close()
                
                # Erfolgsmeldung ausgeben
                print(f"{file} wurde erfolgreich umgewandelt und gespeichert
                in {dest_folder}")

# Pfade zu den Ordnern mit den PDF-Dateien (Quelle) und den JPG-Dateien (Ziel)
src_folder = r"/Users/svenburkhardt/Documents/D_Murger_Männer_Chor_Forschung/Scan_Männerchor/Männerchor_Akten_1925–1945/Scan_Männerchor_PDF"
dest_folder = r"/Users/svenburkhardt/Documents/D_Murger_Männer_Chor_Forschung/Masterarbeit/JPEG_Akten_Scans"


# Funktion aufrufen, um die Konvertierung durchzuführen
convert_pdf_to_jpg(src_folder, dest_folder)

\end{minted}

\subsection{Tagging in Transkribus}\label{subsec:Taggingregeln_Anhang}


    Transkribus und seine Modelle unterstützen nicht nur beim Transkribieren der Texte, sondern erlauben auch das Taggen von \textit{Named Entities}.  
    Für die vorliegende Arbeit sind dabei besonders Personen, Orte, Organisationen und Daten relevant.  
    Um hierfür ein stringentes Verfahren zu entwickeln, wurden die Tags wie folgt definiert:
  
\subsubsection{Strukturelle Tags}
    \begin{description}

    % Abbreviations
    \item\texttt{\textbf{{\colorbox{abbrev}{abbrev}}}}
    
        
    Mit dem Tag \texttt{\texttt{\textbf{{\colorbox{abbrev}{abbrev}}}}} werden alle Abkürzungen getaggt, die für eine eindeutige Entität stehen.

    
    \noindent\textbf{\ding{43} Beispiel 1}: Dr., Prof., St., Hr., Frl., Dipl.-Ing., etc.

    \textbf{\ding{43} Beispiel 2}: Organisationskürzel, wenn sie eindeutig sind:\\\code{<abbrev>~V.D.A.~</abbrev>}.

    \textbf{\textbf{\ding{43} Beispiel 3}}: Falls eine dazugehörige Entität vorhanden ist, wird die Abkürzung getaggt und wird gleichzeitig als zugehörige Entität getaggt:

    \code{<person>~<abbrev>~Dr.~</abbrev>~Weiss~</person>}
    
    % Unclear    
    \item\texttt{\textbf{{\colorbox{unclear}{unclear}}}}
    

    Mit dem Tag \texttt{\texttt{\textbf{{\colorbox{unclear}{unclear}}}}} werden unleserliche oder schwer entzifferbare Textstellen markiert.
    
    \noindent\textbf{\ding{43} Beispiel 1}: Unklare Zeichen oder fehlende Buchstaben: 

    \code{Er wohnte in~<unclear>~[\ldots]~<unclear>}.

    \textbf{\ding{43} Beispiel 2}: Teilweise lesbare Wörter:

    \code{<place>~Frei~<unclear>~[\ldots]~<unclear>~<place>}.

    
%Sic   
    \item\texttt{\textbf{{\colorbox{sic}{sic}}}} 

    Mit dem Tag \texttt{sic} werden Wörter markiert, die im Originaltext in einer falschen oder ungewöhnlichen Schreibweise geschrieben wurden.

    \noindent\ding{43} Beispiel 1: Veraltete oder falsche Schreibweisen: 

    \code{<sic>~daß~</sic>} für dass mit tz.

    \ding{43} Beispiel 2: Offensichtliche Tippfehler, wenn sie im Originaltext so vorkommen: 

    \code{Wir haben {<sic>~einen~</sic>} grosse Freude.}

    \ding{43} Beispiel 3: Falls eine Korrektur notwendig ist, kann sie als Kommentar ergänzt werden. 

    \end{description}
    \endgroup


    \subsubsection{Inhaltliche Tags}
    \begin{description}
    % Person
    \item\texttt{\textbf{{\colorbox{person}{person}}}}
        
    Mit dem Tag \texttt{\colorbox{person}{person}} sollen alle Strings getaggt, die eine direkte Zuordnung einer Person ermöglichen.
    
    \noindent \textbf{\ding{43} Beispiel 1}: Vereinsführer, Alfons, Zimmermann, Alfons Zimmermann, Z. A. Zimmermann, Herr Zimmermann, Herr Alfons Zimmermann, etc. 

    \textbf{\ding{43} Beispiel 2}: Funktionen wie Oberlehrer, Chorleiter, etc.
    Wenn Ort, Name oder Organisation bekannt sind. Eine Person kann sowohl mit ihrem Namen als auch ihrer Funktion (wie Dirigent) getaggt werden.  
    Aus der Korrespondenz ist in der Regel eine zugehörige Organisation ersichtlich, mit deren Verknüpfung eine namentlich nicht genannte Person identifiziert werden könnte.

    
    % Signature    
    \item\texttt{\textbf{{\colorbox{signature}{signature}}}}
        
    Mit dem Tag \texttt{\colorbox{signature}{signature}} werden alle Strings getaggt, die eine handschriftliche Unterschrift darstellen.  
    Der Tag \texttt{\colorbox{signature}{signature}} ist nahezu deckungsgleich mit dem Tag \texttt{\colorbox{person}{person}}.  
    Er dient zur \textbf{graduellen Unterscheidung}, ob ein Name im Fliesstext als gesichert leserlich oder handschriftlich als Signatur vorliegt.  
    
    \noindent \textbf{\ding{43} Beispiel 1}: Eindeutig lesbare Signaturen werden direkt getaggt:  

    \code{<signature>~A. Zimmermann~</signature>}. 

    \textbf{\ding{43} Beispiel 2}: Teilweise unleserliche Signaturen werden mit dem Tag \texttt{\colorbox{unclear}{unclear}} innerhalb von \texttt{\colorbox{signature}{signature}} markiert: 

    \code{<signature>~R. We~<unclear>~[\ldots]~</unclear>~</signature>}. \\
    
    \textbf{\ding{43} Beispiel 3}: Wenn nur ein Teil des Namens lesbar ist, aber eine Identifikation unsicher bleibt, sollte die Unterschrift vollständig im Tag \texttt{\colorbox{unclear}{unclear}} innerhalb von \texttt{\colorbox{signature}{signature}} stehen:\\

    \code{<signature>~<unclear>~\textit{etwas unleserliches}~</unclear>~</signature>}.
    
    \textbf{\ding{43} Beispiel 4}: Wenn eine Signatur einer bekannten Person zugeordnet werden kann, aber nicht vollständig lesbar ist, bleibt die Signatur erhalten und wird \textbf{ohne} den Tag \texttt{\colorbox{person}{person}} zu verwenden: 

    \code{<signature>~A. Zimm~<unclear>~[\ldots]~</unclear>~</signature>}.
    
    \textbf{\ding{43} Beispiel 5}: Wenn eine Unterschrift vollständig transkribiert wurde und die Person bekannt ist, wird sie nur mit \texttt{\colorbox{signature}{signature}} getaggt, \textbf{ohne} den Tag \texttt{\colorbox{person}{person}} zu verwenden:  

    \code{<signature>~Alfons Zimmermann~</signature>}.
    
    % Organization    
    \item\texttt{\textbf{{\colorbox{organization}{organization}}}}
        
    Mit dem Tag \texttt{\colorbox{organization}{organization}} werden alle Strings getaggt, die eine direkte Zuordnung einer Organisation ermöglichen.  
    
    \noindent \textbf{\ding{43} Beispiel 1}: Männerchor Murg, Verein Deutscher Arbeiter (V.D.A.), Murgtalschule, etc.

    \textbf{\ding{43} Beispiel 2}: Abkürzungen, wenn sie eine Organisation eindeutig bezeichnen, z.B. V.D.A., NSDAP, STAGMA, etc.
    

    % Place    
    \item\texttt{\textbf{{\colorbox{place}{place}}}}
        
    Mit dem Tag \texttt{\colorbox{place}{place}} werden alle Strings getaggt, die sich auf einen geografischen Ort beziehen.  
    
    \noindent \textbf{\ding{43} Beispiel 1}: Murg (Baden), Freiburg, Berlin, Murgtal, Schwarzwald, etc.

    \textbf{\ding{43} Beispiel 2}: Orte mit näherer Bestimmung, z.B. \enquote{bei Berlin}, \enquote{im Murgtal} werden getaggt:  

    \code{<place>~im Murgtal</place>}.

    
    % Date    
    \item\texttt{\textbf{{\colorbox{date}{date}}}}
        
    Mit dem Tag \texttt{\colorbox{date}{date}} werden alle expliziten und implizierten Datumsangaben markiert.  
    
    \noindent \textbf{\ding{43} Beispiel 1}:  29.05.1936

    \textbf{\ding{43} Beispiel 2}: 29. Mai 1936

    \textbf{\ding{43} Beispiel 3}: den 29.\ \  d. Mts.:

    \code{\string<date when=\enquote{29.05.1936}>~den 2.~<abbrev>\ \ d. Mts.~</abbrev> </date>}


% Event
    \item\texttt{\textbf{{\colorbox{eventTag}{event}}}}
    
    Mit dem Tag \texttt{\colorbox{eventTag}{event}} werden expliziten und implizierten Ereignisse markiert. Diese Ereignisse haben einen zeitlichen oder räumlichen Bezug, und können benannt werden. Dazu zählen:

    \noindent \textbf{\ding{43} Beispiel 1}: \enquote{Jubiläumskonzert}

    \textbf{\ding{43} Beispiel 2} \enquote{Gründung des Vereins} 

    \textbf{\ding{43} Beispiel 2}\enquote{Kriegsausbruch} oder \enquote{Kriegsende}

    Konzepte, die nicht klar in den Texten benannt werden, wie beispielsweise die Suche nach einem Dirigenten, können nicht immer Ereignis getaggt werden. Sie sollen später aber in der Datenbank implementiert werden.
    \end{description}


\subsection{Prompt der LLM Vorverarbeitung}\label{subsec:LLM-Promt}

    \begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=LightGray,
fontsize=\tiny,
linenos,
breaklines=true,  %Automatischer Zeilenumbruch
breakanywhere=true % Minted darf überall umbrechen (optional)
]{python}

    prompt = f"""
    Systemrolle:
    Du bist ein spezialisiertes XML-Annotationstool für historische Transkribus-Dokumente.

    Aufgabe:
    Analysiere das gesamte PAGE-XML-Dokument. Extrahiere Entitäten aus dem Unicode-Text aller <TextLine>-Elemente und füge strukturierte `custom="..."`-Attribute hinzu. 

    Strikte Regeln:

    1. Dokumentanalyse:
    - Verarbeite ausschliesslich <TextLine>-Elemente.
    - Verwende nur <Unicode>-Inhalte als Eingabetext.

    2. Globale Personenerkennung:
    - Erkenne Personen (inkl. Titel, Vorname, Nachname).
    - Speichere `offset` und `length` für jede erkannte Person pro TextLine.
    - Verwende **immer dieselben Offsets** bei wiederholten Nennungen im Dokument.

    3. Empfängerkennung (`recipient`):
    - Der Kopfbereich endet an der ersten komplett leeren TextLine.
    - Erkenne dort Anreden (z. B. „Herr“, „Frau“, „Sehr geehrter Herr …“).
    - Verknüpfe Anrede mit passender Person und annotiere mit:
    `recipient {{offset:X; length:Y;}}`
    
    4. Autorenkennung (`author`):
    - Der Fussbereich beginnt nach der letzten Grussformel (z. B. „Mit freundlichen Grüssen“).
    - Namen → `author {{offset:X; length:Y;}}`.
    - Funktion (z. B. „Chorleiter“) → `role {{offset:X; length:Y;}}`.

     5. Ort- und Datumsannotation:
    - **Absendeort** (creation_place) und **Erstellungsdatum** (creation_date):
        zusätzlich zu den Tags place und date hinzu:
        creation_place {{offset:X; length:Y;}} und creation_date {{offset:X; length:Y; when:TT.MM.JJJJ;}}.
    - **Empfangsort** (recipient_place):
        Füge im Empfänger-Block die passende Zeile mit:
        place {{offset:X; length:Y;}}.

    6. Entitäten pro Zeile (in dieser Reihenfolge):
    Füge **ein** Attribut `custom="..."` ein mit nur den tatsächlich erkannten Entitäten:


    person {{offset:X; length:Y;}}
    recipient {{offset:X; length:Y;}}
    author {{offset:X; length:Y;}}
    organization {{offset:X; length:Y;}}
    place {{offset:X; length:Y;}}
    date {{offset:X; length:Y; when:TT.MM.JJJJ;}}
    role {{offset:X; length:Y;}}
    event {{offset:X; length:Y;}} → optional mit when:TT.MM.JJJJ;

    Hinweise:
    - Füge **nur tatsächlich vorhandene Entitäten** ein.
    - Keine Platzhalter.
    - Format für `date` und `event` (falls Datum erkennbar): `when:TT.MM.JJJJ;`
    - Mehrzeilige Events (z. B. bei Bindestrich am Ende oder fortgeführtem Satz) erhalten dieselbe `event`-Annotation in allen betroffenen Zeilen.

    6. XML-Regeln:
    - **Verändere nur** `custom`-Attribute innerhalb von `<TextLine>`.
    - Belasse alle anderen XML-Strukturen vollständig unverändert.

    7. Ausgabe:
    - Gib ausschliesslich ein vollständiges, wohlgeformtes XML zurück.
    - Kein Freitext, kein Kommentar, kein Markdown.

    Beispielausgabe:
    <?xml version="1.0" encoding="UTF-8"?>
    <PcGts xmlns="http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15">
    <Page imageFilename="dummy.jpg" imageWidth="1000" imageHeight="1000">
        <TextRegion id="r1">
        <TextLine id="tl1" custom="place {{offset:0; length:7;}} creation_place {{offset:0; length:7;}} date {{offset:8; length:9; when:28.05.1942;}} creation_date {{offset:8; length:9; when:28.05.1942;}}">
            <Coords points="0,0 100,0 100,10 0,10"/>
            <TextEquiv><Unicode>München 28.V.1942</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl2" custom="recipient {{offset:7; length:5;}} person {{offset:7; length:5;}} place {{offset:15; length:6;}} recipient_place {{offset:15; length:6;}}">
            <Coords points="0,20 100,20 100,30 0,30"/>
            <TextEquiv><Unicode>Lieber Otto, Berlin</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl3" custom="event {{offset:24; length:38; when:28.05.1942;}} place {{offset:42; length:7;}}">
            <Coords points="0,40 100,40 100,50 0,50"/>
            <TextEquiv><Unicode>Heute abend fand ein Konzert im Opernhaus in München statt, und ich</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl4" custom="organization {{offset:43; length:28;}} place {{offset:66; length:16;}}">
            <Coords points="0,60 100,60 100,70 0,70"/>
            <TextEquiv><Unicode>lauschte den himmlischen Stimmen des Männerchors Hintertuüpfingen eV.</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl5" custom="organization {{offset:34; length:3;}} organization {{offset:40; length:18;}}">
            <Coords points="0,80 100,80 100,90 0,90"/>
            <TextEquiv><Unicode>Das alles fand im Rahmen des WhW - des Winterhilfswerk statt.</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl6" custom="organization  {{offset:50; length:17;}} place {{offset:72; length:4;}} place {{offset:83; length:6;}}">
            <Coords points="0,100 100,100 100,110 0,110"/>
            <TextEquiv><Unicode>Ich hoffe wir sehen uns bald bei einem Auftritt des Männerchors Murg wieder, oder in Hänner?</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl7" custom="role {{offset:14; length:14;}} person {{offset:29; length:4;}}">
            <Coords points="0,120 100,120 100,130 0,130"/>
            <TextEquiv><Unicode>Grüss mir den Vereinsführer Asal,</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl8">
            <Coords points="0,140 100,140 100,150 0,150"/>
            <TextEquiv><Unicode>Alles Liebe,</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl9" custom="author {{offset:6; length:17;}} person {{offset:6; length:17;}}">
            <Coords points="0,160 100,160 100,170 0,170"/>
            <TextEquiv><Unicode>Deine Lina Fingerdick</Unicode></TextEquiv>
        </TextLine>
        <!-- Neue Zeile für den Empfangsort -->
        <TextLine id="tl10" custom="salutation {{offset:0; length:2;}} recipient {{offset:3; length:13;}} address {{offset:18; length:21;}} place {{offset:41; length:4;}}">
            <Coords points="0,180 100,180 100,190 0,190"/>
            <TextEquiv>
            <Unicode>An Otto Bolliger, Adolf-Hitler Platz 1, Murg</Unicode>
            </TextEquiv>
        </TextLine>
        </TextRegion>
    </Page>
    </PcGts>

    Hier ist das zu annotierende XML:

```
{xml_content}
```
"""
\end{minted}

    
   \end{document}
