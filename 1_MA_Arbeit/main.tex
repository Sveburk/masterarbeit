\sloppy
% cd /Users/svenburkhardt/Developer/masterarbeit/1_MA_Arbeit/
% source /Users/svenburkhardt/Developer/masterarbeit/.venv311/bin/activate
% pdflatex -shell-escape main.tex
% biber main
% pdflatex -shell-escape main.tex
% pdflatex -shell-escape main.tex





%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––    S E T T I N G S        ––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%



% Options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[12pt, a4paper, ngerman, bidi=default]{article}

%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––    S E T T I N G S     ––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––      P A K E T E        ––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
\def\langde{}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[fixed]{fontawesome5} %Fontawesome für Icons und Symbole; siehe https://mirrors.ibiblio.org/CTAN/fonts/fontawesome5/doc/fontawesome5.pdf
\usepackage{amsmath,amssymb}
\usepackage{tcolorbox}
\usepackage{afterpage}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{subcaption} %Für die Verwendung von subfigure
\usepackage{setspace} %Für den Befehl \setstretch
\setlength{\parindent}{0pt}
\usepackage{transparent}
\usepackage{tikz}
  \usetikzlibrary{shapes.geometric, arrows.meta,fit, backgrounds, calc, positioning}
\usepackage{pgf-pie}%Für Kreisdiagramme
\usepackage{pgfplots}%Für Diagramme
  \pgfplotsset{compat=1.18}
\usepackage{eso-pic}%Für Hintergrundbilder
\usepackage{fvextra}    %Muss vor csquotes geladen werden
% \usepackage{csvsimple}%Für CSV-Dateien
% \usepackage{booktabs}   %für \toprule, \midrule, \bottomrule
% \usepackage{longtable}  %falls nicht schon geladen
% \usepackage{placeins}%Für \FloatBarrier
\usepackage[autostyle, german=quotes]{csquotes}
\usepackage[
  backend=biber,
  style=authoryear-ibid,
  autocite=footnote,  % Automatisch Fussnote
  language=ngerman,
  citetracker,
  ibidtracker,
  backref=true,
  backrefstyle=three+
]{biblatex}
\let\cite\footcite


\DeclareBibliographyAlias{software}{misc}
\DefineBibliographyStrings{ngerman}{%
  page = {S\adddot},
  pages = {S\adddot\addspace ff\adddot}
}
\DefineBibliographyStrings{ngerman}{
  urlseen = {Zugriff am}
}
\DefineBibliographyStrings{ngerman}{
  backrefpage = {zit. auf S\adddot},
  backrefpages = {zit. auf S\adddot}
}

\DeclareFieldFormat{urldate}{\addspace#1}
\renewbibmacro*{url+urldate}{%
  \printfield{url}%
  \setunit*{\addspace}%
  \iffieldundef{urldate}
    {}
    {\printtext[urldate]{\printurldate}}%
}

\setlength{\skip\footins}{20pt}
\setlength{\bibitemsep}{1.5\baselineskip}
\addbibresource{assets/Literature_Bib/Masterarbeit.bib}

\renewbibmacro*{date}{% 
     \ifentrytype{online}{% Falls der Eintrag @online ist, nimm das urldate
         \printtext[parens]{Zugriff am \usebibmacro{urldate}}}{\printdate}}

\usepackage[ngerman]{babel}
\usepackage{pifont} %Für die Kästchen und Häkchen-Symbole
\usepackage[dvipsnames,svgnames,x11names,table]{xcolor}
\usepackage[table]{xcolor}
%Zum Definieren und Verwenden von Farben
\definecolor{LightGray}{gray}{0.9}
\definecolor{MediumGray}{gray}{0.7}
\definecolor{UniRot}{HTML}{D20537}                  %Corperate Design Farben Uni Basel 
\definecolor{UniAnthrazit}{HTML}{46505A}            %Corperate Design Farben Uni Basel 
\definecolor{UniMint}{HTML}{A5D7D2}                 %Corperate Design Farben Uni Basel 
\definecolor{VeryLightGray}{gray}{0.95}%Sehr helles Grau
\definecolor{OldPaper}{HTML}{f4eade}


\definecolor{UniBlue}{HTML}{1F78B4}     %Kräftiges Blau
\definecolor{UniGold}{HTML}{FFB000}     %Goldgelb
\definecolor{UniViolet}{HTML}{6A3D9A}   %Violett

\definecolor{abbrev}{RGB}{255,153,153}             %Rot für Tags
\definecolor{add}{RGB}{204,255,238}                %Hellgrün/Türkis für Tags
\definecolor{sic}{RGB}{255,255,153}                %Hellgelb für Tags
\definecolor{unclear}{RGB}{255,230,184}            %Hellorange für Tags
\definecolor{date}{RGB}{153,153,255}               %Blau für Tags
\definecolor{organization}{RGB}{255,153,255}       %Pink für Tags
\definecolor{place}{RGB}{204,153,255}              %Lila für Tags
\definecolor{person}{RGB}{153,255,153}             %Hellgrün für Tags
\definecolor{signature}{RGB}{153,255,153}          %Hellgrün (gleiche Farbe wie Person) für Tags
\definecolor{eventTag}{HTML}{05A9FF}               %Blau für Tags
\definecolor{oldLetter}{RGB}{246,238,227}          %Beige für Hintergrund  
\definecolor{vscode-blue}{HTML}{569CD6}            %VSCode Blau für python
\definecolor{vscode-yellow}{HTML}{DCDCAA}          %VSCode Gelb für python
\newcommand{\code}[1]{\colorbox{VeryLightGray}{\texttt{#1}}} %für die darstellung von Code 

\usepackage{array}
\usepackage{minted}
\usepackage{capt-of}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage[hyphens]{url}
\usepackage{ragged2e}
\usepackage[
  hidelinks,
  unicode=true,
  hyperfootnotes=true,
  colorlinks=true,
  linkcolor=Blue,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=Blue,
  pdfborder={0 0 0}
]{hyperref}
\usepackage{nameref}
\newcommand{\parref}[1]{Abschnitt \nameref{#1}}
\usepackage{orcidlink}
\usepackage{iftex}
\ifPDFTeX%
  \usepackage[utf8]{inputenc}
  \usepackage{lmodern}
  % Authblk, pdfpages etc. hier einbinden
\else
  \usepackage{fontspec}
\fi


% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

% Paragraph spacing configuration depending on the class
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
 }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{0pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother


\usepackage[lmargin=2.5cm,rmargin=2.5cm,tmargin=2cm,bmargin=2cm]{geometry}
\setlength{\emergencystretch}{3em}%prevent overfull lines
% \setcounter{secnumdepth}{-\maxdimen}%remove section numbering %Kapitel Zahl Nummer
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph%
  \renewcommand{\paragraph}{
    \@ifstar%
      \xxxParagraphStar%
      \xxxParagraphNoStar%
 }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph%
  \renewcommand{\subparagraph}{
    \@ifstar%
      \xxxSubParagraphStar%
      \xxxSubParagraphNoStar%
 }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother
\makeatletter
% Neue Definition für \paragraph
\renewcommand\paragraph{\@startsection{paragraph}{4}{0em}%
  {1.5ex \@plus1ex \@minus.2ex}% Abstand davor
  {0.5ex \@plus.2ex}% Abstand danach
  {\normalfont\normalsize\bfseries\itshape}}

% Neue Definition für \subparagraph (optional)
\renewcommand\subparagraph{\@startsection{subparagraph}{5}{0em}%
  {0ex \@plus 0ex}% kein Abstand davor
  {0.5ex \@plus.2ex}% Abstand danach
  {\normalfont\normalsize\itshape\underline}}

\makeatother



\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc}%for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
%allow citations to break across lines
 \let\@cite@ofmt\@firstofone%
%avoid brackets around text for\cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa, #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2]%#1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
 %turn on hanging indent if param 1 is 1
  \ifodd #1 \else%chktex 1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
 %set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth-\csllabelwidth}{\strut#1\strut}} %chktex 8
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname%
  \renewcommand*\contentsname{Inhaltsverzeichnis}
\else
  \newcommand\contentsname{Inhaltsverzeichnis}
\fi
\ifdefined\listfigurename%
  \renewcommand*\listfigurename{Abbildungsverzeichnis}
\else
  \newcommand\listfigurename{Abbildungsverzeichnis}
\fi
\ifdefined\listtablename%
  \renewcommand*\listtablename{Tabellenverzeichnis}
\else
  \newcommand\listtablename{Tabellenverzeichnis}
\fi
\ifdefined\figurename%
  \renewcommand*\figurename{Abbildung}
\else
  \newcommand\figurename{Abbildung}
\fi
\ifdefined\tablename%
  \renewcommand*\tablename{Tabelle}
\else
  \newcommand\tablename{Tabelle}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\providecommand{\listoflistings}{\listof{codelisting}{Listingverzeichnis}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands%
\def\languageshorthands#1{}
\usepackage{bookmark}

\usepackage{placeins}                  %für \FloatBarrier
\usepackage[toc,page]{appendix}        %für eigene Anhangs-Umgebung
% optional:
% \usepackage{pdfcrypt}                  %für PDF-Passwortschutz

\IfFileExists{xurl.sty}{\usepackage{xurl}}{}%add URL line breaks if available
\urlstyle{same}%disable monospaced font for URLs
\hypersetup{
  pdftitle={Konzeption für AG Masterarbeit am
17.01.2025},
  pdfauthor={Sven Burkhardt},
  pdflang={de},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}
}

\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\title{\vspace*{4cm} \LARGE Digitale Harmonie aus historischer Dissonanz
\color{UniMint} \rule{8cm}{1pt} \\  
\vspace{0.2cm}  
\color{white}\large Extraktion, Ordnung und Analyse\\unstrukturierter Archivdaten\\des Männerchor Murg}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{}
\author{Sven Burkhardt}
\date{2025-08-14}%chktex 8

%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––– T I T E L B L A T T   ––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
\begin{document}
\begin{titlepage}
    
% Setzt die Schriftfarbe auf Weiss
\color{white}
\pagecolor[HTML]{46505A} %Seitenfarbe in Uni Basel Anthrazit D20537 (rot)
\pagenumbering{gobble}   %Verhindert die Anzeige der Seitennummer auf dem Titelblatt
\date{}
\author{}
\maketitle
\begin{center}
  \author{\LARGE{\author{\vspace{-0.5cm}Sven Burkhardt}}}\\
  \vspace{4mm}
  \large{\orcidlink{0009-0001-4954-4426} {0009-0001-4954-4426}}\\%chktex 8%Orcid Link und Nummer
  \begin{figure}[h]
    \centering
    \color{white}
    \large{\href{https://dhlab.philhist.unibas.ch/en/persons/sven-burkhardt/}{{\hspace*{0.5mm}\includegraphics[height=4.5
  mm]{./assets/Logos/Uni_basel_logo_white.png}}\hspace{3.4mm}\color{white} 17-056-912}}\\%chktex 8 %logo Unibas + Link + Immatrikulationsnummer
    \faIcon[regular]{calendar-alt}\date{\hspace*{2mm}15.08.2025}% chktex 8
  \end{figure}
  \setcounter{figure}{0}
\end{center}


% ------------ Hexagon grafik beginn -----------
\centering
\AddToShipoutPictureBG*{%
    \put(0,-40){%
        \includegraphics[width=\paperwidth]{./assets/Logos/Hexagon_Deko}
  }
}
\centering
\AddToShipoutPictureBG*{%
    \put(0,810){%
        \includegraphics[width=\paperwidth]{./assets/Logos/Hexagon_Deko}
   }
}
\centering
\AddToShipoutPictureBG*{%
    \put(33,752){%
        \includegraphics[width=\paperwidth]{./assets/Logos/Hexagon_Deko}
   }
}
\centering
\AddToShipoutPictureBG*{%
    \put(-99,752){%
        \includegraphics[width=\paperwidth]{./assets/Logos/Hexagon_Deko}
   }
}



\noindent%Verhindert Einzug des nachfolgenden Textes
% ------------ Hexagon grafik ende -----------



\begin{center}
    \vfill
    \begin{figure}
        \centering
        \begin{subfigure}{.3\textwidth}
          \centering
          \includegraphics[width=.8\linewidth]{./assets/Logos/uni-basel-logo-en_white.png}
        \end{subfigure}%
        \begin{subfigure}{.3\textwidth}
          \centering
          \includegraphics[width=.8\linewidth]{./assets/Logos/dhlab-logo-white.png}
        \end{subfigure}
    \end{figure}
        \setcounter{figure}{0}

    University of Basel\\
    Digital Humanities Lab\\
    Switzerland
\end{center}


\newpage
\color{black}          %Setzt die Schriftfarbe auf Schwarz für die folgenden Seiten
\setstretch{1.5}
\end{titlepage}
\newpage
%________________

%________________

%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––   A B S T R A C T     ––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%


\pagecolor{white}  
\color{black}  %Textfarbe zurücksetzen
\color{black} 
\setstretch{1.5}


\newpage 
Diese Arbeit befasst sich mit dem Archiv des \textit{Männerchor Murg} in den Jahren des Zweiten Weltkrieges. 
Hierfür wird eine automatisierte Pipeline auf Basis von LLMs und Patternmatching vorgestellt, mit deren Hilfe Named Entities extrahiert und weiterverarbeitet werden. Der Hauptfokus der Weiterverarbeitung liegt auf der Ausgestaltung des Entity Matching, um erkannte Entitäten mit einer Groundtruth-Tabelle abzugleichen. 
Ziel ist es, dieses Archiv digital zugänglich und die beteiligten Personen sowie deren Netzwerke sowie die geographische Ausdehnung sichtbar zu machen.



%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––      T A B L E        ––––––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––        O F            ––––––––––––––––––––––––––––––––––%
%–––––––––––––––––––––––––––––   C O N T E N T S     ––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%



\newpage
\renewcommand*\contentsname{Inhaltsverzeichnis}%This controls the title of your table of contents.
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{4}%Sets the maximum sublevel to be displayed within the table of contents.
\tableofcontents
}
\pagenumbering{arabic}\setstretch{1.5}%Overwrites the previous command, pages are counted as normal from this point.


%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––      I N T R O D U C T I O N    ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
\newpage
\section{Einleitung}
\subsection{Ziel und Relevanz der Arbeit}
\subsection{Formulierung der Forschungsfrage}
\subsection{Aufbau der Arbeit}
\subsection{Geografischer und historischer Kontext}
Die vorliegende Arbeit stützt sich auf Unterlagen aus dem Archiv des \enquote{Männerchor Murg} dessen Nachfolge im Jahr 2021 
durch die \enquote{New Gospelsingers Murg} angetreten wurde. Murg ist eine deutsche Gemeinde am Hochrhein, 
rund 30 km Luftlinie von Basel entfernt. Der Ort liegt am gleichnamigen Fluss Murg, der in den Rhein mündet. 
Beide Gewässer bildeten über Jahrhunderte hinweg den wirtschaftlichen Motor der Region: Die Wasserkraft der Murg 
begünstigte früh die Ansiedlung von Mühlen, Hammerwerken und Schmieden entlang des Bachlaufs, während der Rhein mit seiner 
Drahtseil-Fähre eine bedeutende Verkehrs- und Handelsverbindung bot, die bis zum Ersten Weltkrieg privat betrieben wurde.

Mit dem Ausbau der Landstrasse, der heutigen Bundesstrasse 34, sowie dem Anschluss an die Bahnstrecke Basel–Konstanz 
entwickelte sich Murg im 19. Jahrhundert von einer landwirtschaftlich geprägten Siedlung zu einer Gewerbe-, Handels- und 
Industriegemeinde. Die Wasserkraft wurde dabei zu einem entscheidenden Standortfaktor: Die Ansiedlung der Schweizer 
Textilfirma \textit{Hüssy \& Künzli AG} im Jahr 1853\cite[vgl.][]{gemeinde_murg_geschichte_nodate} trug wesentlich 
zum wirtschaftlichen Wachstum der Gemeinde bei. Zahlreiche Arbeitskräfte, vor allem aus der benachbarten Schweiz, 
machten Murg zu einem wichtigen Standort der regionalen Textilindustrie.

Die Gründung des \textit{Männerchor Murg} im Jahr 1861 durch Schweizer Textilarbeiter belegt diesen engen Zusammenhang 
zwischen wirtschaftlicher Migration, Industrialisierung und lokalem Vereinswesen. Diese historische Verflechtung bildet 
eine zentrale Grundlage für die vorliegende Untersuchung.


  

\newpage
\section{Forschungsstand und Forschungslücke}\label{subsec:forschungsstand}

Die vorliegende Arbeit knüpft hauptsächlich an zwei Vorarbeiten an, die in den Jahren 2019 und 2022 am Departement Geschichte der Universität Basel durchgeführt
werden. In drei Digital History-Seminaren\cite[vgl.][]{hodel_machine_2019, serif_digital_2019, serif_von_2022} mit Fokus auf Transkribus werden erste Teilbestände der \enquote{Männerchor Akten 1925--1944} erschlossen und in einem Korpus
von 137 Einzeldokumenten zusammengeführt.\cite[vgl.][]{burkhardt_feldpost_2022}
Ein kleinerer Korpus von rund 50 Dokumenten wird mit Metadaten versehen. Erfasst werden unter anderem die genaue Position im Ordner auf Seitenebene, 
Kurztitel und Entstehungsdatum. Diese Metadaten bilden die Grundlage für eine erstmalige systematische Erschliessung.

Während in einem frühen Projektschritt vorrangig häufig genannte Personennamen (\enquote{Carl Burger}, \enquote{Fritz Jung})
dokumentiert werden, richtet sich der Fokus im zweiten Schritt auf die Feldpost. Ziel ist es, über die Auswertung der Feldpostnummern
Rückschlüsse auf beteiligte Militäreinheiten, deren Stationierungen und Verlagerungen während des Zweiten Weltkriegs zu ziehen.

Für diese Recherchen kommen einschlägige Fachliteratur zu den jeweiligen Fachgebieten zum Einsatz. 
Hier sind besonders die Bücher von Alex Buchner\cite[vgl.][]{buchner_handbuch_1989}, Christian Hartmann\cite[vgl.][]{hartmann_wehrmacht_2010}, 
Werner Haupt\cite[vgl.][]{haupt_buch_1982}, Christoph Rass\cite[vgl.][]{rass_deutsche_2009}, Georg Tessin\cite[vgl.][]{tessin_verbande_1977} 
und Christian Zentner\cite[vgl.][]{zentner_illustrierte_1983} zu nennen.

Darüberhinaus werden eigene Recherchen in den Beständen des \textit{Bundesarchivs – Militärarchiv Freiburg}\cite{hollmann_freiburg_2025} durchgeführt. Ergänzende Recherchen 
stammen aus den Suchlisten des \textit{Deutschen Roten Kreuzes (DRK)}\cite{reuter_drk_2025}. Hinzu kommen philatelistische Übersichts-Websites\cite{noauthor_feldpost_nodate}, 
die bei der Entzifferung von Briefmarken und Stempeln helfen.
Absoult essentiell für den Erfolg dieser Recherchen sind Citizen-Science-Foren\footnote{vor Allem werden verwendet: 
\textit{Forum der Wehrmacht}~\parencite{hermans_forum_nodate} und das \textit{Lexikon der Wehrmacht}~\parencite{altenburger_lexikon_nodate}.}. Sie ergänzen und validieren eigene Forschung.

Parallel zur inhaltlichen Erschliessungen entsteht 2022 eine erste digitale Storymap mit \textit{ArcGIS},
die zentrale Ergebnisse des Projekts öffentlich zugänglich macht. Grundlage bildet die Sichtung, konservatorische Aufbereitung und Digitalisierung
von zunächst rund 30 der etwa 800 Seiten Vereinsakten. Der Teilkorpus wird entheftet, gescannt und mit Metadaten wie Absender, Datum,
Feldpostnummer und Einheit versehen. Da jedes Dokument einen anderen Verfasser aufweist, erfolgt die Transkription manuell. Eine automatische Handschriftenerkennung 
ist aufgrund der heterogenen Schriftbilder nicht praktikabel.
Am Beispiel einzelner Sänger wie \textit{Emil Durst} lässt sich durch die Rechercheergebnisse mithilfe der Feldpostnummern und ergänzender Kartenmaterialien der Aufenthaltsort
bis auf Gebäude oder wenige Meter genau rekonstruieren. Diese Erkenntnisse werden mit historischen Karten, Luftbildern und Ortsrecherchen verknüpft
und in einer interaktiven ArcGIS-Karte visualisiert, die Stationierungen, Märsche und Frontverschiebungen der Chormitglieder anschaulich darstellt. 

Die in diesen Vorprojekten erarbeiteten Listen, Geodaten, Transkriptionen und Visualisierungen fliessen in die vorliegende Arbeit ein
und bilden eine wesentliche Grundlage für die erweiterte, automatisierte Pipeline, die im Folgenden vorgestellt wird. Dazu gehören beispielsweise auch die Verbandsabzeichen, 
Taktische Zeichen\cite[vgl.][S.64-66]{haupt_buch_1982} der jeweiligen Einheiten, die auch in die Groundtruth der vorliegenden Arbeit inkorporiert werden.

Abgesehen von diesen Vorarbeiten ist der Quellenkorpus wissenschaftlich unerschlossen. Mit dieser Arbeit liegt erstmals eine
umfassendere wissenschaftliche Auswertung vor.

Mit der notwendigen manuellen Recherche in oben dargelegten Datenbankstrukturen wird zugleich sichtbar, wie sehr es an Brücken fehlt, 
um unterschiedliche Klassifikationen, fachspezifische Ordnungslogiken und semantische Webtechnologien nachhaltig miteinander zu verbinden. Ein verhältnismässig einfaches
Webscraping nach Informationen zu diesem Korpus ist nahezu unmöglich. Ausgeführt werden diese Probleme beispielsweise bei 
Smiraglia und Scharnhorst (2021)\cite[vgl.][]{richard_linking_2022}, 
 die anhand konkreter Fallstudien verdeutlichen, wie fragmentiert semantische Strukturen bislang entwickelt werden 
 und welche Hürden bei der praktischen Verknüpfung heterogener Wissensorganisationen bestehen. Dabei benennen sie insbesondere die 
 Herausforderungen bei der Übersetzung historisch gewachsener Klassifikationen in standardisierte semantische Formate, 
 die Notwendigkeit dauerhafter technischer Wartung und die Abhängigkeit von nachhaltigen Infrastruktur-Partnern\cite[vgl.][Kap. 2 und 5]{richard_linking_2022}.

Für eine Einordnung zu historischen Netzwerkanaylsen sei auf Gamper\&Reschke\cite{gamper_knoten_2015} verwiesen. Der Sammelband
\textit{Knoten und Kanten III} verdeutlicht, dass die historische Netzwerkanalyse zwar von einem interdisziplinär etablierten Methodenkanon 
profitiert, jedoch nach wie vor vor erheblichen Herausforderungen steht. Dazu zählen die Fragmentierung historischer Quellen, der hohe manuelle 
Erfassungsaufwand und methodische Desiderate im Umgang mit zeitlichen und räumlichen Dimensionen. Erschwerende Faktoren einer systematische Erfassung 
relationaler Strukturen. Dennoch eröffnen netzwerkanalytische Verfahren – besonders im Zusammenspiel mit relationaler Soziologie 
und Figurationsansätzen – neue Perspektiven auf Macht, Abhängigkeiten und Akteurskonstellationen in historischen Gesellschaften.






%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––              Korpus            ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––% 
\section{Korpus}\label{section:Korpus}
Aus dem Bestand des Ordners \textit{``Männerchor Akten 1925--1944''} werden für diese Arbeit ausschliesslich Akten verwendet, 
die während des Zweiten Weltkriegs verfasst wurden. Der Analysezeitraum erstreckt sich dementsprchend zwischen dem 01. September 1939 
und dem 8. Mai 1945, dem Tag der bedingungslosen Kapitulation Deutschlands.

Die zeitliche Eingrenzung ist notwendig, um die Funktionalität der im Folgenden beschriebenen Pipeline 
in einem klar definierten historischen Kontext demonstrieren zu können. Gleichzeitig führt TeXshop?diese Auswahl zu einer 
bewussten Reduzierung der potenziell erfassten Akteurinnen und Akteure, Orte und Organisationen. Diese Fokussierung ist 
insbesondere im Hinblick auf die Erstellung einer verlässlichen Groundtruth bedeutsam, die durch ergänzende Archivrecherchen 
mit historischen Metadaten angereichert wird.

Die Kombination aus einer präzise definierten Quellengrundlage und der digitalen Anreicherung dient dazu, das 
Potenzial der computergestützten Auswertung historischer Dokumente exemplarisch aufzuzeigen. 
Zugleich unterstreicht sie, dass die Qualität der Ergebnisse wesentlich von der sorgfältigen Eingrenzung 
des Korpus und der manuellen Validierung und Anreicherung abhängt.
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––              Quellen            ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%

\subsection{Quellen}
\subsubsection{Quellentradierung}
In den Lagerräumen der New Gospel Singers Murg, dem Nachfolgeverein des Männerchors Murg, 
wird im Jahr 2018 mehrere je ca. 800 Seiten umfassende Ordner mit historischen Unterlagen gefunden. 
Für diese Arbeit wird ein Ordner mit der Aufschrift \textit{\enquote{Männerchor Akten 1925--1944}} gewählt, da er neben dem Ordner 
\textit{\enquote{Männerchor Akten 1946--1950}} den grössten Zeitraum abdeckt. Darüberhinaus bietet er das Potential, 
aufschlussreiche Einblicke in das Vereinsleben in der Zeit vor und während des Nationalsozialismus, insbesondere des Zweiten Weltkrieges, zu geben.\\ 
Der Ordner umfasst insgesamt 780 Seiten und deren Inhalt kann als \enquote{Protokoll}, \enquote{Brief}, \enquote{Postkarte}, \enquote{Rechnung}, 
\enquote{Regierungsdokument}, \enquote{Noten}, \enquote{Zeitungsartikel}, \enquote{Liste}, \enquote{Notizzettel} oder \enquote{Offerte} kategorisiert werden.

Die Unterlagen könnten bereits direkt nach ihrer Entstehung in die Ordner eingelegt worden sein. Einzelne Akten sind mit einem 
\enquote{Heftstreifen}, auch \enquote{Aktendulli} genannt, zusammengefehtet. In der Plastikversion, wie er in diesen Akten vorliegt, wurde er bereits 1938 patentiert\cite[vgl.][]{noauthor_heftstreifen_2023}.
Wer die Akten so archiviert hat lässt sich nicht mehr sagen. 
Der sogenannte \enquote{Archival-Bias} des Archivars, also die Grundeinstellung, weshalb etwas aufbewahrt oder
vernichtet wurde, lässt sich damit nicht mehr feststellen. 

\subsubsection{Quellenbeschrieb}\label{Dokumententypen}
\noindent
\begin{minipage}[t]{0.49\textwidth}
  \setstretch{1.5}
  \justifying
  Für diese Arbeit wurde ein Korpus selektiert, dessen Auswahl in~\nameref{section:Korpus} näher beschrieben wird. 
  Erfasst, benannt und tabellarisch mit groben Metadaten versehen werden sämtliche Unterlagen aus dem Ordner \textit{\enquote{Männerchor Akten 1925--1944}}. 
  Diese Auflistung in der Datei \textit{Akten\_Gesamtübersicht.csv} erlaubt die Zuordnung zu folgenden Kategorien: Briefe, Postkarten, Protokolle, Regierungsdokumente, Zeitungsartikel, 
  Rechnungen und Offerten. Die Verteilung ist ungleichmässig: Briefe bilden mit 282 von 381 Seiten die grösste Kategorie; Rechnungen und Offerten sind jeweils nur einseitig vertreten.
\end{minipage}%
\hfill
\begin{minipage}[t]{0.49\textwidth}
  \centering
  \vspace*{0.3cm} % Feinjustierung nach Bedarf
  \begin{tikzpicture}
    \begin{axis}[
      ybar,
      bar width=16pt,
      height=5.5cm,
      ymin=0,
      ylabel={Anzahl Dokumente},
      symbolic x coords={Brief, Postkarte, Protokoll, Regierungsdok., Zeitungsartikel, Rechnung, Offerte},
      xtick=data,
      nodes near coords,
      nodes near coords align={vertical},
      x tick label style={rotate=45, anchor=east},
      axis lines=left,
      enlarge x limits=0.15,
      grid=major,
      grid style={dashed, gray!30},
    ]
      \addplot[fill=gray!60] coordinates {
        (Brief,282) 
        (Postkarte,51) 
        (Protokoll,36) 
        (Regierungsdok.,5) 
        (Zeitungsartikel,4) 
        (Rechnung,1) 
        (Offerte,1)
      };
    \end{axis}
  \end{tikzpicture}
  \captionof{figure}{Verteilung der Dokumententypen im untersuchten Bestand (150 Akten – 381 Seiten).}\label{fig:dokumententypen-bar}
\end{minipage}

\noindent
\begin{minipage}[t]{0.48\textwidth}
  \setstretch{1.5}
  \justifying
  Auf Grundlage der oben erwähnten, händisch erstellten Gesamtliste der Akten können durch 
  die systematische Benennung der Dokumente auch Rückschlüsse auf den bislang nicht untersuchten 
  Teil des Bestandes gezogen werden. Mithilfe des Sprachmodells von OpenAI wurde eine grobe Näherung zur 
  Zusammensetzung des restlichen Korpus erarbeitet, wie sie in der rechtsstehenden Darstellung visualisiert ist.
  
  Diese Übersicht erhebt keinen Anspruch auf Vollständigkeit oder Genauigkeit, sondern dient der Veranschaulichung, 

\end{minipage}%
\hfill
\begin{minipage}[t]{0.48\textwidth}
  \centering
  \vspace*{0.2cm} % Feinjustierung der vertikalen Ausrichtung
  
  \begin{tikzpicture}
    \begin{axis}[
      ybar,
      bar width=16pt,
      height=5.5cm,
      ymin=0,
      ylabel={geschätzte Verteilung restlicher Korpus},
      symbolic x coords={Briefe, Postkarten, Protokolle, Regierungsdok., Zeitungsartikel, Rechnungen, Offerten},
      xtick=data,
      nodes near coords,
      nodes near coords align={vertical},
      x tick label style={rotate=45, anchor=east},
      axis lines=left,
      enlarge x limits=0.15,
      grid=major,
      grid style={dashed, gray!30},
    ]
      \addplot[fill=gray!60] coordinates {
        (Briefe,155) 
        (Postkarten,12) 
        (Protokolle,7) 
        (Regierungsdok.,60) 
        (Zeitungsartikel,25) 
        (Rechnungen,8) 
        (Offerten,12)
      };
    \end{axis}
  \end{tikzpicture}
  \captionof{figure}{geschätzte Verteilung der Dokumententypen im restlichen Bestand.}\label{fig:dokumententypen-schätzung}
\end{minipage}

\vspace{1em}

\noindent
dass die Verteilung der Quellengattungen im Zeitraum vor dem Zweiten Weltkrieg möglicherweise deutlich anders gestaltet ist. Eine vertiefte Untersuchung dieser bislang unbearbeiteten Bestände erscheint daher notwendig und markiert eine zentrale Forschungslücke, die im Rahmen dieser Arbeit erstmals systematisch benannt wird.

\subsubsection{Sichtung \& Kategorisierung in Akten}
Für diese Arbeit werden alle Seiten in dem Ordner \textit{\enquote{Männerchor Akten 1925--1944}} 
zwei mal gesichtet und gelesen. Beim ersten Durchgang werden explorativ nach zusammenhängenden Unterlagen gesucht, die im Anschluss zu einer Akten gefasst werden können.

Ausschlaggebend für eine Zusammenfassung in einer Akte sind folgende Faktoren:
\begin{itemize}
  \item Historische Gliederung durch Bindung (Büroklammern, Heftstreifen, etc)
  \item gleiche Autorenschaft in direkt aufeinanderfolgenden Seiten
  \item gleiches Datum ["~"~"~"]
  \item gleiches Thema ["~"~"~"]
\end{itemize}
Auf dieser Grundlage wird eine Aktenübersicht\footnote{genannt Akten\_Gesamtübersicht.csv; in den Projektdaten} im CSV-Format erstellt. Sie seztt sich zusammen aus der Aktennummer, die die Reihenfolge innehalb des ursprünglichen Ordners beschreibt. In diesem ersten Schritt gilt die Lage als 
Identifikator für die Unterlagen. Jeder Nummer wird darüber hinaus ein beschreibender Titel, und das Erstellungsdatum zugewiesen.

Vorgreifend soll auch die zweite Quellensichtung beschrieben sein, in der diese Daten in der CSV um Metadaten auf Seitenebene und aus Transkribus ergänzt werden. Die Kategorisierung findet also in einem parallelen Prozess mit der Digitalisierung statt.
Hierfür werden die Akten auf Seitenebene genau ausgebaut. Dem zugrunde liegt eine internen Seiten-ID, die den Aktennamen und die Position innerhalb der Akte kombiniert (Bsp: Akt\_078\_S001.jpg). 
Ab dem Zeitpunkt des Uploads bei Transkribus wird diese jedoch durch Traskribus-Dokument-ID abgelöst. Beide IDs werden zu besseren Nachvollziehbarkeit in der CSV notiert.

Auch inhaltlich wird nochmals schärfer kategorisiert. Mit Tags in Apple-Dateien und der CSV wird nun folgendes erfasst:
\begin{itemize}
  \item Handschrift
  \item Maschinenschrift
  \item Bild
  \item Signatur
\end{itemize}
Die in \nameref{Dokumententypen} dargestellten Kategorisierungen werden nun als Groundtruthdaten in die CSV aufgenommen, um sie später in der Pipeline auszuwerten.

\subsection{Digitalisierung der Quellen}

Überlieferte analogen Dokumente müssen zunächst fachgerecht für das Projekt und den Digitalisierungsprozess aufbereitet werden. 
Hierzu werden die Akten aus ihren ursprünglichen Ablagesystemen entnommen und sorgfältig von Heftklammern, Büro- und Gummibändern 
befreit. Diese konservatorischen Massnahmen sind notwendig, um die langfristige Materialerhaltung zu gewährleisten, 
da insbesondere Korrosionsspuren ehemaliger Metallklammern die Papierfasern nachhaltig schädigen können. Zudem finden sich häufig 
Anzeichen von Säurefrass, sofern nicht säurefreies Archivmaterial verwendet wurde.

Für die eigentliche Digitalisierung kommt die native \enquote{Dateien}-Applikation von 
Apple\footnote{vgl.\href{https://support.apple.com/de-de/guide/preview/prvw28034/mac}{Apple Support: Dateien-App}} zum Einsatz. 
Diese bietet neben einer vergleichsweise hochauflösenden Erfassung die Möglichkeit zur direkten Speicherung in einem Cloud-basierten 
Speichersystem sowie eine automatische Texterkennung (OCR). Ziel dieser Vorgehensweise ist es, die digitalisierten Inhalte möglichst 
schnell durchsuchbar zu machen und standortunabhängig für das Projekt zugänglich zu machen.

Die Aufnahme der Dokumente erfolgt mithilfe eines Tablets, das auf einem stabilen Stativ exakt im rechten Winkel (90\textdegree)
über dem zu digitalisierenden Schriftgut positioniert wird. Diese einfache, jedoch effiziente Konfiguration gewährleistet eine 
gleichbleibend hohe Bildqualität bei gleichzeitig hoher Verarbeitungsgeschwindigkeit. Die digital erfassten Dateien werden konsistent
 benannt und folgen einer vorab definierten Gesamtübersicht der Bestände. Mehrseitige Konvolute werden dabei als zusammengehörige 
 Akteneinheiten geführt, während Einzeldokumente entsprechend separat erfasst werden. Die Archivierung erfolgt sowohl analog als auch 
 digital auf Seitenebene, um eine möglichst feingranulare Erschliessung zu ermöglichen.

Die initiale Speicherung erfolgt dabei standardmässig im PDF-Format. Für die anschliessende Verarbeitung mit den unten dargestellten 
Transkriptionswerkzeugen müssen die Dokumente jedoch in das JPEG-Format konvertiert werden. Die Umwandlung erfolgt automatisiert 
mithilfe eines eigens erstellten Python-Skripts, wie in Anhang~\ref{section:PDF_to_JPEG} beschrieben.\cite{burkhardt_githubpdf_to_jpegpy_2025}
Es extrahiert die Seiten, speichert im geeigneten 
Format ab und ergänzt die Dateinamen systematisch um eine dreistellige, führend nullengefüllte Seitennummer.

\subsection{Transkription}
Nach der Digitalisierung und Konvertierung der Dokumente beginnt die eigentliche Transkription. 
Wie im Kapitel \nameref{section:Transkriptionen_Methoden} dargestellt, 
entscheidet sich dieses Projekt bewusst für Transkribus als zentrale Plattform. 
Ausschlaggebend sind insbesondere die Möglichkeit, ein eigenes HTR-Modell auf Basis einer projektspezifischen Groundtruth zu trainieren, 
sowie die integrierten Funktionen zur Annotation von Named Entities direkt im Transkriptionsprozess.
Für die effiziente Transkription soll im Folgenden der Workflow beschrieben werden, der einen Mixed Method Ansatz verfolgt. 

\begin{minipage}[t]{0.52\textwidth}
  \setstretch{1.5}
  \justifying
  \noindent
Wie das Beispiel Abbildung \nameref{fig:Carl-handschrift} rechts verdeutlichen soll, sind viele der Akten schwer entzifferbar. Auch Transkribus kommt wegen der kleinen
Sets unterscheidlicher Autoren und Autorinnen für spezifische Handschrift an seine Grenzen. Mit Expertise sind diese nur mit hohem zeitlichen Aufwand transkribierbar. 
Die Baselines\footnotemark~ist in diesem Beispiel verhältnismässig homogen. Schwierigkeiten bereiten jedoch Postkarten oder Zeitungsartikel, die mit einer komplexeren 
Schrift-Setzung einen höheren Aufwand in der Transkription benötigen. 
\noindent
\end{minipage}%
\hfill%
  % Rechte Minipage = Bild
  \begin{minipage}[t]{0.43\textwidth}
    \centering
    \vspace*{0cm} % ► schiebt alles um 0.3cm nach unten
  \includegraphics[width=0.8\textwidth]{./assets/Images/Akte_076_S001.jpg}
  \captionof{figure}{Beispiel für handschriftlichen Text in Akte\_076}\label{fig:Carl-handschrift}
\end{minipage}
\vspace{1em}
\footnotetext{Baselines = Schriftausrichtung}

Ausgangspunkt für dieses Projekt ist das generischen Modell \textit{The German Giant I}, 
das mit einer CER\footnote{\textbf{CER} (Character Error Rate): Kennzahl für die Anzahl falsch erkannter Zeichen.} von 8,30\% zunächst auf 70 Akten angewendet wird. 
Sie umfassen 158 Seiten mit insgesamt 22.155 Wörtern. Die Ergebnisse
sind dabei jedoch sehr unpräzise, wie Abbildung \nameref{fig:Carl-Transkribus} veranschaulicht. 
In insgesamt vier Durchläufen über diese Selektion wird daher manuell eine Groundtruth für ein eigenes Modell erstellt und gleichzeitig regelbasiert und strukturiert Personen, Orte, Daten und
Organisationen getaggt. Zwar erweist sich ChatGPT in der direkten Texterkennung aus Bilddateien (OCR) als nicht ausreichend zuverlässig. 




\begin{minipage}[t]{0.52\textwidth}
  \setstretch{1.5}
  \justifying
  \noindent
Wie oben ausgeführt wird zur 
Erstellung des Groundthruth-Korpus bei der manuellen Korrektur OpenAIs ChatGPT-4o-Modell für die Rechtschreibprüfung verwendet. Die vermeintliche Schwäche 
bei der Transkription, passende Begriffe zu halluzinieren, stellen sich als besonders hilfreich heraus. Insbesondere in der Rekonstruktion fehlender Worte oder 
Satzteile aus dem semantischen Kontext heraus Wörter oder Satzteile. 
In Kombination mit der philologischen Expertise bei der Entzifferung einzelner Buchstaben entsteht so ein kollaborativer Transkriptionsprozess, 
bei dem Maschine und Mensch sich wechselseitig ergänzen. Die automatische Transkription wird in \autoref{fig:Carl-Transkribus} dargestellt, die überarbeitete LLM-Version folgt in \autoref{fig:Carl-LLM}.
\end{minipage}
\vspace{1em}
\begin{minipage}[t]{0.5\textwidth}
    \centering
    \vspace*{0cm}
  \begin{tcolorbox}[colback=oldLetter, colframe=black, sharp corners, width=0.8\textwidth]
    \tiny{Murg. 15. Aug 41 
    
    Mein lieber Alfons! 

    Sehen lunge Lreitt es mich dem Männer- 

    chor wieder einmal ein Liedehen zu stehten. 

    und kam mir die gestege Gelegenheit gussend. 

    Männechor Venstad um den Title das Liedchen 

    zu erhalten, wo sie zum Abschied am Aute 

    sängen ``auf Wiederschen Owohl ich Frei! 

    märke beifügte, keine Aentwarb. Vielleicht 

    gelingt es Dir diesen Iitel zu erhalten. 
    
    Weiterhin sänge ich fal Lied nur 
    
    ``Bas alte Lied'' von being. Rerohl 

    Es wurde 1928 am 10. Dachub. Sängerb. Frst 

    von Begrüssungsabend in Dien gesungen. 

    und erntete überaus grossen Reifall. 

    Es ich schwer das Richtige zu finden. 

    Aler Alfon, werst das Vemsladler Liedchen. 

    alsdann das Biener Lidchen und wenn 

    Leides unmöglich, dann freu Nall. 

    Mit herzl. Grüsse 

    Dein 

    Carl} 
  \end{tcolorbox}
  \captionof{figure}{LLM-Version von \autoref{fig:Carl-handschrift}\label{fig:Carl-Transkribus}}
\end{minipage}
\hfill%



\begin{minipage}[t]{0.52\textwidth}
  \setstretch{1.5}
  \justifying
  \noindent
Die so entstehnde Groundtruth wird für das Training des Modells
(\href{https://app.transkribus.org/models/public/287793}{ModelID: 287793})\footnotemark{} verwendet.
Dieses trainierte Modell erreicht eine CER von 6,58\% und kommt anschliessend für die automatische Transkription der übrigen Dokumente zum Einsatz. Auch hier ist eine manuelle Überprüfung der durch das eigens trainierte Modell erstellten
Transkription unabdingbar, die knapp 1.7\% geringere CER macht sich jedoch beim Korrekturaufwand bereits bemerkbar. Gleichzeitig wird diese Korrektur für das Taggen benutzt, das folgend beschrieben werden soll.

\end{minipage}
\footnotetext{\cite[vgl.][]{burkhardt_transkribus_2024}}
\begin{minipage}[t]{0.5\textwidth}
    \centering
    \vspace*{0cm}
  \begin{tcolorbox}[colback=oldLetter, colframe=black, sharp corners, width=0.8\textwidth]
\tiny{Murg, 15. Aug. 41

Mein lieber Alfons!

Schon lange treibt es mich, dem Männerchor wieder einmal ein Liedchen zu stiften, und kam mir die günstige Gelegenheit gelegen.

Ich schrieb vergangenes Jahr an den Männerchor Venstad, um den Titel des Liedchens zu erhalten, das sie zum Abschied am Auto sangen: \enquote{Auf Wiedersehen, o wohl ich frei!}

Ich fügte eine Frankierung bei, erhielt jedoch keine Antwort. Vielleicht gelingt es Dir, diesen Titel zu erhalten.

Weiterhin sang ich das Lied nur \enquote{Das alte Lied von Wien}. Obwohl es am 10. Dezember 1928 beim Sängerbund-Fest von Begrüssungsabend in Wien gesungen wurde und überaus grossen Beifall erntete, ist es schwer, das Richtige zu finden.

Aber Alfons, zuerst das Venstadler Liedchen, dann das Wiener Liedchen und wenn beides unmöglich, dann Fröhlichsein.

Mit herzlichen Grüssen

Dein

Carl}

\end{tcolorbox}
  \captionof{figure}{Transkription durch ChatGPT von \autoref{fig:Carl-Transkribus}\label{fig:Carl-LLM}}
\end{minipage}
\hfill%




\subsubsection{Tagging mit Transkribus und LLM}

Während der manuellen Korrektur der Transkriptionen erfolgt parallel die Annotation zentraler Entitäten. 
Transkribus bietet hierfür ein flexibles Tagging-System, mit dem sowohl strukturelle als auch semantische Informationen direkt im Dokument markiert werden können. 
Im Zentrum stehen dabei Tags für Personen, Orte, Organisationen und Datumsangaben. Diese Kategorien sind für die spätere Analyse besonders relevant, 
etwa für die Modellierung historischer Netzwerke oder die Kontextualisierung von Ereignissen.

Ein Mixed-Method-Verfahren kommt dort zum Tragen, wo die Transkription an ihre Grenzen stösst: 
Fehlende Buchstaben, fehlerhafte Worttrennungen oder unleserliche Handschriften lassen sich durch die Kombination aus Modellwissen 
und menschlichem Quellenverständnis rekonstruieren. ChatGPT liefert hier auf Basis des Kontexts plausible Vorschläge, 
die von einer historisch geschulten Bearbeitung geprüft und übernommen oder verworfen werden. Dieser kollaborative Vorgang verbessert 
nicht nur die Lesbarkeit, sondern erhöht auch die semantische Genauigkeit der rekonstruierten Passagen.

Ein Beispiel zeigt die schrittweise Entwicklung einer Transkription: Ausgehend von einem gescannten Originalbrief (\autoref{fig:Carl-handschrift}) 
wird zunächst eine maschinelle Transkription erstellt (\autoref{fig:Carl-Transkribus}), die anschliessend durch ein LLM geglättet und lesbarer gemacht wird 
(\autoref{fig:Carl-LLM}).

In einem letzten Schritt erfolgt die manuelle Annotation mit Transkribus-Tags (\autoref{fig:Tagging-Carl-LLM}): 
Hier werden etwa \texttt{Murg} und \texttt{Wien} als Orte, \texttt{Alfons}, \texttt{Carl} und \texttt{Kirchl} als Personen sowie das 
\texttt{Deutsch. Sängerb. Fest} als Ereignis markiert. Auch der Liedtitel \enquote{Das alte Lied von Wien} wird in seinen Bestandteilen 
zwischen Person, Ort und kulturellem Kontext aufgeschlüsselt. Für unklare oder unleserliche Textstellen, wie etwa das Fragment \texttt{auf Wiederschen}, 
kommt das Tag \texttt{unclear} zum Einsatz – häufig auf Grundlage einer Vorschlagsformulierung durch ChatGPT.

Zusätzlich zur semantischen Markierung ermöglicht Transkribus auch die Kennzeichnung struktureller Eigenschaften. 
So wird beispielsweise die Abkürzung \texttt{V.D.A.} – für \textit{Verein für das Deutschtum im Ausland} – mit dem Tag \texttt{abbrev} versehen, 
auch wenn diese Tags in der XML-Exportstruktur teilweise nicht vollständig erhalten bleiben (vgl. Kapitel~\ref{section:Transkriptionen_Methoden}).

Für die spezifischen Anforderungen dieses Korpus wird das Tagging-Schema gezielt erweitert, etwa um den benutzerdefinierten Tag \texttt{signature}, 
der handschriftliche Unterschriften maschinenlesbar ausweist. Das zweite Beispiel – ein poetischer Brief an Otto (\autoref{fig:Tagging-Carl-LLM}, unten) – 
zeigt die Anwendung dieses Verfahrens in lyrischer Sprache. Auch hier werden alle erwähnten Personen (u.\,a. \texttt{Otto}, \texttt{Lina Fingerdick}, 
\texttt{Otto Bollinger}, \texttt{Alfons Zimmermann}), Orte (\texttt{Murg}, \texttt{Laufenburg (Baden)}, \texttt{Rhina}) sowie Organisationen 
(\texttt{Männerchor}) mit den entsprechenden Tags versehen. Die adressierte Funktion \texttt{Vereinsführer des Männerchor} wird dabei 
als Organisationseinheit erfasst und semantisch vom Personenbezug getrennt.

Fehlerhafte oder fehleranfällige Passagen – insbesondere historisch bedingte Schreibungen oder Transkriptionsunschärfen – 
werden mit dem Tag \texttt{sic} versehen. In diesen Fällen folgt die standardisierte Lesart unmittelbar auf das markierte Original, 
wodurch ein differenzierter Umgang mit dem Quelltext sichergestellt ist.

Alle Tags werden während des Transkriptionsprozesses konsistent dokumentiert und in einem projektspezifischen Regelwerk festgehalten. 
Dieses dient nicht nur der internen Nachvollziehbarkeit, sondern auch als Grundlage für die spätere Verarbeitung durch Sprachmodelle, 
die auf die gleichen semantischen Kategorien angewiesen sind. Das strukturierte Tagging bildet somit die Brücke zwischen manueller 
Quellenarbeit und automatisierter Weiterverarbeitung.

% % \begin{itemize}
% %   \item Ausgangslage und technische Bedingungen
% % \item Zeitlicher Verlauf des Digitalisierungsprojekts
% % \item Geräte, Software (iPad, Apple Scan, ökonomische Gründe)
% % \item Mangelnde DH-Vorkenntnisse bei Beginn
% % \item Qualität und OCR-Auswirkungen
% % \item Erste Transkription und Groundtruth-Erstellung
% % \item Nutzung von German Giant I
% % \item Erste Fehleranalyse, CER = 8,3 %
% % \item Auswahl von 70 Akten für manuelle Korrektur
% % \item Iterative Verbesserung in 4 Schritten
% % \item Tagging-Strategie
% % \item Manuelles Tagging von Personen, Orten, Daten, Organisationen
% % \item Ergänzung um Custom-Tags wie \texttt{signature}
% % \item Verwendung von \texttt{sic} für fehlerhafte Vorlagen
% % \item Dokumentation der Regeln zur Weitergabe an LLMs
% % \item Modelltraining und finale Transkription
% % \item Aufbauendes Modell mit CER 6,58 %
% % \item Nutzung des Modells für 80 restliche Akten
% % \item Vorteile im Workflow, aber weiterhin manuelle Kontrolle nötig
% % \item Beispielhafte Ergebnisse (Referenz auf Boxen + Abbildungen)
% % \item Bildquelle → Transkription → korrigierte, getaggte Version
% % \item Beobachtungen: LLM transformiert Inhalt lesbar, aber semantische Fehler (z.B. „Venstad“ vs. „Neustadt“)

% % \end{itemize}

% % Für die Transkrition der Daten wurde ein best-practise Ansatz gewählt. Nach Tests mit dem Python-Modul \textit{\enquote{Tesseract}} 
% % und unterschiedlichen LLMs wurde auf Transkribus zurückgegriffen. Eine Gegenüberstellung der drei erwähnten Tools findet 
% % sich im Kapitel~\nameref{section:Transkriptionen_Methoden}


% \subsection{Tagging}
% blabla
% \subsubsection{Tagging mit Transkribus}
% blabla
% \subsubsection{Tagging mit LLM}
% blabla
% \subsection{Export}
% blabla

%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––           Forschungsstand        ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––% 


%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––           Methodisches Vorgehen        ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––% 
\section{Methodisches Vorgehen}

Digitale Methoden spielen für die Durchführung dieser Arbeit eine zentrale Rolle. Von der Digitalisierung der Quellen
über die Transkription bis hin zur Auswertung durchlaufen die Daten zahlreiche Prozessschritte, die mithilfe von Large 
Language Models, Deep-Learning-Modellen und anderen digitalen Werkzeugen verarbeitet und visualisiert werden. 
Die Auswahl der Tools orientierte sich dabei an Kriterien wie Verfügbarkeit (Open~Source vs.\ proprietär), 
Kompatibilität, Community-Support, erforderlichem Arbeitsaufwand und selbstverständlich dem konkreten Mehrwert für 
die Forschungsfragen.

In diesem Kapitel werden sowohl Werkzeuge vorgestellt, die tatsächlich eingesetzt wurden, als auch solche, die sich 
im Verlauf des Projekts als ungeeignet erwiesen. Transparenz ist hierbei ein wesentlicher Aspekt: Ein grosser Teil der 
Methodik entwickelte sich erst im Forschungsprozess selbst. Da sich Large Language Models rasant weiterentwickeln, 
ist nicht immer von Beginn an klar, ob ein Tool für den eigenen Anwendungsfall geeignet ist.
Um diese Unsicherheiten zu dokumentieren, werden hier auch gescheiterte Versuche dargestellt.

\subsection{LOD – Linked Open Data}\label{subsec:LOD}

Linked Open Data (LOD) bezeichnet einen dezentral organisierten Ansatz zur Veröffentlichung und 
Verknüpfung strukturierter Daten im Web. Ziel ist es, Datensätze verschiedener Institutionen und Akteure 
maschinenlesbar zugänglich zu machen und über standardisierte Formate wie RDF und SPARQL miteinander zu 
verbinden\cite[vgl.][S.~VI und 13f]{garoufallou_metadata_2020}.
Wesentliches Merkmal der LOD-Cloud ist dabei die Nutzung semantischer Beziehungen, insbesondere Äquivalenzen einzelner Daten. 
Hierfür wird häufig das Prädikat \texttt{owl:sameAs} genutzt, um z.B. mit \colorbox{VeryLightGray}{\textit{:Choir owl:sameAs wd:Q131186}} eine eigene 
Instanz als identisch mit der Wikidata-Entität für einen Chor zu deklarieren.
Klassen oder Instanzen können so aus unterschiedlichen Datenquellen eindeutig identifiziert und zusammengeführt werden.

Die OWL Web Ontology Language, entwickelt vom World Wide Web Consortium (W3C), ist damit ein zentrales Werkzeug für die Realisierung von 
LOD.\cite[ vgl.][]{smith_owl_2004} 
Mit ihr lassen sich Ontologien definieren, die Domänen über Klassen, Individuen und deren Relationen formal beschreiben. 
Sie ermöglichen, logische Schlussfolgerungen zu ziehen, um verteilte Datenbestände zu verknüpfen und maschinenlesbar auszuwerten.
Besonders relevant ist dabei \texttt{owl:sameAs}, das als Identitätsrelation fungiert: 
Es deklariert Instanzen, die in unterschiedlichen Quellen unter verschiedenen URIs\footnote{Abk. \textbf{URI}\: Uniform Resource Identifier} geführt werden, 
als dasselbe reale Objekt\cite[ vgl.][2.3. Data Aggregation and Privacy]{smith_owl_2004}
und ermöglicht so eine präzise Zusammenführung von Informationen — ein Grundpfeiler für die Interoperabilität im Semantic Web.
Die OWL-Spezifikation baut auf RDF\footnote{Abk. \textbf{RDF}\; Resource Description Framework} auf und erweitert es um zusätzliche Konzepte.
Die RDF-Daten werden häufig im Turtle-Format (TTL) serialisiert, einer textbasierten Notation für RDF, die eine kompakte, leicht lesbare Schreibweise bietet.
Dieses Format eignet sich besonders für den Austausch und die manuelle Bearbeitung von RDF-Tripeln.
Die Sprache liegt in drei Varianten vor\footnote{OWL Lite, OWL DL und OWL Full}, die sich im Grad ihrer Ausdrucksstärke 
unterscheiden.\cite[ vgl.][1.1. The Species of OWL.]{smith_owl_2004}
Insbesondere OWL DL bietet einen praktikablen Mittelweg zwischen hoher Ausdruckskraft und vollständigem, entscheidbarem Schliessen (Reasoning) 
und ist daher für viele LOD-Anwendungsfälle geeignet.

Trotz ihres Potenzials wird diese Form der Datenverknüpfung bislang jedoch nicht von allen Websites konsequent 
umgesetzt.\cite[ vgl.][S. 14]{garoufallou_metadata_2020}. Für die technische 
Umsetzung für diese Arbeit werden zwei zentrale Werkzeuge genutzt: Protégé zur Modellierung der Ontologie und GraphDB für deren Verwaltung und Abfrage.

\subsubsection{Protégé} Zur praktischen Modellierung der Ontologie kam \textit{Protégé} zum Einsatz. 
Protégé ist eine weit verbreitete Open-Source-Software zur Erstellung, Visualisierung und Verwaltung von Ontologien.
Die grafische Oberfläche unterstützt eine intuitive Klassendefinition, 
Relationserstellung und Instanzverwaltung. 
Mit Hilfe von Plugins können darüber hinaus logische Konsistenzprüfungen durchgeführt und 
Ontologien direkt im OWL-Format exportiert werden, um sie in LOD-Workflows einzubinden.
Die initiale Version der Ontologie für dieses Projekt entstand zuerst im Codeeditor \textit{Visual Studio Code} wurde aber schnell vollständig in Protégé überarbeitet.
Damit bildet das Programm die Grundlage für erste Experimente mit Abfragen in SPARQL.%

\subsubsection{GraphDB} Für die Speicherung und Abfrage der Ontologie wurde \textit{GraphDB} verwendet. 
GraphDB ist eine spezialisierte RDF-Triplestore-Datenbank, die es ermöglicht, 
grosse Mengen an semantisch verknüpften Daten effizient zu verwalten. 
Mit der integrierten SPARQL-Schnittstelle können Benutzer gezielt nach Instanzen, Klassen und Relationen suchen 
und komplexe Muster in den Datenbeständen erkennen. 
Im Rahmen dieser Arbeit diente GraphDB als Backend, um die in Protégé entwickelte Ontologie zu testen 
und mit realen Entitäten aus den untersuchten Quellen abzugleichen.



\noindent
\begin{minipage}[t]{0.52\textwidth}
  \setstretch{1.5}
  \justifying%
\subsubsection{LOD-Ontologie}
Ein wichtiger Aspekt dieser Arbeit ist die Unstrukturiertheit relevanter Informationen. 
Aus diesem Grund wurde auf der Basis der oben beschriebenen Semantik begonnen, eine eigene Ontologie zu entwickeln, die die identifizierten Entitäten systematisch erfasst.
Beim Schreiben dieser initialen Ontologie aus rund 2000 Zeilen Code erweist sich schnell ein neues Problem. Die Datengrundlage aus den geschilderten Vorprojekten (vgl.~\hyperref[subsec:forschungsstand]{Forschungsstand und Forschungslücke}) ist 
zu klein, um daraus eine aussagekräftige Netzwerkanalyse zu machen. Hierfür erweisen sich die Unterschiede der Daten zusätzlich als zu gross und damit aufwendig. Der Fokus der Arbeit verschiebt sich dementsprechend von der Ontologieentwicklung auf die Extraktion von Entitäten.
\end{minipage}%
\hfill%
  % Rechte Minipage = Bild
  \begin{minipage}[t]{0.43\textwidth}
    \centering
    \vspace*{0.3cm} % ► schiebt alles um 0.3cm nach unten
    \includegraphics[width=\linewidth]{assets/Images/Bildschirmfoto_ttl_ontologie_Ausschnitt.png}
    \captionof{figure}{Ausschnitt der TTL-Ontologie.}\label{fig:ttl-ontologie}
\end{minipage}

\vspace{1em}

Hinzu kommen externe Quellen, und deren Zugänglichkeit. Die Hauptquellen für Informationen 
über militärische Einheiten und deren Feldpostnummern sind das \enquote{Forum der Wehrmacht}\cite[ vgl.][]{altenburger_lexikon_nodate} 
und der \enquote{Suchdienst des DRK}\cite[ vgl.][]{reuter_drk_2025}. In beiden Fällen liegen die Daten jedoch nicht als LOD vor, sondern im Forum als einfache Strings 
und beim Deutschen Roten Kreuz als OCR-PDF\footnote{OCR = Optical Character Recognition} historischer Suchlisten aus der Nachkriegszeit. Ein manuelles Recherchieren dieser Daten scheint zu diesem Zeitpunkt
den Rahmen der Arbeit zu sprengen. Daher wird im Januar 2025 entschieden, den Aufbau einer LOD Datenbank in dieser Form abzubrechen, und einen Fokus auf die Extraktion von Named Entites und das Entity Matching zu legen. Die bis zu diesem Schritt geleistete Vorarbeit beim Sortieren und Klassifizieren von Entitäten, wird in späteren Prozessschritten wieder aufgegriffen\footnote{siehe Abschnitt~\hyperref[subsec:Nodegoat_chapter]{Nodegoat}}. Auch die Recherche der IDs in Wikidata und Geonames, wie im Folgenden beschrieben, wird weiterverwendet. 

\subsection{Wikidata}\label{subsec:wikidata}

\textit{Wikidata}\cite[vgl.][]{noauthor_wikidata_nodate} ist eines der zentralen Repositorien für Linked Open Data, und bietet eine hohe Interoperabilität durch standardisierte
URIs, SPARQL-Endpunkte und offene APIs zu den Entitäten.
Jede Entität erhält dabei eine eindeutige, persistente URI 
(z.B. \colorbox{VeryLightGray}{\textit{wd:Q131186}} für einen Chor), die in LOD-Szenarien 
als stabiler Referenzpunkt dient.
Neben anderen betonen Martinez \& Pereyra Metnik (2024) beispielsweise: \\
\textit{\enquote{Wikidata stands out for its great potential in interoperability and its ability to connect data from various domains.}}\cite{martinez_comparative_nodate}

Wikidata entspricht, ebenso wie das nachfolgend beschriebene GeoNames, den FAIR-Prinzipien: Die Daten sind 
\textbf{F}\textit{indable} und \textbf{A}\textit{ccessible}, \textbf{I}\textit{nteroperable} und \textbf{R}\textit{eusable}\cite[ vgl.][S. 2]{wilkinson_fair_2016}.

Im Rahmen dieser Arbeit dient Wikidata als zentrale externe Referenz, um lokal erhobene Entitäten mit international etablierten Datenobjekten 
zu verknüpfen und so ihre Interoperabilität sicherzustellen. Die Plattform ermöglicht eine eindeutige Identifizierung sowie die maschinenlesbare 
Anreicherung um zusätzliche Informationen.

Die praktische Umsetzung zeigt jedoch eine strukturelle Einschränkung. Für diese Arbeiteigens angelegter Einträge auf Wikidata werden
trotz systematischer Verknüpfung mit anderen dort verwalteten Entitäten, etwa mit Armeen, Militäreinheiten, Orten und Personen, entfernt die Community–Moderation etwa 70\% dieser Einträge. 
Das zeigt einerseits hohe internen Qualitätsanforderungen auf, andererseits werden diese jedoch nicht klar kommuniziert. Mit regidem Löschen neuer Einträge wird die Verlässlichkeit und den Nutzen der 
geleisteten Arbeit erheblich begrenzt. 
Aufwand und Unsicherheit über die Persistenz der Einträge machen den ursprünglich vorgesehenen LOD–Ansatz in dieser Form nicht praktikabel. Es findet innerhalb der Pipeline daher optionale Anwendung. Wenn einzelne Entitäten eine Wikidata-ID haben, wird diese in die \nameref{subsec:Nodegoat_chapter}-Datenbank aufgenommen, und für das Entity Matching verwendet.

\subsection{GeoNames}\label{subsec:geonames}

Ebenso wie Wikidata bietet \textit{GeoNames}\cite[vgl.][]{noauthor_geonames_nodate} eine Open-Source-Plattform für interoperable Daten. GeoNames fokussiert sich hierbei auf geografische Informationen 
und stellt eine umfassende Datenbank mit über 25 Millionen Ortsnamen und rund 12 Millionen eindeutigen geografischen Objekten bereit.  
Die Plattform integriert Daten zu 
Ortsnamen in verschiedenen Sprachen, Höhenlagen, Bevölkerungszahlen und weiteren Attributen aus unterschiedlichen nationalen und internationalen Quellen.  
Sämtliche Geokoordinaten basieren auf dem WGS84–System\cite[\textit{WGS84: geodätische Grundlage des Global Positioning System (GPS)} ;vgl.][]{noauthor_wgs84_nodate}
und können über frei zugängliche Webservices oder eine API abgerufen werden.  
Darüber hinaus erlaubt GeoNames registrierten Nutzenden, bestehende Datensätze über eine Wiki–Oberfläche zu bearbeiten oder zu ergänzen, wodurch eine 
kollaborative Qualitätssicherung gewährleistet wird.

GeoNames wird in dieser Arbeit intensiv zur Referenzierung von Ortsnamen verwendet und bildet die Basis für die Groundtruth, wie sie in den Kapiteln \nameref{subsec:Nodegoat_chapter} und 
\nameref{subsec:place_matcher_chapter} beschrieben ist. Im Gegensatz zu Wikidata wurde hier von Beginn an darauf verzichtet, eigene Ortsdatensätze zu ergänzen. Dies liegt einerseits an 
den klar kommunizierten Community-Guidelines und andererseits daran, dass der Datensatz bis auf wenige, sehr lokale Flurnamen als nahezu vollständig gelten kann\footnote{der \enquote{Totenbühl} in Murg ist beispielsweise ein solcher Flurname}.

Historische Gebäude wie Gaststätten oder Spitäler fehlen folgerichtig in der GeoNames–Datenbank. Diese Lücke ist erwartbar, aber erwähnenswert, da GeoNames 
ansonsten eine nahezu vollständige und ausgesprochen detaillierte Datengrundlage bietet.

\subsection{Nodegoat}\label{subsec:Nodegoat_chapter}

Nachdem sich die Implementierung von Linked Open Data (LOD) für das vorliegende Projekt aufgrund des hohen zeitlichen 
Aufwands als nicht realisierbar erwiesen hat, wird mit \textbf{Nodegoat}\cite{kessels_nodegoat_2013}
eine praktikable und zugleich forschungsnahe Alternative eingeführt. Im Folgenden soll das Tool näher beschrieben, und seine Anwendung für das Projekt erläutert werden.

Nodegoat ist eine webbasierte Plattform, die sich besonders in den digitalen Forschungsprojekten in den Geisteswissenschaften etabliert hat. Es unterstützt Forschende in der Modellierung, 
Verwaltung, Analyse und Visualisierung komplexer Datenbestände. Ein zentrales Merkmal von Nodegoat ist die grafische Benutzeroberfläche, die eine vergleichsweise niedrige Einstiegshürde bietet. 
Auch Forschenden ohne tiefgehende Programmierkenntnisse wird so die Möglichkeit  eröffnet, eigene Datenmodelle zu definieren, zu pflegen und weiterzuentwickeln. Kritisiert werden muss an Nodegoat jedoch die fehlende Dokumentation. Viele Informationen finden sich nur über Drittparteien\cite[beispielsweise durch Schulungsunterlagen von Universitäten, 
hier besonders:\\][]{gubler_nodegoat_nodate} oder, wie unten geschildert, auf konkrete Nachfrage bei den Entwicklern. Der dann geleistete Support 
ist jedoch ausgesprochen zeitnah und umfassend erfolgt.
Die Plattform folgt einem modularen Prinzip: Über das UI\footnote{Abk.:\textbf{UI} User-Interface} können beliebig viele Datenmodelle 
erstellt werden, die sich flexibel an die spezifischen Forschungsfragen anpassen lassen. Diese hohe Individualisierbarkeit der Datenstrukturen erlaubt es, innerhalb kürzester Zeit projektspezifische 
Datenbanken zu konzipieren und fortlaufend zu erweitern oder an sich ändernde Bedürfnisse anzupassen.

Die Grundstruktur eines Modells unterscheidet in Nodegoat zwischen sogenannten \code{Object Descriptions} und \code{Sub-Objects}. 
Erstere legen die grundlegenden Merkmale eines Objekts fest, etwa Zeichenketten, Zahlen oder Verweise auf andere Einträge. 
So kann eine \code{Person} in diesem Projekt neben einem Feld für Vor- und Nachname auch eine Referenz auf ein \code{Gender}-Objekt enthalten, 
das eine eindeutige Geschlechtszuordnung ermöglicht.

Für das hier behandelte Forschungsvorhaben übernimmt Nodegoat die zentrale Verwaltung der Groundtruthdaten, die später als CSV-Export in die Pipeline integriert werden. 
Für die Groundtruth werden folgende Entitäten modelliert:

\begin{table}[h]
  \renewcommand{\arraystretch}{1.5}
  \centering
  \rowcolors{1}{VeryLightGray}{VeryLightGray}
  \begin{tabular}{|p{0.45\textwidth}|p{0.45\textwidth}|}
    \hline
    Personen & Orte \\ \hline
    Organisationen & Ereignisse \\ \hline
    Dokumente & Rollen \\ \hline
    Gender & \\ \hline
  \end{tabular}
  \caption{\small Übersicht der erfassten Entitäten}
\end{table}

Die Auflistung ist dabei so geordnet, dass die Entitäten mit der grössten Anzahl oder Vielfalt an zugehörigen Sub-Objects zuerst genannt werden. \code{Sub-Objects} erlauben eine
noch vielfältigere Abbildung abhängiger Informationen. Das können in dieser Arbeit zum Beispiel Quellenbelege, sowie temporale oder lokale Attribute, die einem Hauptobjekt zugeordnet werden. Lokale Attribute 
werden in Verknüpfung mit \hyperref[subsec:geonames]{GeoNames} und für Länder, Flurnamen oder Gewässer im Geojson-Format\cite[Weiterführend:][]{thomson_geographic_2017}. So lassen sich beispielsweise für\code{Persons}
in den Sub-Objekten \code{Geburt} oder \code{Tod} das Datum und der Ort modelieren, sowie die Todesursache notieren.

Die in den Kapiteln \hyperref[section:Transkriptionen_Methoden]{Transkriptionen und Methoden}, \hyperref[subsec:forschungsstand]{Forschungsstand und Forschungslücke} 
sowie \hyperref[subsec:unmatched_logger]{Unmatched Logger} beschriebenen Verarbeitungsschritte bilden die Grundlage für die in Nodegoat hinterlegten Entitäten.
Die strukturierte Erfassung dient dabei nicht nur der internen Qualitätssicherung, sondern ermöglicht auch eine nachhaltige Referenzierbarkeit durch eindeutig 
vergebene Identifikatoren, die beispielsweise beim Abgleich von Personendaten genutzt werden, um Textstellen präzise mit den zugehörigen Datenbankeinträgen zu verknüpfen.

Darüber hinaus wird Nodegoat über seine API\footnote{Programmierschnittstelle} mit einer eigens entwickelten Webanwendung verknüpft. Diese Webanwendung fungiert als publikumsorientierte 
Such- und Präsentationsplattform für die erarbeiteten Quellen. Die in den strukturierten JSON-Daten enthaltenen Nodegoat-IDs verknüpfen hier ebenfalls jede identifizierte Named Entity eindeutig mit dem 
zugehörigen Objekt in der Nodegoat-Datenbank.\footnote{an dieser Stelle sei ein Rückverweis auf die Kapitel \nameref{subsec:geonames} und \nameref{subsec:wikidata} gegeben.} 

 


\subsection{Transkriptionen (Methodenvergleich)}\label{section:Transkriptionen_Methoden}
\subsubsection{Tesseract}\label{subsubsection:Tesseract}

Da bereits zu Beginn des Projekts klar ist, dass ein Grossteil der Datenverarbeitung mit Python-Code erfolgen soll, wird gezielt 
nach Werkzeugen gesucht, die eine automatische Transkription von gescannten Dokumenten ermöglichen. 
Als besonders etabliert erweist sich die OCR-Engine Tesseract, ein Open-Source-Projekt zur Texterkennung in Bilddateien. 
Tesseract wird seit den 1980er-Jahren entwickelt, zunächst von Hewlett-Packard, später von Google weitergeführt, 
und ist über GitHub öffentlich zugänglich.\cite[vgl.][]{weil_tesseract-ocrtesseract_2025}

Die Software basiert seit Version 4 auf einem LSTM-basierten\cite[Abk.: \textbf{LSTM} steht für \textit{Long Short-Term Memory};  Architektur der frühen Generation rekurrenter neuronaler Netzwerke \textit{RNNs}. LSTMs wurden entwickelt, um Sequenzdaten zu verarbeiten und dabei sowohl kurzfristige als auch langfristige Abhängigkeiten in der Datenfolge zu erfassen – ein typisches Beispiel sind Texte, Sprache, Zeitreihen oder Handschrift. ;vgl.][p.1-2]{beck_review_2020} neuronalen Netzwerk, das besonders bei der Erkennung von zusammenhängenden gedruckten Textzeilen eine 
hohe Genauigkeit bietet. Tesseract unterstützt neben modernen Schrifttypen auch historische Schriftsätze wie Fraktur, was es besonders geeignet 
für den Einsatz in digitalisierten Archiven macht.\cite[vgl.][]{weil_tesseract-ocrtesseract_2025}

Vorbereitend für den Einsatz von Tesseract müssen alle gescannten PDF in das JPEG Format umgewandelt werden, wofür ein kurzes Python-Script verwendet wird\cite[vgl.][]{burkhardt_githubpdf_to_jpegpy_2025}. 
Im praktischen Einsatz scheiterte die Integration von Tesseract jedoch an der Heterogenität des Korpus: uneinheitliche Layouts, wechselnde 
Schrifttypen, maschinen- und handschriftliche Texte sowie komplexe Textverläufe. Beispielhaft sollen hier überlagerte oder mehrspaltig angeordnete 
Passagen auf Postkarten und Zeitungsartikeln genannt werden, die zu massiven Erkennungsfehlern führten. Auch mit angepassten Segmentierungsparametern konnte keine zufriedenstellende Texterkennung erzielt werden.

Tesseract wurde daher nicht weiterverwendet.

\subsubsection{LLM}\label{subsubsec:LLM_transcript}
Analog zum Einsatz von Python\footnote{siehe Abschnitt \nameref{subsubsec:Tesseract}} stellt die Integration von Large Language Models (LLMs) von Beginn an einen zentralen Bestandteil der Projektkonzeption dar.
Aus diesem Grund wird in einer frühen Phase auch der Einsatz von LLMs, spezifisch ChatGPT\footnote{eine detaillierte Erläuterung findet sich in \nameref{subsec:OpenAI}}, bei der Transkription der Unterlagen erprobt.

Der Einsatz von LLMs wie ChatGPT für die Transkription historischer Quellen erweist sich als ambivalent. Während die Modelle nach gezielter 
Anleitung eine erstaunlich präzise Rekonstruktion von Layoutstrukturen und maschinell erfassten Textdaten leisten, bestehen erhebliche Einschränkungen. Im Detail sind das
erhebliche Probleme bei der semantischen Genauigkeit. Das LLM beginnt sehr schnell mit sinnverändernden Haluzinationen, die unklare Textpassagen aus dem gelernten Kontext stimmig auffüllt.
Eine genaue Transkription, mit forcierter Notation von unklaren Stellen\footnote{Beispielsweise durch das Einfügen von \enquote{[\dots]}} gelingt in der Regel nicht. 
Die Verarbeitung handschriftlicher Dokumente scheitert weitgehend und führt zu stark spekulativen oder fehlerhaften Inhalten.

Hinzu kommen zu Projektbeginn technische Begrenzungen: Da ein API-Zugang zu OpenAI noch nicht verfügbar ist, erfolgt der Zugriff 
über die Weboberfläche. Diese stösst bei umfangreichen Eingaben rasch an Kapazitätsgrenzen; 
Sitzungen brechen häufig ab oder lassen sich nicht zuverlässig fortsetzen. Das kann zu inpersistenzen in der Promptstrukturierung und
dem Kontext des LLMs führen, was wiederum direkten Einfluss auf die Verarbeitung der Unterlagen hat.

Zum Zeitpunkt der exploratien Nutzun stelt das zentrales Hindernis der integrierte Content-Filter der Modelle dar. Inhalte mit Bezug zum Nationalsozialismus
führen zu einem sofortigen Abbruch der Verarbeitung. Beispielhaft sollen etwa Grussformeln wie \enquote{\textit{Heil Hitler}} genannt sein. Auffällig ist jedoch, dass
sich diese Filtermechanismen durch alternative Schreibweisen in den Quellen umgehen lassen. Die schreibweise \enquote{\textit{Heil -- Hitler}} umgeht den Filter komplett 
und wird ohne Einschränkung transkribiert.

Ohne den Zugang zur API und dem damit notwendigen Umweg über den Webclient zeigt sich 
zudem, dass LLMs ohne persistente Promptstrukturierung dazu neigen, wichtige Hintergrundinformationen zu vergessen. 
Eine Kombination aus Ground-Truth-gestützter Anleitung und manuellem Review ist daher notwendig, um eine verlässliche Transkription zu gewährleisten. Sie wird in dem Abschnitt \nameref{subsubsec:LLM_use} näher ausgeführt.

Aus den genannten Gründen kommt auch eine Transkription mittels generativem LLM nicht zum Einsatz.

\subsubsection{Transkribus}\label{subsubsec:Transkribus}
Transkribus ist eine webbasierte Plattform zur automatisierten Handschrifterkennung (HTR) und Texterkennung (OCR), die sich seit ihrer Entwicklung im EU-Projekt READ 
(Recognition and Enrichment of Archival Documents)\cite[vgl.][]{noauthor_recognition_nodate} als Standardwerkzeug in den digitalen Geschichtswissenschaften etabliert 
hat\cite[vgl.][]{muhlberger_transkribus_2019}. Betrieben wird Transkribus durch die READ-COOP SCE, einer europäischen Genossenschaft.

Die Plattform bietet zwei zentrale Zugriffsmöglichkeiten: einerseits die schlanke Webanwendung \textit{Transkribus Lite}, andererseits den \textit{Expert Client}, 
eine umfangreiche Desktopsoftware zur Bearbeitung und Verwaltung grosser Dokumentenkorpora. Beide Varianten ermöglichen die Transkription von gescannten Dokumenten, 
die Annotation von strukturellen und semantischen Einheiten sowie den Export in verschiedenen Dateiformaten. 

Die Nutzung des Expert Clients erlaubt darüber hinaus eine detaillierte Kontrolle über Transkriptionsprozesse und das zugrunde liegende Datenmanagement. 
Über integrierte Schnittstellen lassen sich grosse Datenmengen effizient verwalten. Auch externe FTP-Clients können zur Anbindung an das interne Dateisystem verwendet werden, 
um beispielsweise umfangreiche Digitalisate in strukturierter Form einzubinden.

Ein zentrales Merkmal von Transkribus ist die Möglichkeit, \textit{Tags} zu vergeben. Diese umfassen sowohl strukturelle Merkmale wie Abkürzungen, Unklarheiten oder Layout-Elemente, 
als auch semantische Einheiten wie Personen, Orte, Organisationen und Daten. Tags können individuell erweitert oder angepasst werden und werden im XML-Export maschinenlesbar dargestellt.

Die Exportfunktion von Transkribus erlaubt den Download der Transkriptionen im standardisierten PageXML-Format. Dieses Format ist auf die langfristige Nachnutzung 
struktureller Informationen ausgelegt und bildet die Grundlage für weiterführende Auswertungsschritte etwa in Digital Humanities-Projekten.

In der praktischen Handhabung zeigt sich jedoch eine teils deutliche Diskrepanz zwischen den im Interface sichtbaren Informationen und der tatsächlichen XML-Ausgabe. 
So werden beispielsweise benutzerdefinierte Abkürzungsauflösungen oder Listenstrukturen nicht zuverlässig im XML ausgegeben. 
Informationen, die manuell innerhalb der Transkriptionsumgebung gepflegt wurden, gehen im strukturierten Export unter Umständen verloren. 
Insbesondere bei Listenobjekten, etwa für Personenverzeichnisse oder Inventare, bleibt die XML-Struktur häufig leer. Eine Möglichkeit zur systematischen Nachbearbeitung 
oder maschinellen Extraktion steht bislang nicht bereit.

Diese Einschränkungen wurden auch in aktuellen Studien festgestellt. So verweisen Capurro et al.\cite[][]{capurro_experimenting_2023} im Rahmen ihrer Analyse mehrsprachiger Handschriftenkorpora 
auf signifikante Herausforderungen bei der automatisierten Layoutanalyse sowie bei der Verarbeitung komplexer Dokumentstrukturen. 
Sowohl beim Tagging als auch bei der Postcorrection sei weiterhin eine umfangreiche manuelle Nachbearbeitung notwendig, um konsistente und weiterverwendbare Datenformate zu erzeugen.

Trotz dieser Limitierungen liegt der methodische Mehrwert von Transkribus insbesondere in der Möglichkeit, ein eigenes HTR-Modell auf Basis einer spezifischen Groundtruth zu trainieren. 
Dies erlaubt es, auf charakteristische Eigenschaften eines konkreten Korpus einzugehen und so die Character Error Rate (CER) gegenüber generischen Modellen deutlich zu reduzieren. 
Darüber hinaus kann durch strukturierte Annotation eine Grundlage für die spätere Modellbewertung oder den Vergleich mit LLM-basierten Verfahren geschaffen werden.

Insgesamt stellt Transkribus eine leistungsfähige Plattform zur initialen Bearbeitung und Annotation historischer Quellen dar. 
Die automatisierte Erkennung unterstützt den Einstieg in umfangreiche Korpora, ersetzt jedoch nicht die editorische Kontrolle und Nachbearbeitung. 
Gerade für forschungsorientierte Projekte mit Fokus auf strukturierte, semantisch angereicherte Daten bleibt eine kritische Auseinandersetzung mit den technischen Grenzen unerlässlich.

\subsection{Large Language Models} \label{subsubsec:LLM_use}
Ein zentrales Werkzeug bei der Verarbeitung der historischen Quellen ist die weiter unten näher beschriebene Python-Pipeline, die auf der Verarbeitung von XML-Dateien basiert. Vorgreifend sei erwähnt, dass diese XML-Verarbeitung ein Large Language Model (LLM) zum Custom-Tagging nutzt. Nebst dem Tagging stellt das Programmieren dieser Pipeline eine der Kernherausforderungen dieses Forschungsprojekts dar. 
Für das Tagging und die Entwicklung der Pipeline werden verschiedene Large Language Models intensiv getestet und eingesetzt.
\subsubsection{Msty}
Um ein dafür geeignetes LLM zu evaluieren, werden zu Beginn des Projektes beispielhafte Prompts erstellt und deren Ergebnisse systematisch verglichen. Um diesen Vergleich zu erleichtern, wird die Desktop-Anwendung Msty\cite[vgl.][]{noauthor_msty_nodate} eingesetzt. Zu den zentralen Funktionen gehören parallele Chatinterfaces (\enquote{Parallel Multiverse Chats}),
eine flexible Verwaltung lokaler Wissensbestände (\enquote{Knowledge Stacks})\cite[vgl.][]{noauthor_msty_nodate}, sowie eine vollständige Offline-Nutzung ohne externe Datenübertragung. Msty dient dazu, verschiedene Modelle zu testen, durch die Parallel Multiverse Chats Antworten zu vergleichen und Konversationen strukturiert zu verzweigen und auszuwerten.

Hervorzuheben ist, dass dies kein klassisches Benchmarking auf Basis vergleichbarer Resultate ist. Es wird zu diesem frühen Projektzeitpunkt weder systematisch überprüft, welche Qualität der jeweilige Codeteil hat, noch wird gemessen, wie viel Prozent der Named Entities jeweils richtig erkannt werden. Der direkte Vergleich der getesteten LLMs liefert jedoch schnell ein Bild, welches Modell sich für die gleiche Aufgabe besser eignet. Erprobt werden zu Beginn des Projektes im November die folgenden Anbieter und Modelle:
\begin{itemize}
    \item Alphabet – Gemini
    \item Anthopic – Claude
    \item OpenAI – ChatGPT
\end{itemize}
Sie sollen nachfolgend eingeordnet und deren Verwendung erläutert werden.

\subsubsection{Alphabet – Gemini}
Google Gemini\footnote{ehemals Google Bard} wird im Dezember 2023\cite{noauthor_gemini_2023} von Google zunächst einer eingeschränkten Userzahl verfügbar gemacht und gilt als direkte Antwort auf OpenAIs ChatGPT. Es nimmt in dieser Arbeit keinen grossen Raum ein, da es sich im direkten Vergleich mit ChatGPT zum Zeitpunkt des Tests im Winter 2024 und Frühjahr 2025 als weniger präzise bei der Annotation von XML-Files, und weniger zuverlässig beim coden in Python herausstellte. Zur Falsifizierung dieser Annahme gelegentlich durchgeführte Überprüfungen im April und Juni 2025 brachten das selbe Ergebnis. Auch aus ökonomischen Gründen wurde auf ein zusätzlich zu OpenAI abgeschlossenes Abonnement verzichtet.

\subsubsection{Anthropic – Claude Code}\label{subsec:claude_code}

Claude Code wird am 22.~Mai~2025 offiziell veröffentlicht\cite{noauthor_claude_nodate} und ist bereits ab dem 24.~Februar~2025 in einer öffentlichen Betaphase verfügbar. In diesem Projekt erfolgt die Integration des Tools bereits sehr früh. Am 3.~April~2025 wird es erstmals eingesetzt, um komplexe Aufgaben bei der Entwicklung der Verarbeitungs- und Analysepipeline direkt in VS-Code\footnote{\textbf{Abk.:} \textit{Visual Studio Code}, die verwendete Programmierumgebung} zu übernehmen.

In den ersten Tagen erzielt Claude Code beeindruckende Resultate. Es unterstützt erfolgreich bei der Generierung von Programmcode, insbesondere bei strukturierenden Aufgaben bei der Initialisierung von Python-Funktionen. Einzelne Funktionsbausteine lassen sich durch das Tool effizient erstellen, was zunächst zu einer spürbaren Beschleunigung des Entwicklungsfortschritts führt.

Im weiteren Verlauf zeigen sich jedoch klare Begrenzungen. Bereits im April ist der Kontextumfang des Projekts so gross, dass Claude Code Schwierigkeiten hat, über mehrere Module hinweg konsistent zu arbeiten. Eine zentrale Schwäche besteht darin, dass projektweite Variablen nicht zuverlässig erkannt und übergeben werden. So kann etwa die Variable \texttt{mentioned\_persons} aus dem Modul \texttt{person\_matcher.py} nicht als wichtig erkannt und korrekt in \texttt{letter\_matcher.py} eingebunden werden.

Ein weiteres Problem ist die tiefgreifende Veränderung bestehender Funktionsstrukturen. Claude Code verändert mitunter voll funktionsfähige Module, ohne auf deren interne Abhängigkeiten Rücksicht zu nehmen. Diese Eingriffe führen häufig dazu, dass vormals stabile Komponenten nicht mehr korrekt ausgeführt werden. Variablennamen werden ohne Anzeige modifiziert, Dictionaries durch Listen ersetzt. Der Aufwand für das anschliessende Debugging übersteigt in vielen Fällen den Nutzen der automatisierten Generierung. Zwar können einzelne Änderungsschritte von Claude Code angezeigt werden, Veränderungen können aber so subtil sein, dass sie oft unbemerkt bleiben.

Hinzu kommen erhebliche Nutzungskosten. Aufgrund der Projektgrösse entstehen bereits im April für einzelne Prompts Kosten von bis zu vier US-Dollar. Durch präzises Prompting\footnote{Beispielsweise durch Angabe von Dateipfaden und Zeilennummern} lässt sich dieser Betrag zwar begrenzen, doch unterschreitet der Preis pro Anfrage selten 0,40\$. Die Arbeit mit Claude Code ist damit nicht nur zeitintensiv durch den notwendigen Korrekturaufwand, sondern auch kostenintensiv im operativen Betrieb.

Nach einer intensiven Probephase von etwa fünf Wochen wird der Einsatz von Claude Code im Projekt vor dem Public Release beendet. Die Entscheidung basiert auf einer kritischen Abwägung von Nutzen, Aufwand und Nachhaltigkeit im Hinblick auf die langfristige Wartbarkeit des Codes.

\subsubsection{OpenAI – ChatGPT}\label{subsec:OpenAI}

Im Verlauf der Arbeit wird ChatGPT intensiv in mehreren Funktionen genutzt. Dazu zählen Generierung und Überarbeitung von Quellcode, Fehleranalyse in Python- und LaTeX-Skripten, strukturierte Formulierung von Dokumentationsinhalten\footnote{in der Code-Dokumentation, Modulzusammenfassung oder beim Überarbeiten dieses Textes} sowie die semantische Annotation von Personen, Orten und Ereignissen im Transkriptionskorpus. Zum Einsatz kommen insbesondere die Modelle \textit{GPT-4o}, \textit{GPT-4.5}, \textit{o3-mini} sowie deren Varianten \textit{mini high} und \textit{GPT-4.1 mini}\cite[vgl.][]{noauthor_model_nodate}. 
Bereits vor der eigentlichen Konzeptionsphase erfolgen erste Versuche mit dem Vorgängermodell \textit{GPT-3.5}, um grundlegende Anwendungsbereiche für historische Datenverarbeitung auszuloten. Im Folgenden sollen die drei Einsatzfelder \textbf{Custom-GPT, Custom-Projekt} und \textbf{API} beschrieben und eingeordnet werden.

\paragraph{Custom-GPT} Ein speziell für das Projekt konfiguriertes \textit{Custom-GPT}\footnote{\textbf{Abk.: GPT} = \textbf{G}enerative \textbf{P}retrained \textbf{T}ransformer} wird in dieser Frühphase eingerichtet und dient als Initialer Funktionstest für den Einsatz nicht-domänenspezifischer multimodaler Sprachmodelle. 
Dieses Modell\cite[vgl.][]{burkhardt_gpt_nodate} basiert auf projektspezifischen Anweisungen, Orts- und Personenlisten sowie Groundtruth-Daten aus dem Korpus des Männerchors. Es stellt damit eine Vorform der heute verfügbaren \enquote{Projekte}-Funktion im Webinterface von ChatGPT dar. Konfiguriert wird es auf die Verarbeitung und Analyse historischer Dokumente. Zum Einsatz kommen in der Vorphase des Projektes erste Transkriptionen, Metadaten aus der Akten\_Gesamtübersicht.csv und ein erster kleiner Korpus sowie ein Manuskript von einem Chormitglied\cite[][Dieses autobiografische Manuskript wurde anfangs noch in den Korpus zu integrieren versucht, später jedoch entfernt. Es kommt aber in der Verifikation der Archivforschung (bspw. im Militärarchiv Freiburg) zum Einsatz.]{durst_sklaven_1948}. Getestet wird, ob das Modell  erkannte Personennamen mit einer bereitgestellten Namensliste vergleichen kann und die Verarbeitung diverser Formate (CSV, Excel, PDF, Bilddateien, Klartext) unterstützt.

 Der Abgleich mit den hinterlegten Namenslisten erweist sich als unzureichend. Die Ergebnisse stellen sich als methodisch unzuverlässig bzw. nicht reproduzierbar heraus. Auch eine Weiterverarbeitung durch eine vermeintlich direkte Abfrage von Wikidata zur semantischen Anreicherung historischer Entitäten bleibt ohne belastbare Resultate. Wikidata-IDs werden willkürlich generiert, so verweist die angebliche ID für München auf die Golden Gate Bridge. Gleichwohl zeigen sich punktuelle Stärken bei der automatisierten Erkennung von Strings, etwa bei der Identifikation potenzieller Personen- oder Ortsnamen. Dies legt den Grundstein für den Preprocessing–Schritt\footnote{vgl. \nameref{subsec:preprocessing}} und die später automatisierte NER Pipeline\footnote{vgl. \nameref{sec:transkribus_to_base}}. Im Dezember 2024 führt OpenAI \enquote{Custom-Projekten} ein. Ab diesem Zeitpunkt erfolgt der Wechsel zur Projektfunktion, die im Folgenden beschrieben wird.

\paragraph{Custom-Projekt}
Projekte ermöglichen eine persistente, thematisch fokussierte Interaktion mit dem Modell über längere Zeiträume hinweg. Dabei wird kontextuell relevantes Hintergrundwissen – etwa über Datenstrukturen, verwendete Tools, Entitätenlisten oder Groundtruth-Dateien – dauerhaft im System gespeichert. Diese Informationen stehen in allen Konversationen innerhalb des Projekts zur Verfügung, ohne erneut übergeben werden zu müssen. Im Unterschied zu den oben genannten \textit{Custom GPTs}, bei denen ein Modell über eine Benutzeroberfläche gezielt mit Systemmeldungen und Beispieldaten konfiguriert wird, basiert das Projektkonzept nicht auf einer einmaligen statischen Initialisierung, sondern auf fortlaufender Kontexterweiterung durch Nutzerinteraktionen. Projekte sind nicht öffentlich verfügbar, nicht durch Dritte nutzbar, und bieten keine explizite Konfigurationsmaske. 
Für dieses längerfristige Forschungsvorhaben mit sich entwickelnden Anforderungen eignet sich die Projekt-Funktion gut. Sich entwickelnde Änderungen im Code können nicht nur durch regelmässigen Uploads neuer Dateien abgebildet werden, sondern werden fortlaufend durch die Interaktion mit dem Modell erweitert, ergänzt und angepasst. So erhält das Projekt auch einen Überblick über andere Module, an denen gerade gearbeitet wird, und schliesst sie in die Antworten zu einem gewissen Grad ein. Insgesamt werden in dem Custom-Projekt \enquote{Masterarbeit} 17 Dateien hochgeladen, darunter fallen die Groundtruth-Dateien und die einzelnen Python-Module. Der Hauptfokus des Projektes liegt auf der Unterstützung beim Coden der im Kapitel \nameref{subsec:Module_im_Detail} erläuterten Codeteile.

\paragraph{API}
Für die automatisierte Verarbeitung und Annotation historischer Dokumente wird in diesem Projekt die Programmierschnittstelle (API) von OpenAI verwendet. Die API ermöglicht eine präzise Steuerung zentraler Parameter wie den Modelltyp, die Kontextlänge und die Temperatur\footnote{Die Temperatur steuert den Grad an Zufälligkeit in der Textgenerierung. Bei niedrigen Werten (z.B. 0,2) bevorzugt das Modell sehr wahrscheinliche Ausgaben; bei höheren Werten (bis 1{,}0) steigt der Anteil weniger wahrscheinlicher, "kreativer" Antworten.}.
 Die Nutzung der API erleichtert die Integration in automatisierte, skalierbare Workflows und erlaubt eine Wiederverwendung von Prompts über große Dokumentenmengen hinweg. Gleichwohl ist die tatsächliche Reproduzierbarkeit der Ergebnisse nur eingeschränkt gegeben. Selbst bei identischen Eingaben und konstanten Parametern kann es zu leichten Variationen in den Ausgaben kommen, insbesondere bei höheren Temperatureinstellungen oder bei Modellen mit probabilistischer Antwortgenerierung. Reproduzierbarkeit ist daher in diesem Kontext primär als funktionale Replizierbarkeit von Abläufen, weniger als identische Ergebniswiedergabe zu verstehen. So ist eine der Grundannahmen dieser Arbeit, dass ChatGPT den Prompt exakt so ausführt und den Inhalt der historischen Texte komplett unverändert lässt. Dies soll in einem späteren Projektschritt durch eine automatisierte Pipeline überprüft werden. \footnote{Beispielsweise durch stichprobenartigen Abgleich einzelner Page-XMLs vor und nach der Verarbeitung, einem Stripping aller Tags und einem direkten Stringvergleich mit Levenshtein und einer ausgegebenen Check-Summe. Aufgrund zeitlicher Limitationen ist dies jedoch aktuell nicht der Fall}
 
 Ein zentrales Anwendungsfeld der API im Projekt ist die Named Entity Recognition, die über prompt-basierte Anfragen realisiert wird. Dabei kann auf ein aufwendiges Modelltraining und damit einhergehende Groundthruth-Generierung verzichtet werden, da LLMs wie GPT-4 auch ohne Fine-Tuning eine kontextsensitive Entitätserkennung ermöglichen. Bereits 2020 verweisen Brown et al. darauf, dass sich die Lösung nicht explizit trainierte Tasks mit zunehmender Grösse des Models verbessert: \small\textit{\endquote{Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a
smooth trend of improvement with scale [KMH+20]. Since in-context learning involves absorbing many skills and
tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong
gains with scale.}}\cite[vgl.][S.4]{brown_language_2020} 

Dementsprechend wird OpenAIs LLM in diversen konzeptionellen und operativen Phasen dieses Projektes verwendet. Konkrete Beispiele hierfür finden sich unter anderem in dem Kapiteln \nameref{subsec:preprocessing}.

\subsection{\colorbox{red}{Webtool}}\label{subsec:webtool_chapter}
Die Planung sieht vor, oben beschriebene Verknüpfung zu nutzen, um aus der Webanwendung heraus bei Bedarf Detailinformationen zu den einzelnen Entitäten abzurufen. 
Dazu wird über spezifische API-Requests die jeweilige Objektbeschreibung geladen. In einem exemplarischen Anwendungsfall wird etwa eine Abfrage an eine URL der Form
\begin{figure}
  \hspace*{-0.6cm}%
  \centering\code{https://api.nodegoat.dasch.swiss/data/type/\colorbox{MediumGray}{11680}object/ngEL9c68pELQqGVuoFN49t/}
\end{figure}
gesendet. Hierbei steht die Type-ID \textit{11680} im Beispiel für den jeweiligen Modelltyp \enquote{Organisation} und die Zeichenkette am Ende für die eindeutige Nodegoat-ID 
des Objekts\footnote{im Beispiel eine gekürzte Nodegoat-ID}.

Die dabei zurückgelieferte JSON-Antwort enthält die gespeicherten Metadaten (z.B. Namensvarianten, Zugehörigkeiten, Quellenbelege). Um diese Informationen benutzerfreundlich darzustellen, 
wird angestrebt, den aus Nodegoat bekannten \enquote{Object Detail View} in Form eines iFrames direkt in die Webanwendung einzubetten. Dabei kann über gezielte CSS-Regeln gesteuert werden, dass 
lediglich der gewünschte Objektbereich angezeigt wird, ohne die übrigen Bestandteile der öffentlichen Nodegoat-Oberfläche zu übernehmen.

In der technischen Umsetzung wurde zudem erörtert, ob die interne Object-ID oder die plattformübergreifende Nodegoat-ID als Referenz verwendet werden sollte. Die Rückmeldung bei den
Nodegoat-Entwicklern \footnote{Kessels und van Bree} legt auf Anfrage nahe, dass beide ID-Typen im Prinzip austauschbar sind: Die Object-ID gewährleistet eine eindeutige Identifikation 
innerhalb einer spezifischen Nodegoat-Instanz, 
während die Nodegoat-ID eine konsistente Referenz über mehrere Installationen hinweg ermöglicht — auch im Hinblick auf LOD-Kompatibilität.

Für die Integration in die eigene Webanwendung wird daher ein hybrides Modell verfolgt: In den JSON-Daten der Quelltexte werden Nodegoat-IDs gespeichert, um langfristig eine offene Verknüpfbarkeit sicherzustellen. 
Gleichzeitig wird über die API der jeweilige Objekttyp (z.B.\enquote{Person}, \enquote{Organisation}) abgefragt, um die semantischen Eigenschaften der Entität zu laden. Diese werden mit dem 
Model-Endpunkt kombiniert (z.B. \code{https://api.nodegoat.dasch.swiss/model/type/11680}), um etwa Labelstrukturen oder benutzerdefinierte Felder korrekt abzubilden.

Auf diese Weise kann die Webanwendung den Nutzenden nicht nur eine reine Objektliste liefern, sondern auch kontextreiche Detailansichten generieren. Sie sind als iFrame eingebettet oder dynamisch 
gerendert und visualisieren alle relevanten Informationen direkt aus Nodegoat. Um die Serverlast zu minimieren, wird hierbei eine Caching-Strategie empfohlen, sodass wiederholte API-Abfragen 
effizient verarbeitet werden können.

\subsubsection{NDRCore}

https://ndrcore.org/ --> CMS und API-Wrapper
\subsubsection{mongoDB}
https://www.mongodb.com/de-de  --> Datenbanksystem für Korpusdaten
\subsubsection{Cantaloupe}
https://cantaloupe-project.github.io/  --> IIIF Image API server
\subsubsection{Nodegoat}
Zusammengefasst fungiert \nameref{subsec:Nodegoat_chapter} somit als zentrales Bindeglied zwischen der internen Datenhaltung und der öffentlichen Präsentation: Es vereinfacht die Pflege konsistenter Groundtruth-Daten, 
unterstützt deren Ausspielung über standardisierte Schnittstellen und ermöglicht eine modular erweiterbare Verknüpfung mit Webportalen und Suchsystemen.



% \subsection{Netzwerkanalyse als Methode}
%   \subsubsection{Theoretischer Hintergrund der Netzwerkanalyse}
%   \subsubsection{Ziele der Netzwerkanalyse im Kontext der Quellen}
%   \subsubsection{Technische Umsetzung (Tools, Datenbankstruktur)}



%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––           Pipeline        ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––% 
\section{Pipeline}

\subsubsection{Übersichtsgrafik der Pipeline}

Die untenstehende Grafik soll im Folgenden als visuelle Orientierung dienen. 
Sie bildet die logische Gliederung der Pipeline ab, wie sie im weiteren Verlauf des 
Kapitels analysiert wird. Zum Einsatz kommt sie als Referenz für nachfolgende Module in Form schematischer Zeichnungen bei der Einordnung der verschiedenen Verarbeitungsschritte und Modulabhängigkeiten. 
Als zentrale Übersicht gliedert sich diese Darstellung in drei farblich differenzierte Ebenen, die im Folgenden erläutert werden.

Grün markiert sind externe Ressourcen, die die Grundlage der Verarbeitung bilden. Dazu zählen 
zum einen XML-Dateien, die aus dem Transkriptionsprogramm Transkribus stammen, und zum anderen 
strukturierte Groundtruth-Daten im CSV-Format, die zuvor aus Nodegoat exportiert 
wurden. Dieser Prozess ist im Kapitel \nameref{section:Transkriptionen_Methoden} und \nameref{subsec:Nodegoat_chapter} dargestellt.

Orange steht für die zentralen Verarbeitungsschritte der Pipeline. Dazu gehört insbesondere das 
Preprocessing durch das LLM, wie im Abschnitt \nameref{subsec:preprocessing} 
ausgeführt, sowie das Hauptmodul \code{transkribus\_to\_base.py}. Dieses Modul koordiniert den 
gesamten Verarbeitungsablauf und wird im nachfolgenden Kapitel \nameref{sec:transkribus_to_base} ausführlich behandelt.

Blau gekennzeichnet sind die einzelnen Funktionsmodule, in die sich \code{transkribus\_to\_base.py} 
unterteilt. Diese übernehmen spezialisierte Aufgaben, etwa die Erkennung und Anreicherung von Personen, 
Orten, Organisationen oder Ereignissen. Eine detaillierte Beschreibung der jeweiligen Module erfolgt im 
Abschnitt \nameref{subsec:Module_im_Detail}.


\vspace{0.9\baselineskip}
  \centering
  \resizebox{0.99\textwidth}{!}{
    \begin{tikzpicture}[
  module/.style={rectangle, draw=black, fill=blue!10, thick, minimum width=4.5cm, minimum height=1.2cm, align=center},
  process/.style={rectangle, draw=black, fill=orange!20, thick, minimum width=4.8cm, minimum height=1.2cm, align=center},
  source/.style={rectangle, draw=black, fill=green!30!gray!20, thick, minimum width=4.2cm, minimum height=0.7cm, align=center},
  group/.style={draw=gray, dashed, rounded corners, inner sep=0.5cm},
  arrow/.style={-Latex, thick}, arrowboth/.style={<Latex>-<Latex>, thick},
  node distance=0.8cm and 1.6cm
]


% --- Hauptquellen oben ---
\node[source] (transkribus) at (-14.5, 0) {\textbf{app.transkribus.org} \\ Transkription};
\node[process] (dir) at (0, -3.2) {\textbf{Dateiverzeichnis} \\ mit XML-Dateien};
\node[process] (llmpre) at (6, -3.2) {\textbf{llm\_XML\_preprocessing.py}};
\node[process, below=7.5cm of dir] (main) {\textbf{Transkribus\_to\_base.py} \\ Hauptverarbeitung};

% ------------------- MODULE (FLOWCHART) ------------------------
\node[module, below=2cm of main] (init) {Initialisierung \\ (CSV-Import, Matcher)};
\node[module, below=of init]    (xml)  {XML \& Custom-Tags \\ Parsen, extract\_metadata};

% Referenzpunkt zentriert unter xml
\coordinate (modcenter) at ($(xml.south) + (0,-1.8)$);

\node[module] (roles)   at ($(modcenter) + (-8cm, -0.5)$) {Rollen \\ zuweisen, anreichern};
\node[module] (persons) at ($(modcenter) + (-2.7cm, -0.5)$) {Personen \\ match, split, enrich};
\node[module] (orgs)    at ($(modcenter) + (2.7cm, -0.5)$)  {Organisationen \\ extrahieren, deduplizieren};
\node[module] (places)  at ($(modcenter) + (8cm, -0.5)$)  {Orte \\ Kontext + Kombination};
\node[module] (dates)  at ($(modcenter) + (13cm, -0.5)$) {Daten \\ combine\_dates(), count};
\node[module] (events)  at ($(modcenter) + (-13.5cm, -0.5)$)  {Events \\ extract\_events\_from\_xml()};
\node[module, below=1.2cm of persons] (authors) {Autoren \& Empfänger \\ infer, dedup, score};
\node[module] (unmatched) at ($(modcenter) + (0cm, -7.5cm)$) {unmatched\_logger.py};
\node[module] (json)  at ($(modcenter) + (0cm, -9.5cm)$)  {BaseDocument Build \\ + Rollenpostprocessing};
\node[coordinate] (joinpoint) at ($(unmatched.north) + (0, 1.5cm)$) {};
\node[coordinate] (joinpointxml) at ($(xml.south) + (0, -0.5cm)$) {};
\node[source, below=of json] (save) {Speicherung \\ JSON pro Seite + total\_json};
\node[source, right=of save] (unmatchedjson) {unmatched.json};

% CSV-Quellen
\node[source] (csv1) at ($(transkribus.south) + (0.5, -4.5)$) {\textbf{export-person.csv}};
\node[source, below=0.6cm of csv1] (csv2) {\textbf{export-place.csv}};
\node[source, below=0.6cm of csv2] (csv3) {\textbf{export-roles.csv}};
\node[source, below=0.6cm of csv3] (csv4) {\textbf{export-organisationen.csv}};

\node[group, fit=(csv1)(csv2)(csv3)(csv4), name=nodegoatbox,
  label={[anchor=north west, xshift=-0.5cm, yshift=0.7cm]north west:\texttt{Vorverarbeitung: Nodegoat-Exporte}}] {};

\node[group, fit=(transkribus), name=pretranskribusbox,
  label={[anchor=north west, xshift=-0.5cm, yshift=0.7cm]north west:\texttt{Vorverarbeitung: Transkribus}}] {};

 %--- Gruppenrahmen für Modulblock ---
\node[group, fit=
  (persons)(authors)(roles)
  (orgs)(dates)(events)
  (places)(joinpoint), name=flowchartbox] {};

% --- Gruppenrahmen um Hauptverarbeitung + Flowchart ---
\node[group, fit=(main)(flowchartbox)(save),
  label={[anchor=north west, xshift=12.5cm, yshift=+0.7cm]north west:\texttt{Hauptverarbeitung der Transkribus\_II-Pipeline}}] {};


% Verbindungen
\draw[arrow] (transkribus.east) -- ++(1.2,0)-- ++(0,-3.2)-- node[pos=0.6, left, font=\small, xshift=-2cm, yshift=1.3cm]{\textit{Export als XML}} (dir.west);  
\draw[arrow] (dir.east) -- (llmpre.west);
\draw[arrow] (llmpre.north) -- ++(0,1.2) -| node[pos=0.5, above, font=\small, xshift=3cm] {\textit{Custom-Tags}} (dir.north);
\draw[arrow] (dir.south) -- (main.north);
\draw[arrow] (main.south) -- (init.north);
\draw[arrow] (init.south) -- (xml.north);

\draw[arrow] (xml.south) -- (joinpointxml.south);
\draw[arrow] (joinpointxml) -- ($(roles.north |- joinpointxml)$) -- (roles.north);
\draw[arrow] (joinpointxml) -- ($(persons.north |- joinpointxml)$) -- (persons.north);
\draw[arrow] (joinpointxml) -- ($(orgs.north |- joinpointxml)$) -- (orgs.north);
\draw[arrow] (joinpointxml) -- ($(places.north |- joinpointxml)$) -- (places.north);
\draw[arrow] (joinpointxml) -- ($(dates.north |- joinpointxml)$) -- (dates.north);
\draw[arrow] (joinpointxml) -- ($(events.north |- joinpointxml)$) -- (events.north);
\draw[arrow] (unmatched.east) -- ($(unmatchedjson.north |- unmatched.east)$) -- (unmatchedjson.north);

\draw[arrow, dashed] (persons.south) -- (authors.north);
\draw[arrow] (persons.west) -- (roles.east);
\draw[arrow] (roles.east) -- (persons.west);   
\draw[arrow] (persons.east) -- (orgs.west);
\draw[arrow] (orgs.west) -- (persons.east); 
\draw[arrow] (persons.east) -- (orgs.west);
\draw[arrow] (orgs.west) -- (persons.east);      
\draw[arrow] (orgs.east) -- (places.west);  
\draw[arrow] (places.west) -- (orgs.east);  

             
% Doppelte Pfeile mit definiertem Stil
\draw[arrow] (events.south) -- ($(events.south |- joinpoint)$);
\draw[arrow] ($(events.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (roles.south) -- ($(roles.south |- joinpoint)$);
\draw[arrow] ($(roles.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (authors.south) -- ($(authors.south |- joinpoint)$);
\draw[arrow] ($(authors.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (orgs.south) -- ($(orgs.south |- joinpoint)$);
\draw[arrow] ($(orgs.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (places.south) -- ($(places.south |- joinpoint)$);
\draw[arrow] ($(places.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (dates.south) -- ($(dates.south |- joinpoint)$);
\draw[arrow] ($(dates.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (joinpoint.south) -- (unmatched.north);


\draw[arrow] (json.south) -- (save.north);
\draw[arrow] 
  (nodegoatbox.south) |- ([xshift=-0.4cm]init.west)
  node[pos=0.6, left, font=\small, xshift=2.6cm, yshift=2.8cm]{\textit{liefert Groundtruth}};
\end{tikzpicture}
  }
  \captionof{figure}{Übersicht der gesamten XML-to-JSON-Pipeline}\label{fig:pipeline-uebersicht}
\vspace{\baselineskip}
\justifying. %Blocksatz nach Grafik

\subsection{Vorverarbeitung}\label{subsec:preprocessing}
Neben Transkribus, das bei der Transkription einen elementaren Schritt in der Vorverarbeitung aller Dokumente darstellt,
müssen die Seiten für bessere Ergebnisse weiter vorbereitet werden. Die Kernherausforderung der vorliegenden Arbeit ist die Named Entity Recognition (NER). Wie bereits im Abschnitt \nameref{subsubsec:Transkribus} ausgeführt, werden viele Inhalte bereits während des Transkriptionsprozesses manuell auf Basis von im Anhang erläuterten Regeln getaggt\footnote{vgl. Anhang \nameref{subsec:Taggingregeln_Anhang}}. Dieser Prozess ist zwar sehr genau, überschreitet aber auch den begrenzten zeitlichen Rahmen aufgrund der Menge an Unterlagen. Daher wird für das Projekt auch eine zweite Verarbeitungsform gewählt, eine \textbf{NER durch ein LLM}.

Im Zentrum des Vorverarbeitungsskripts steht eine strukturierte Anbindung an die OpenAI-API, um ausgewählte 
PAGE-XML-Dateien aus dem Transkribus-Export automatisiert mit Annotationen anzureichern. Das Skript ist modular
aufgebaut und folgt einer klar definierten Abfolge von Verarbeitungsschritten, die im Folgenden näher erläutert werden.

Die Vorverarbeitung beginnt mit der Funktion \code{get\_api\_client()}, die eine Verbindung zur OpenAI-Programmierschnittstelle 
aufbaut. Dabei wird der API-Schlüssel über eine Umgebungsvariable geladen und für spätere Anfragen bereitgestellt.
Die zentrale Annotation erfolgt in der Funktion \code{annotate\_with\_llm()}, die den vollständigen Unicode-Text der XML-Datei
verarbeitet und an das Modell GPT-4o übergibt. Grundlage ist ein präzise formulierter Prompt \footnote{vgl. Anhang \nameref{subsec:LLM-Promt}}, der die Struktur des zu erwartenden
Outputs definiert. Der Prompt spezifiziert, dass ausschliesslich \code{<TextLine>}-Elemente bearbeitet und mit einem \code{custom}-Attribut
versehen werden dürfen. Innerhalb dieses Attributs werden ausschliesslich tatsächlich erkannte Entitäten in standardisiertem Format codiert, 
darunter Personen (\code{person}), Rollen (\code{role}), Orte (\code{place}), Organisationen (\code{organization}), Daten (\code{date}) 
sowie autoren- und empfängerbezogene Markierungen (\code{author}, \code{recipient}, \code{creation\_place}). Für jedes einzelne Tag gibt es genaue Anweisungen
an das Modell. Auch ein Beispielresultat der Annotation wird jedes Mal mitgeliefert, um möglichst wenig Varianz in den Antworten zu erhalten. Gleichzeitig liefert
das Beispielresultat auch Informationen über Abkürzungen, die nicht von dem Modell als Organisationen erkannt werden. Das \enquote{WhW - das Winterhilfswerk} ist eine
Abkürzung, die offenbar nicht in den Trainingsdaten des Modells vorkommt. Dieses Vorgehen stellt den Versuch dar, ein nicht domänenspezifisch trainiertes Modell 
auf eine historische Quellenlage zu adaptieren. Es ist jedoch davon auszugehen, dass ein speziell auf den historischen Korpus abgestimmtes Sprachmodell 
deutlich präzisere Ergebnisse liefern würde.

Die Antwort des Sprachmodels ist eine vollständige XML-Datei, die sämtliche bestehenden Strukturinformationen beibehalten soll. 
Der Rückgabewert wird zunächst in der Funktion \code{clean\_llm\_output()} auf mögliche Formatierungen überprüft. Diese Funktion 
extrahiert den tatsächlichen XML-Inhalt aus dem Rückgabestring, etwa wenn dieser durch Markdown-Wrapper 
wie \code{\textasciigrave\textasciigrave\textasciigrave xml-content\textquotesingle\textquotesingle\textquotesingle} eingefasst wurde.

Die konkrete Verarbeitung einzelner Dateien erfolgt über die Funktion \code{process\_file()}, die eine originale Transkribus-XML-Datei einliest,
an das Modell übergibt, das Ergebnis prüft und anschliessend unter verändertem Dateinamen (Suffix \code{\_preprocessed}) abspeichert. Vor dem 
Schreiben erfolgt eine strukturelle Validierung mittels XML-Parser, um syntaktische Fehler oder unvollständige Rückgaben zu 
erkennen. Fehlermeldungen werden protokolliert, fehlerhafte Dateien übersprungen.

Die eigentliche Ausführung der Batch-Verarbeitung wird durch die \code{main()}-Funktion gesteuert. Diese durchläuft das in 
\code{TRANSKRIBUS\_DIR} konfigurierte Arbeitsverzeichnis, wobei alle Unterordner der Form \code{<7-stelliger Ordner>/Akte\_<Nummer>/page/} 
rekursiv analysiert werden. XML-Dateien, die bereits mit \code{\_preprocessed} enden, werden übersprungen. Zusätzlich wird ein Verzeichnis 
ignoriert, wenn bereits mehr als fünfzig Prozent der Dateien annotiert wurden. Die verbleibenden Dateien werden schrittweise mit dem Modell 
verarbeitet. Hintergrund ist eine kosteneffiziente Verarbeitung, die verhindern soll, dass Dateien mehrfach durch die API bearbeitet werden. Die Funktion 
\code{annotate\_with\_llm()} berechnet darüber hinaus die Anzahl der vom Modell verarbeiteten Eingabe- und Ausgabetokens. Auf Basis dieser Werte wird für 
jede Datei eine Schätzung der anfallenden API-Kosten vorgenommen. Diese Informationen werden für jede Anfrage protokolliert, um eine transparente 
Kostenkontrolle sicherzustellen.

Am Ende dieses Prozesses entsteht pro annotierter Seite eine neue, syntaktisch validierte XML-Datei, 
die alle Annotationen als \code{custom}-Attribute enthält. Diese dienen in den nachfolgenden Modulen der 
Pipeline (insbesondere \code{transkribus\_to\_base.py}) als Grundlage für die strukturierte Extraktion und Validierung von Entitäten.


\subsection{Hauptmodul -- Transkribus\_to\_base}\label{sec:transkribus_to_base}
\subsubsection{Initialisierung und Pfadlogik}

Die Initialisierungsphase des Skripts\footnote{bis ca. Line 250} dient der Einrichtung sämtlicher Systempfade, Datenquellen, 
Modulabhängigkeiten und Ressourcen. Im Zentrum steht dabei die Festlegung der Projektstruktur sowie die dynamische Integration aller untergeordneten Module und Datenbestände.

Zu Beginn werden die Python-Standardbibliotheken sowie externe Abhängigkeiten geladen, darunter \code{pandas} für 
Tabellenverarbeitung, \code{spacy} für die linguistische Analyse und \code{rapidfuzz} für den Vergleich von ähnlichen Strings. Zur besseren Nachvollziehbarkeit von
Verarbeitungsläufen beim Debugging wird das aktuelle Datum mit Zeitstempel generiert und als Konsolenoutput ausgegeben. 

Das Projekt ist so organisiert, dass sämtliche zentralen Datenverzeichnisse, Ressourcen und Modulstrukturen relativ
zur Wurzel definiert und im Code programmatisch zugänglich gemacht sind. Das Projektwurzelverzeichnis wird dynamisch erkannt, und fungiert als zentraler Referenzpunkt für alle nachfolgenden Verzeichnispfade. Dadurch lassen sich alle enthaltenen Funktions- und Klassendefinitionen zentral importieren. Absolute Pfadangaben werden vollständig vermieden.\\

Zur Initialisierungslogik gehört  beispielsweise das standardisierte Einlesen der 
Unterverzeichnisse \code{Data} für CSV-basierte Groundtruth-Informationen sowie \code{Module} für Verarbeitungsfunktionen.\footnote{vgl. \nameref{subsec:Module_im_Detail}}. Die jeweiligen Nodegoat-Exporte sind zentral im Ordner \code{/Data/Nodegoat\_Export} gespeichert. 
Die dort hinterlegten Informationen werden aus den 
CSV-Dateien geladen, in \code{pandas.DataFrames} überführt und durch Fehlerbehandlung in Form von
\code{try- \& exept}-Blöcken abgesichert. 
Eine Konsolenausgabe informiert über Anzahl und Status der geladenen Einträge.

Es folgen Abfragen, ob  das deutsche Sprachmodell 
\code{de\_core\_news\_sm} von \code{spaCy} und ein gültiger API-Schlüssel für ChatGPT in der Umgebungsvariable geladen ist. Gegebenenfalls wird auf ein Fehlen hingewiesen. Sollte letztgenannter Schlüssel nicht vorhanden sein, wird der am Ende folgende Enrichment-Prozess automatisch deaktiviert.

Die standardisierte Datenstruktur für Personen wird unmittelbar nach dem Einlesen der 
Groundtruth-Dateien erzeugt. Hierzu werden die relevanten Felder aus der CSV-Datei 
\code{export-person.csv} extrahiert und als strukturierte Python-Dictionaries gespeichert. 
Diese beinhalten Attribute wie \code{forename}, \code{familyname}, \code{alternate\_name}, 
\code{title} sowie die eindeutige \code{nodegoat\_id}. Die Struktur orientiert sich an der in 
\code{document\_schemas.py} definierten Klasse \code{Person}, bildet jedoch in dieser Phase noch 
keine Instanzen davon.\footnote{Ein Relikt des dynamisch entstandenen Codes, das aus Zeitgründen noch nicht aufgelöst wurde} Sie dient als Referenz für alle nachfolgenden Matching- und 
Deduplikationsvorgänge.

Abschliessend stehen zwei Hilfsfunktionen\footnote{\textbf{Hilfsfunktionen:} \code{load\_known\_persons} und \code{person\_exists\_in\_known\_list}} bereit, um neu erkannte Personen in die bestehende 
CSV-Struktur zu überführen. Auf Basis des \enquote{fuzzy stringmatchings} mit der sogenannten Levensthein-Distanz\cite[\textbf{Levensthein-Distanz:} Die minimale Anzahl von elementaren Editieroperationen (Einfügen, Löschen, Ersetzen), die notwendig sind, um ein Wort a in ein anderes Wort b zu überführen. vgl:][]{levenshtein_binary_nodate} wird eine Duplikatsprüfung 
durch \code{Rapidfuzz} realisiert und stellt sicher, dass nur bisher unbekannte Einträge ergänzt werden.

Damit ist die Initialisierungs- und Pfadlogik abgeschlossen, der Grundstein für eine skalierbare, 
reproduzierbare und systematisch strukturierte Weiterverarbeitung in den folgenden Modulen gelegt.

\subsubsection{Extraktion von Struktur und Fliesstext}

Die Verarbeitung einer Transkribus-Seite beginnt mit der strukturierten Auswertung der zugehörigen PAGE-XML-Datei. Ziel ist die Gewinnung zentraler Informationen, die sowohl die technische Identifikation als auch die inhaltliche Analyse des Dokuments ermöglichen. Dieser Extraktionsprozess lässt sich in drei Segmente gliedern:

\begin{enumerate}
\item Extraktion technischer Metadaten
\item Extraktion des transkribierten Fließtexts
\item Extraktion semantisch annotierter Entitäten aus Custom-Tags
\end{enumerate}

Die folgenden Funktionen bilden jeweils die Kernoperationen dieser drei Schritte und bereiten die Daten für die nachgelagerte Entitätenanreicherung, Validierung und den Export in ein standardisiertes JSON-Format vor.

\vspace{1em}
\paragraph{1. Technische Metadaten}

\subparagraph{extract\_metadata\_from\_xml~(~)}\label{subpara:extract_metadata_from_xml} Diese Funktion liest die im \code{<TranskribusMetadata>}-Block gespeicherten Informationen aus, die für die eindeutige Identifikation jeder Seite erforderlich sind. Dazu zählen insbesondere die Dokumenten-ID (\code{docId}), die Seiten-ID (\code{pageId}), die Transkriptions-ID (\code{tsid}) sowie die Pfade zu Bild und XML-Datei. Diese Informationen werden in einem Dictionary gespeichert und später im Attributblock des JSON-Dokuments hinterlegt. Im Falle fehlender Metadaten wird ein leeres Dictionary zurückgegeben, um die Robustheit der Verarbeitung zu gewährleisten.

\subparagraph{get\_document\_type~(~)} ist eine in \nameref{subsection:type_matcher} genauer ausgeführte Funktion. Sie liefert Informationen, ob ein Dokument beispielsweise als Brief,  Postkarte oder Protokoll kategorisiert werden kann.

\vspace{1em}
\paragraph{2. Transkribierter Fließtext}

\subparagraph{extract\_text\_from\_xml~(~)}\label{subpara:extract_text_from_xm} Der aus dem PAGE-XML extrahierte Fließtext bildet die Grundlage für eine Vielzahl regelbasierter und LLM-gestützter Analyseschritte. Die Funktion iteriert über alle \code{TextLine}-Elemente und extrahiert jeweils den Inhalt der Elemente \code{<TextEquiv>} bzw. \code{<Unicode>}-Unterelements. Die resultierenden Zeilen werden in einzeilige Strings überführt und mit Zeilenumbrüchen (\code{\textbackslash n}) getrennt. Dadurch bleibt der Zeilenkontext auch in der späteren Analyse (z.B. bei der Zuordnung von Rollen oder Orten) erhalten. Zusätzlich wird dieser Text verwendet, um die in den Custom-Tags angegebenen Offset-Positionen auf den tatsächlichen Textinhalt zu projizieren.

\vspace{1em}
\paragraph{3. Semantische Annotation über Custom-Tags}

\subparagraph{extract\_custom\_attributes~(~)} Diese zentrale Funktion wertet die semantischen Annotationen aus, die über das \code{custom}-Attribut in den \code{TextLine}-Elementen gespeichert sind. Jede Zeile wird einzeln geprüft, ob sie eines oder mehrere Custom-Tags enthält. Ist dies der Fall, werden die in diesen Tags enthaltenen Entitätsangaben mittels spezialisierter Subfunktionen analysiert und strukturiert erfasst. Für jede Entität wird der zugehörige Textausschnitt mithilfe der im Tag angegebenen \code{offset}- und \code{length}-Angaben aus dem Fließtext extrahiert. Die Ergebnisse werden in einem Dictionary gesammelt, das nach Entitätskategorien gegliedert ist. Die relevanten Subfunktionen sind im Folgenden beschrieben:

\begin{itemize}
\item \textbf{extract\_person\_from\_custom~(~)} erkennt Personen anhand expliziter Namensangaben im XML-Tag (\code{firstname}, \code{lastname}) oder, falls diese fehlen, heuristisch über Offset-basierte Textsegmente. Die Namen werden mit \code{split\_name\_string()} zerlegt und ggf.\ durch \code{correct\_swapped\_name()} korrigiert. Zusätzlich werden Titel \footnote{z.B. \enquote{\texttt{Herr}}, \enquote{\texttt{Fräulein}}} sowie Rollen\footnote{z.B.\ \enquote{\texttt{Führer}}, \enquote{\texttt{Schneiderin}}} identifiziert und in strukturierter Form gespeichert. Der Abgleich mit bekannten Personen erfolgt über die Funktion \code{match\_person()}, die Fuzzy-Matching und ID-Zuweisung auf Basis einer CSV-basierten Groundtruth durchführt. Bei unvollständigen Matches wird das Feld \code{needs\_review=true} gesetzt. Als ergänzender Mechanismus dient \code{fallback\_match\_by\_familyname\_and\_gender~(~)}, um die Nodegoat-ID einer weiblichen Personen bei fehlendem Vornamen dennoch zu identifizieren\footnote{z.B. um \texttt{\enquote{Frau Zimmermann}} nicht mit \code{\enquote{Alfons Zimmermann} zu matchen}}.

\item \textbf{extract\_organization\_from\_custom~(~)} verarbeitet Custom-Tags der Form \code{organization} oder \code{org}. Die Funktion extrahiert über Offset-Angaben den relevanten Textabschnitt und identifiziert zusätzlich eine potenzielle \code{wikidata\_id} mit Hilfe der Hilfsfunktion \code{extract\_wikidata\_id~(~)}. Die resultierenden Objekte enthalten strukturierte Felder für Name, Position, Wikidata-ID sowie Ort und Organisationstyp. Die Einträge werden dann mit \code{\nameref{subsection:organization_matcher}} gegen eine Groundtruth-Organisationstabelle validiert.

\item \textbf{extract\_place\_from\_custom~(~)} ist zuständig für die Extraktion von Ortsnamen aus Custom-Tags der Form \code{place}. Basierend auf den Offset-Positionen wird der Ortstext extrahiert und über eine Instanz des \code{\nameref{subsec:place_matcher_chapter}}-Moduls mit bekannten Orten abgeglichen. Dabei werden GeoNames- und Wikidata-Daten einbezogen. Treffer werden anhand eines Score-Werts bewertet und mit Feldern wie \code{matched\_name}, \code{geonames\_id} oder \code{confidence} angereichert. Auch fehlerhafte oder nicht eindeutig zuordenbare Orte werden zurückgegeben, um eine nachträgliche manuelle Validierung zu ermöglichen.

\item \textbf{extract\_date\_from\_custom~(~)} erkennt Datumsangaben anhand typischer Custom-Tag-Strukturen. Dabei wird das Datum aus dem Text extrahiert und in eine standardisierte Zeichenkette überführt. Die extrahierten Daten werden später zur Zählung und semantischen Kontextualisierung genutzt.

\item \textbf{parse\_custom\_attributes~(~)} Diese Hilfsfunktion übernimmt die Umwandlung der Custom-Attributinhalte (z.B. \code{offset:10; length:12}) in strukturierte Dictionaries. Sie bildet die Grundlage für alle weiteren Extraktionsfunktionen und sorgt für eine einheitliche Zugriffsebene auf die annotierten Positionsdaten.
\end{itemize}

Die in \code{extract\_custom\_attributes()} erkannten Entitäten werden in einem standardisierten Dictionary gespeichert, das fünf zentrale Schlüssel enthält: \code{persons}, \code{roles}, \code{organizations}, \code{places} und \code{dates}. Dieses Dictionary bildet die Basis für nachgelagerte Validierungsschritte, die Deduplikation der Entitäten sowie die finale Konvertierung in ein BaseDocument-konformes JSON-Schema.

\subsubsection{Orchestrierungsfunktion}
\code{process\_transkribus\_file()} ist der Kern der Dokumentenverarbeitung. Hier werden alle Module gebündelt aufgerufen, die Informationen der Verarbeitungsschritte abgefragt, und in das BaseDokument final zusammengefügt. Diese Orchestrierung erfolgt in insgesamt 18 Schritten, die nachfolgend erläutert werden sollen. 

\paragraph{1. XML-Verarbeitung und Dokument-Identifikation}
Das XML-Dokument wird mit \code{ET.parse()} geladen und in ein Elementbaumobjekt \code{root} überführt. Danach ermittelt \code{\nameref{subsection:type_matcher}} den Dokumenttyp, während \code{\nameref{subpara:extract_metadata_from_xml}} die extrahierten Metadaten liefert.

\paragraph{2. Transkriptextraktion}
Der mit \nameref{subpara:extract_text_from_xm} aus dem XML extrahierte Transkriptionstext wird zu einem fortlaufenden Fließtext zusammengesetzt. Dokumente mit zu kurzem oder fehlendem Text werden an dieser Stelle ausgeschlossen (\code{return None}).

\paragraph{3. Raw\_author und Raw\_recipient}
Die Funktionen \code{match\_authors()} und \code{match\_recipients()} aus dem Modul \code{\nameref{subsection:letter_metadata_matcher}} liefern potenzielle Verfasser:innen und Adressat:innen. Die Ausgabe dieser Funktionen kann in unterschiedlichen Formaten erfolgen – als \code{Person}-Instanz, als \code{dict} mit Namensfeldern oder als Liste solcher Elemente.

Zur Vereinheitlichung und robusten Weiterverarbeitung werden alle Rückgaben in einheitliche \code{List[Person]}-Objekte überführt. Dabei wird für jedes Element geprüft, ob es sich bereits um eine gültige Instanz der Klasse \code{Person} handelt, oder ob es über \texttt{code.from\_dict()} aus einem Dictionary erzeugt werden muss. Diese Zwischenspeicherung in \code{temp\_authors} und \code{temp\_recipients} dient als Grundlage für spätere Validierungs-, Deduplikations- und Anreicherungsprozesse innerhalb der Pipeline.

\paragraph{4. LLM-gestützte Verfeinerung} \
\texttt{infer\_authors\_recipients()} verwendet ein Sprachmodell zur Klassifikation von Personenrollen auf Basis des Textkontexts. \texttt{ensure\_author\_recipient\_in\_mentions()} stellt sicher, dass alle erkannten Personen auch in \texttt{mentioned\_persons} auftauchen.

\paragraph{5. Rollenanreicherung für Autor:innen} \
\texttt{assign\_roles\_to\_known\_persons()} weist bekannten Personen basierend auf Textkontext ihre Rollen zu. Diese werden direkt in die Objekte der \texttt{authors}-Liste eingetragen.

\paragraph{6. Extraktion aus Custom-Tags} \
Die Funktion \texttt{extract\_custom\_attributes()} durchsucht XML-Custom-Tags nach annotierten Entitäten. Ergänzend erkennt \texttt{extract\_standalone\_roles()} kontextuell verwendete Rollenbezeichnungen ohne direkte Personenbindung.

\paragraph{7. Organisationen im Fließtext} \
Zusätzlich zu Custom-Tags werden Organisationen direkt im Fließtext mittels \texttt{match\_organization\_from\_text()} identifiziert und in die Datenstruktur übernommen.

\paragraph{8. Personenanreicherung und Deduplikation} \
Rollenbezeichnungen innerhalb von Tokens (z. B. \enquote{Dirigent Maier}) werden durch \texttt{extract\_role\_in\_token()} erkannt. \texttt{deduplicate\_persons()} entfernt doppelte Einträge, und \texttt{get\_best\_match\_info()} gleicht mit bekannten Personenprofilen ab.

\paragraph{9. Autor-Rückübertragung} \
Zur Verbesserung der Datenqualität werden fehlende Felder bei \texttt{authors} aus den angereicherten Personendaten (\texttt{enriched\_persons}) ergänzt.

\paragraph{10. Logging neuer Personen} \
Nicht in der Groundtruth enthaltene Personen werden für spätere Kontrolle im Logfile vermerkt.

\paragraph{11. Konvertierung zu \texttt{Person}-Objekten} \
Sämtliche angereicherten und nicht verworfenen Personeninformationen werden in valide Instanzen der Klasse \texttt{Person} überführt und der Liste \texttt{mentioned\_persons} hinzugefügt.

\paragraph{12. Organisationen verarbeiten} \
Die Liste der Organisationen wird in Instanzen der Klasse \texttt{Organization} übertragen und entsprechend normalisiert.

\paragraph{13. Orte deduplizieren und zuordnen} \
Ortsangaben werden durch \texttt{place\_m.deduplicate\_places()} bereinigt und in Instanzen der Klasse \texttt{Place} überführt.

\paragraph{14. Ereignisse extrahieren} \
Die Funktion \texttt{extract\_events\_from\_xml()} identifiziert strukturierte Ereignisse aus den XML-Tags.

\paragraph{15. Konstruktion des \texttt{BaseDocument}} \
Alle extrahierten und angereicherten Informationen werden in einem strukturierten \texttt{BaseDocument}-Objekt gebündelt.

\paragraph{16. Zweite Rollenanreicherung (Autoren und Empfänger)} \
Rollen und Rollenschemata werden erneut über \texttt{assign\_roles\_to\_known\_persons()} ermittelt. \texttt{flatten\_organisation\_entry()} sorgt für saubere Strukturen bei verknüpften Organisationen.

\paragraph{17. Finaler Deduplikationsschritt} \
\texttt{deduplicate\_and\_group\_persons()} konsolidiert die Entitätenlisten endgültig. \texttt{infer\_gender\_for\_person()} füllt ggf. fehlende Geschlechtsangaben. Auf Basis von ID oder Score werden die finalen \texttt{authors} und \texttt{recipients} bestimmt.

\paragraph{18. Validierung und Speicherung} \
Das erstellte Dokument wird durch \texttt{validate\_extended()} validiert. Bei Erfolg erfolgt die Ausgabe als JSON-Datei. Die Funktion \texttt{update\_total\_json()} aktualisiert das zentrale Gesamtverzeichnis \texttt{total\_json.json}.












Zweck: Erkennung, Anreicherung und Zuordnung von Personen, Rollen, Orten, Organisationen und Ereignissen.
\begin{itemize}
\item \texttt{mentioned\_places\_from\_custom\_data~(~)}
\item \texttt{extract\_and\_prepare\_persons~(~)}
\item \texttt{assign\_roles\_to\_known\_persons~(~)}
\item \texttt{match\_organization\_entities~(~)}
\item \texttt{extract\_events\_from\_xml~(~)}
\item \texttt{combine\_dates~(~)}
\item \texttt{assign\_sender\_and\_recipient\_place~(~)}
\end{itemize}


\subsubsection{Deduplikation und Validierung}
Zweck: Zusammenführung mehrfach erkannter Entitäten und finale Konsistenzprüfung.
\begin{itemize}
\item \texttt{deduplicate\_and\_group\_persons~(~)}
\item \texttt{ensure\_author\_recipient\_in\_mentions~(~)}
\item \texttt{count\_mentions\_in\_transcript\_contextual~(~)}
\item \texttt{postprocess\_roles~(~)}
\item \texttt{mark\_unmatched\_persons~(~)}
\item \texttt{validate\_extended~(~)}
\end{itemize}

\subsubsection*{JSON-Export und Logging}
Zweck: Erstellung der finalen JSON-Dateien im gewünschten Basisschema und Protokollierung von problematischen Einträgen.
\begin{itemize}
\item Erstellung von \texttt{BaseDocument~(~)}
\item \texttt{doc.to\_json~(~)}
\item \texttt{update\_total\_json~(~)}
\item \texttt{log\_unmatched\_entities~(~)}
\item Terminalausgabe bei Validierungsfehlern
\end{itemize}

JSon Export weil menschenlesbar und leichte Abwandelbarkeit in andere Formate.

\subsubsection{Review-Prozess}
Zweck: Markierung und Protokollierung unsicherer, unvollständiger oder nicht eindeutig gematchter Entitäten für eine spätere manuelle Überprüfung.

\begin{itemize}
\item \texttt{mark\_unmatched\_persons~(~)} – Kennzeichnung von Personen ohne ID, mit niedrigem Score, unklarem Namen
\item \texttt{needs\_review = true} bei allen problematischen Einträgen
\item \texttt{review\_reason} zur Beschreibung der Ursache (z.B. „nur Vorname“, „nicht in Groundtruth“)
\item \texttt{log\_unmatched\_entities~(~)} – Protokollierung in den Dateien:
  \begin{itemize}
  \item \texttt{unmatched\_persons.json}
  \item \texttt{unmatched\_places.json}
  \item \texttt{unmatched\_roles.json}
  \item \texttt{unmatched\_events.json}
  \end{itemize}
\item Kontextbasierte Filterung durch Zeilenumfeld (z.B. keine Dopplung bei Rolle+Name in direkter Nachbarschaft)
\end{itemize}


\vspace{1em}\textbf{DAS HIER IST EIN ALTER ABSCHNITT, DER FÜR DIE OBEN ZU ERGÄNZENDEN KAPITEL VERWENDET WERDEN SOLL}\\
------------------------------------\\
Die wesentliche Verarbeitung der durch ChatGPT verarbeiteten XML-Files für jede einzelne Seite wird im 
Hauptmodul \code{Transkribus\_to\_base.py} gesteuert. Es ist das umfangreichste
Modul für dieses Projekt, dessen Funktionsweise im Folgenden beschrieben werden soll.

Nach Abschluss der Vorverarbeitung und der Anreicherung mit Annotationen werden die XML-Dateien mithilfe des 
Moduls \texttt{transkribus\_to\_base.py} in eine strukturierte JSON-Repräsentation überführt. Diese stellt das 
im Projekt definierte Basisschema dar und dient als Grundlage für die nachfolgenden Analyseschritte. Ziel ist es, 
aus dem strukturierten und annotierten XML-Dokument ein validiertes JSON-Objekt zu erzeugen, das alle im Dokument 
erkannten Entitäten eindeutig, formal konsistent und datenmodellkonform beschreibt.

Das Modul \code{transkribus\_to\_base.py} ist als zentraler Verarbeitungsknoten konzipiert. Es verarbeitet die Inhalte 
der zuvor erzeugten XML-Dateien schrittweise, prüft und transformiert sie und strukturiert sie in einer einheitlichen Objektklasse 
(\code{BaseDocument}). Die Verarbeitung beginnt mit dem Einlesen der XML-Datei. In einem ersten Schritt werden aus dem XML-Header 
Informationen wie \code{docId}, \code{pageId}, \code{tsid} sowie Referenzen zu Bild- und Quelldateien extrahiert. Diese Metadaten 
bilden die Basis für die eindeutige Identifikation jeder Seite. Zusätzlich wird aus dem Dateinamen das Dokumentformat (z.B. Brief, 
Postkarte, Protokoll) abgeleitet. Dieses wird später im Feld \code{document\_type} gespeichert und dient der Klassifikation innerhalb 
der Datenstruktur.

Parallel dazu wird der vollständige Transkriptionstext aus den \code{<TextEquiv>} bzw. \code{<Unicode>}-Blöcken extrahiert. Dieser 
Text bildet die Grundlage für alle heuristischen, regelbasierten und modellgestützten Erkennungsverfahren. Die im vorherigen Schritt von 
ChatGPT annotierten \code{custom}-Attribute werden nun systematisch ausgelesen und auf ihre Struktur analysiert. Dabei werden Personen, 
Rollen, Orte, Organisationen und Daten extrahiert. Jede dieser Kategorien wird durch eine eigene Funktionsgruppe behandelt, die intern auf 
vorab geladene Groundtruth-Daten zurückgreift. Die Groundtruth-Dateien stammen aus \textit{Nodegoat} und werden projektweit als 
CSV-Dateien verwaltet.

Die Extraktion von Personen erfolgt über die Funktion \code{extract\_person\_from\_custom()}, die für jeden in der XML-Datei 
annotierten \code{person}-Tag eine initiale Zerlegung vornimmt. In einem mehrstufigen Matchingverfahren wird versucht, die 
extrahierten Namen mit bekannten Personen zu verknüpfen. Dabei kommen Fuzzy-Matching-Techniken zum Einsatz, die über die Funktion 
\code{match\_person()} gesteuert werden. Zusätzlich werden Titel wie \enquote{Herr}, \enquote{Frau}, \enquote{Sängerbruder} oder 
\enquote{Witwe} als Geschlechtsindikatoren erkannt und gespeichert. Für jede identifizierte Person wird ein Eintrag erzeugt, der sowohl 
die extrahierten als auch die gematchten Informationen enthält. Bei fehlender Übereinstimmung wird der Eintrag mit dem Vermerk 
\code{needs\_review} gekennzeichnet.

Die Ortsverarbeitung basiert auf einem spezialisierten \code{PlaceMatcher}-Objekt, das die extrahierten Ortsnamen mit bekannten 
Ortsbezeichnungen aus der Groundtruth sowie mit externen Ressourcen wie Geonames oder Wikidata abgleicht. Bei unklaren oder 
mehrdeutigen Ortsangaben kann der Matcher mehrere Kandidaten zurückgeben. In diesem Fall erfolgt eine Gewichtung anhand von 
Konfidenz- und Ähnlichkeitswerten. Die Funktion \code{extract\_place\_from\_custom()} ist dabei für die Initialextraktion 
zuständig, während die Funktion \code{deduplicate\_places()} eine Zusammenführung ähnlicher Ortsangaben durchführt.

Zusätzlich zu den durch das Sprachmodell erzeugten Custom-Tags werden weitere Entitäten heuristisch aus dem Fliesstext erkannt. 
Besonders betrifft dies Rollenbezeichnungen, die in unmittelbarer Nähe zu Personennamen vorkommen. Eine regelbasierte Extraktion 
dieser Kontexte wird durch die Funktion \code{assign\_roles\_to\_known\_persons()} realisiert. Auch hier wird das Ergebnis validiert 
und – sofern die Rolle einer standardisierten Ontologie entspricht – in das Feld \code{role\_schema} überführt.

Die Kombination der verschiedenen Erkennungsmethoden kann zu Duplikaten führen. Um konsistente und eindeutige Entitäten zu erzeugen, 
erfolgt ein deduplizierender Abgleich über die Funktion \code{deduplicate\_and\_group\_persons()}. Diese vergleicht alle Personen 
aus den Kategorien \code{authors}, \code{recipients} und \code{mentioned\_persons} untereinander. Dabei werden vorhandene Scores 
(wie \code{match\_score}, \code{recipient\_score} und \code{confidence}) zusammengeführt und priorisiert.

Das so angereicherte Dokument wird in ein Objekt der Klasse \code{BaseDocument} überführt. Dieses enthält strukturierte Felder für 
Metadaten, Volltext, Autoren, Empfänger, erwähnte Personen, Orte, Organisationen, Datumsangaben und Ereignisse. Jede Entität wird gemäss 
den Typdefinitionen in \code{document\_schemas.py} validiert. Ein abschliessender Validierungsschritt erfolgt über 
\code{validate\_extended()}, das auf Fehler in der Struktur, Inkonsistenzen oder fehlende Pflichtfelder prüft.

Abschliessend wird das Dokument im JSON-Format gespeichert. Neben der Einzelseite wird auch eine aggregierte Datei 
\code{total\_json.json} fortgeschrieben, in der alle Seiten einer Akte gesammelt werden. Zusätzlich wird für jede Seite 
eine Prüfung auf nicht zuordenbare Entitäten durchgeführt. Diese werden in der Datei \code{unmatched.json} gespeichert, 
um eine spätere manuelle Nachbearbeitung zu ermöglichen. Das Ergebnis dieser Konvertierung bildet die Grundlage für die weitere 
Verarbeitung in \code{Nodegoat} sowie für die explorative Analyse der Akteursnetzwerke.

------------------------------------


\subsection{Module im Detail}\label{subsec:Module_im_Detail}
\subsubsection{document\_schemas.py}\label{subsec:document_schema}
Das Modul \code{document\_schemas.py} definiert die zentrale Schema- und Datenstruktur 
zur Modellierung aller extrahierten Inhalte aus den Transkribus-Dokumenten des Projekts. Es gewährleistet die einheitliche Repräsentation, 
Serialisierung und Validierung der im Projekt verarbeiteten Entitäten und ihrer Relationen. Die definierten Klassen bilden die Grundlage für die JSON-Ausgabe der angereicherten Dokumente und dienen zugleich der Nachvollziehbarkeit und strukturierten Weiterverarbeitung in externen Anwendungen wie Nodegoat.

\paragraph{Person}
Die Klasse \code{Person} bildet Einzelpersonen ab, wie sie in den Briefen, Postkarten oder Protokollen erscheinen. 
Neben klassischen Namensfeldern (\code{forename}, \code{familyname}, \code{title}) unterstützt die Klasse auch 
alternative Namen (\code{alternate\_name}) und Rolleninhalte. Die Rollenverarbeitung erfolgt in einem zweistufigen Verfahren:
Zum einen wird die Freitextrolle (\code{role}) als Originaleintrag gespeichert, zum anderen erfolgt eine normierte 
Zuordnung über \code{role\_schema}, basierend auf dem internen Mapping in \code{Assigned\_Roles\_Module.py}. 

Titel wie \code{"Herr"} oder \code{"Frau"} dienen in \code{person\_matcher.py} zugleich als Grundlage zur 
Ableitung des Geschlechts der Person. Das Feld \code{gender} erlaubt in \code{document\_schemas.py} eine 
geschlechtsspezifische Zuordnung, basierend auf Titelbezeichnungen oder Groundtruth-Matching. 

Ergänzt wird die Person um Kontexte wie \code{associated\_place} und \code{associated\_organisation}, sowie um Bewertungsparameter 
wie \code{match\_score}, \code{recipient\_score}, \code{confidence} und \code{mentioned\_count}.\footnote{Detailliert wird auf diese Logiken in 
\nameref{subsec:person_matcher.py} eingegangen, sie sollen hier nur umrissen werden.} 
Letzterer gibt an, wie oft 
die Person im Transkript erwähnt wurde, unter Berücksichtigung einer kontextbasierten Zähllogik.
\code{match\_score} beschreibt einen Wert der Wahrscheinlichkeit, dass es sich bei der im Dokument beschriebenen Person auch 
um die Person handelt, die schlussendlich gematcht wurde. Vor- und Nachnamen erhöhen beispielsweise den Score, während das
Fehlen solch einzelner Informationen den Wert senken. 

Das Feld \code{confidence} beschreibt die Modell-Sicherheit der Zuordnung, z.B.beim LLM-basierten Matching. 
\code{recipient\_score} bewertet die Wahrscheinlichkeit, dass es sich bei der Person um den tatsächlichen Empfänger des Dokuments handelt. 

Ein optionales \code{needs\_review}-Flag sowie ein \code{review\_reason} ermöglichen die gezielte Markierung unsicherer oder 
maschinell nur teilweise auflösbarer Fälle.

\paragraph{Organization}
Die Klasse \code{Organization} dient der Abbildung aller im Text genannten Vereine, Gruppen oder Institutionen, 
darunter z.B. Gesangsvereine, NS-Organisationen. Neben dem Namen, Typ und eventuellen 
Alternativnamen (\code{alternate\_names}) wird insbesondere die eindeutige \code{nodegoat\_id} gespeichert. Für den Spezialfall 
militärischer Einheiten enthält die Klasse ein separates Feld \code{feldpostnummer}. Da sie in der Pipeline jedoch nicht seperat getaggt und verarbeitet sind,
werden diese aktuell ausschliesslich über die Groundtruth abgerufen. Über \code{match\_score} und \code{confidence} 
werden auch in der Klasse \code{Organization} Qualität und Unsicherheiten der Zuordnung nachvollziehbar gemacht. 
Organisationen können zusätzlich über das Feld \code{associated\_place} mit einem geographischen Ort verknüpft werden, z.B. \enquote{Männerchor Murg} mit \enquote{Murg}.

\paragraph{Place}
Die Klasse \code{Place} strukturiert geographische Orte, wie sie z.B. als Absender-, Empfänger- oder 
Veranstaltungsorte im Dokument auftreten. Unterstützt werden neben der Hauptbezeichnung (\code{name}) 
auch alternative Formen (\code{alternate\_place\_name}), sowie standardisierte Identifikatoren aus Geonames, 
Wikidata und Nodegoat. Diese Orte sind über eigene Felder sowohl im Metadatenblock (\code{creation\_place}, 
\code{recipient\_place}) als auch in der Liste \code{mentioned\_places} verortet, falls erstere nicht genau zugeordnet werden können. 
Wie bei Personen, kann auch bei Orten 
über das Flag \code{needs\_review} eine manuelle Überprüfung unsicherer Matches angestossen werden.

\paragraph{Event}
Ereignisse werden über die Klasse \code{Event} abgebildet. Diese enthält einen Namen, eine optionale Beschreibung, 
Datumsangaben und Referenzen auf beteiligte Personen, Orte und Organisationen. Über das Feld \code{inferred} wird angegeben, 
ob das Ereignis direkt im Text genannt oder aus dem Kontext erschlossen wurde. Die genaue Event-Extraktion findet sich 
in \nameref{subsection:event_matcher}.

\paragraph{BaseDocument}
Alle genannten Entitäten werden im zentralen Objekttyp \code{BaseDocument} zusammengeführt. Diese Klasse bildet 
die Grundlage für die strukturierte Speicherung jedes einzelnen Dokuments und enthält unter anderem:
\begin{itemize}
\item einen Attributblock mit \code{document\_type}, \code{object\_type}, \code{creation\_date}, \code{creation\_place}, \code{recipient\_place}
\item Listen der \code{authors}, \code{recipients} und \code{mentioned\_persons}
\item Listen von \code{mentioned\_organizations}, \code{mentioned\_places}, \code{mentioned\_events} und \code{mentioned\_dates}
\item die Originaltranskription (\code{content\_transcription}) sowie inhaltliche Tagging-Kategorien (\code{content\_tags\_in\_german})
\item optionale Zusatzinformationen im Feld \code{custom\_data}, z.B.Zwischenoutputs oder Debug-Daten
\end{itemize}

Zur automatisierten Konvertierung vom oder ins JSON-Format stehen die Methoden \code{to\_dict()}, \code{from\_dict()}, \code{to\_json()} und \code{from\_json()} zur Verfügung. Darüber hinaus erlaubt die Methode \code{validate()} eine strukturierte Prüfung der Einträge auf Konsistenz und Vollständigkeit, etwa im Hinblick auf unvollständige Daten oder ungültige Formate.

\paragraph{Documenttype}
Für die wichtigsten Dokumenttypen existieren spezialisierte Unterklassen wie \code{Brief}, 
\code{Postkarte} oder \code{Protokoll}, die zusätzliche Felder (z.B.\code{greeting}, \code{postmark}, \code{meeting\_type}) aufnehmen. 
Diese können über die zentrale sogenannte \enquote{factory function} \code{create\_document()} automatisch erzeugt werden, wenn ein Dokumenttyp erkannt wurde. 
Diese spezialisierten Erkennungstypen stehen aktuell lediglich für eine spätere Verwendung bereit, und kommen in der aktuellen Anwendung nur in der Benennung des jeweiligen Typs zum Einsatz.

Alle Ergebnisse werden anschliessend in strukturierter Form in eine JSON-Datei geschrieben, wobei jeder Eintrag dem oben definierten Schema 
entspricht. 
\subsubsection{\_\_init\_\_.py}\label{subsec:init_module}
Das Modul \code{\_\_init\_\_.py} fungiert als zentrale Importschnittstelle für alle Funktionalitäten der Projektpipeline. Es aggregiert sämtliche zentralen Komponenten aus den verschiedenen Teilmodulen (z.B. \code{person\_matcher.py}, \code{document\_schemas.py}, \code{place\_matcher.py}) und stellt sie über das \code{\_\_all\_\_}-Array einheitlich zur Verfügung. Dadurch wird eine übersichtliche, modulübergreifende Nutzung der wichtigsten Funktionen und Klassen in anderen Programmteilen (z.B. in \code{transkribus\_to\_base.py}) ermöglicht.
Auf diese Weise kann das Hauptprogramm \code{transkribus\_to\_base.py} auf alle notwendigen Komponenten mit einem einzigen Importbefehl zugreifen, ohne die internen Modulpfade kennen zu müssen. Die Datei übernimmt damit die Funktion einer projektinternen API und gewährleistet eine saubere Trennung zwischen interner Modulstruktur und externer Nutzung. 
\newpage
\begin{minipage}[t]{0.35\textwidth}
  \setstretch{1.5}
  \justifying%

  \subsubsection{Person\_matcher.py}\label{subsec:person_matcher.py}

  Die Erkennung und das Matching von Personen stellt bei weitem die grösste Herausforderung in diesem Projekt dar. Wie vorausgehend bereits angedeutet, gibt es eine Vielzahl von Möglichkeiten, eine Person in einem Text zu benennen oder nur Anzudeuten, was ein zuverlässiges Matching erschwert. Oft ist der Kontext relevant um Personen eindeutig zu identifizieren. Hinzu kommen ganz praktische Probleme. Ein Stringabgleich ist beispielsweise kasus- und genussensitiv. Bei einer Match-Prüfung müssen die   \end{minipage}
  
%================================
%Person PROZESSDIAGRAMM
%================================
  \hfill
\begin{minipage}[t]{0.6\textwidth}
\leavevmode
  %\vspace*{0.7cm} %► schiebt alles um 0.7cm nach unten
\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}


%TikZ styles
\tikzset{
  module/.style={rectangle, draw=black, fill=blue!10, minimum width=0.4cm, minimum height=0.2cm,align=center},
  source/.style={rectangle, draw=black, fill=green!20, minimum width=0.4cm, minimum height=0.2cm,align=center},
  highlight/.style={rectangle, draw=black, fill=blue!40, minimum width=0.4cm, minimum height=0.2cm,align=center},
  arrow/.style={-stealth, line width=0.15mm},
  arrowboth/.style={stealth-stealth, line width=0.15mm},
  process/.style={rectangle, draw=black, fill=orange!20, thick, minimum width=6cm, minimum height=1cm, align=center},
  large/.style={rectangle, draw=black, fill=blue!10, thick, minimum width=4.5cm, minimum height=1.2cm, align=center}
};
%==========================
% Mini-Diagramm oben rechts
%==========================

\begin{scope}[shift={(10.1,0.45)}, node distance=0.3cm]

\node[module] (init) at (0,0) {};
\node[module] (xml) at (0, -0.5) {};
\node[module]    (events)  at (-2.0, -1.3) {};
\node[module]    (roles)   at (-1.2, -1.3) {};
\node[highlight] (persons) at (-0.4, -1.3) {};
\node[module]    (orgs)    at ( 0.4, -1.3) {};
\node[module]    (places)  at ( 1.2, -1.3) {};
\node[module]    (dates)   at ( 2.0, -1.3) {};
\node[module] (authors)    at (-0.4, -2  ) {};

\node[coordinate] (joinpoint) at ($(xml.south) + (0, -1.8cm)$) {};
\node[coordinate] (joinpointxml) at ($(xml.south) + (0, -0.3cm)$) {};

\node[module, below=of joinpoint] (unmatched) {};
\node[module, below=of unmatched] (json) {};
\node[source, below=of json] (save) {};
\node[source, right=0.5cm of save] (unmatchedjson) {};
\draw[arrow, dashed] (persons.south) -- (authors.north);
\draw[arrow] (unmatched.east) -| (unmatchedjson.north);

\draw[arrow] (json.south) -- (save.north);
\draw[arrow] (init.south) -- (xml.north);
\draw[arrowboth] (roles.east) -- (persons.west);
\draw[arrowboth] (persons.east) -- (orgs.west);
\draw[arrowboth] (orgs.east) -- (places.west);
%Pfeile oben
\draw[arrow] (xml.south) -- (joinpointxml.south);
\draw[arrow] (joinpointxml) -- ($(roles.north |- joinpointxml)$) -- (roles.north);
\draw[arrow] (joinpointxml) -- ($(persons.north |- joinpointxml)$) -- (persons.north);
\draw[arrow] (joinpointxml) -- ($(orgs.north |- joinpointxml)$) -- (orgs.north);
\draw[arrow] (joinpointxml) -- ($(places.north |- joinpointxml)$) -- (places.north);
\draw[arrow] (joinpointxml) -- ($(dates.north |- joinpointxml)$) -- (dates.north);
\draw[arrow] (joinpointxml) -- ($(events.north |- joinpointxml)$) -- (events.north);

%Pfeile unten Doppelte Pfeile mit definiertem Stil
\draw[arrow] (events.south) -- ($(events.south |- joinpoint)$);
\draw[arrow] ($(events.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (roles.south) -- ($(roles.south |- joinpoint)$);
\draw[arrow] ($(roles.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (authors.south) -- ($(authors.south |- joinpoint)$);
\draw[arrow] ($(authors.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (orgs.south) -- ($(orgs.south |- joinpoint)$);
\draw[arrow] ($(orgs.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (places.south) -- ($(places.south |- joinpoint)$);
\draw[arrow] ($(places.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (dates.south) -- ($(dates.south |- joinpoint)$);
\draw[arrow] ($(dates.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (joinpoint.south) -- (unmatched.north);
\draw[arrow] (unmatched.south) -- (json.north);
\draw[arrow] (json.south) -- (save.north);
\end{scope}

%==========================
% Haupt-Diagramm
%==========================

\begin{scope}

\node[large] (personsmod) at (0,0) {\textbf{Personen} \\ match, split, enrich};

\node[process, below=1.2cm of personsmod] (split)  {\textbf{def split\_and\_enrich\_persons} \\ \textit{(Extrahiere, verbinde Titel, Kontext-Rolle, Match)}};

\node[process, below=2.5cm of joinpoint] (LoadPers) {\textbf{load\_known\_persons\_from\_csv} \\ Lädt CSV, erzeugt KNOWN\_PERSONS};

\node[source, below=of LoadPers] (GTPers)  {\textbf{export-person.csv} \\ \textit{(Groundtruth CSV)}};

\node[process, below=of split] (parse)  {\textbf{def extract\_person\_data} \\ (Zerlege Namen, erkenne Titel \& Rollen)};

\node[process, below=of parse] (swapped)  {\textbf{def correct\_swapped\_name} \\ (Korrigiere Vor-/Nachname vertauscht)};

\node[process, below=of swapped] (exrawrole)  {\textbf{def extract\_role\_from\_raw\_name} \\ (Trenne Rolle aus Namensstring)};

\node[process, below=of exrawrole] (match)  {\textbf{def match\_person} \\ (Vergleiche mit Groundtruth, OCR/Fuzzy)};

\node[process, below=of match] (inferrole)  {\textbf{def infer\_role\_and\_organisation} \\ (Zuweisung Rolle + Organisation aus Kontext)};

\node[large, left=2.5cm of inferrole] (orgmatch) {\textbf{Organization\_Matcher.py} \\ (Normalisiert Organisation)};

\node[large, above= of orgmatch] (rolematch) {\textbf{Assigned\_Roles\_Module.py} \\ (Normalisiert Rollen)};

\node[process, below=of inferrole] (score)  {\textit{recipient\_score-Berechnung} \\ (intern: Score-Kontextlogik)};

\node[process, below=of score] (dedup)  {\textbf{def deduplicate\_and\_group\_persons} \\ (Gruppiere, merge Scores, Rollen)};

\node[process, below=of dedup] (count)  {\textbf{def count\_mentions\_in\_transcript\_contextual} \\ (Kontext-Zählung der Nennungen)};

% ================
% Mini-Helfer seitlich
% ================
\node[process, left=2.3cm of split] (merge) {\textbf{def merge\_title\_tokens} \\ (Titel + Name zusammenführen)};
\node[process, left=3.1cm of parse] (norm) {\textbf{def normalize\_name\_string} \\ (Namen säubern, Titel trennen)};
\node[process, right=3.8cm of match] (ocr) {\textbf{def ocr\_error\_match} \\ (OCR-Heuristik für Matches)};
\node[process, right=3.5cm of inferrole] (fuzzy) {\textbf{def fuzzy\_match\_name} \\ (Rapidfuzz-Suche)};
\draw[arrow] (GTPers) -- (LoadPers);
%\draw[arrow] (LoadPers) -- (split.east);
\draw[arrow] (LoadPers.west) -- ++(-1.1,0) |- (split.east);



% ================
% Hauptpfeile
% ================
\draw[arrow] (personsmod.south) -- (split.north);
\draw[arrow] (split.south) -- (parse.north);
\draw[arrow] (parse.south) -- (swapped.north);
\draw[arrow] (swapped.south) -- (exrawrole.north);
\draw[arrow] (exrawrole.south) -- (match.north);
\draw[arrow] (match.south) -- (inferrole.north);

\draw[arrow] (exrawrole.west) -| (rolematch.north);
\draw[arrow] (inferrole.west) -- (orgmatch.east);

\draw[arrow] (inferrole.south) -- (score.north);
\draw[arrow] (score.south) -- (dedup.north);
\draw[arrow] (dedup.south) -- (count.north);

\draw[arrow] (rolematch.east) -- ++(1.5,0) |- (inferrole.west);

% ================
% Helferpfeile
% ================
\draw[arrow] (merge.east) -- (split.west);
\draw[arrow] (norm.east) -- (parse.west);
\draw[arrow] (ocr.west) -- (match.east);
\draw[arrow] (match.east) -- ++(1.0,0) |- (fuzzy.west);
%\draw[arrow] (fuzzy.west) -- (match.east);

% ================
% Gruppenbox
% ================
\node[draw=gray, dashed, rounded corners, inner sep=0.5cm, fit=(split)(count)] (group) {};
\end{scope}
\end{tikzpicture}
  }
  \captionof{figure}{\\\small Oben links: \small Prozessdiagramm für \code{Personen\_matcher.py},\\{\small Oben rechts: Pipelineübersicht}}\label{fig:person_matcher.py}
\end{minipage}
\vspace{1em}
\justifying

Algorithmen daher ein Wort im Nominativ Maskulinum genau so erkennen wie im Akkusativ Femininum. Daraus ergibt sich für den Person\_Matcher allein auf heurisitscher Ebene eine grosse Komplexität, die im Folgenden lediglich andeutungsweise dargelegt werden kann.
  
Im Laufe dieser Arbeit sind hierfür vier Parameter entwickelt worden, die diese Zuordnung ermöglichen sollen:
\textbf{Vorname,
Nachname,
Rolle,
Geschlecht} 

Der Aufbau des Moduls gliedert sich wie folgt:\\[0.5em]
\begin{minipage}[t]{0.45\textwidth}
  \setstretch{1.5}
  \justifying
  \begin{enumerate}\small{
    \item Blacklist \& Configuration 
    \item Initialisierung
    \item Namen- und Titelerkennung
    \item Levenshtein-Fallback
    \item Fuzzy-Matching}
  \end{enumerate}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
  \setstretch{1.5}
  \vspace*{0cm}
  \justifying
  \begin{enumerate}
    \setcounter{enumi}{5}
    \small{
    \item Matching
    \item \textit{Extract Person Data} mit Rolleninfos
    \item Split und Enrichment
    \item Deduplication 
    \item Detail‑Info zum besten Match}
  \end{enumerate}
\end{minipage}
\vspace*{0.4cm}\\

Die nachfolgende Erläuterung des Moduls orientiert sich eng an dessen Aufbau im Quellcode, um die interne Struktur und Funktionalität möglichst transparent nachzuvollziehen.

\paragraph{Blacklist \& Configuration}\label{paragraph:Pers_match/Blacklist}

In der Initialisierung des Moduls \code{person\_matcher.py} werden neben der Groundtruth-Tabelle \texttt{export-person.csv} eine Vielzahl domänenspezifischer Konfigurationslisten geladen, um die Erkennung und Validierung von Personennamen zu unterstützen.
\code{NICKNAME\_MAP} liefert beispielsweise ein Dictionary mit üblichen deutschen Spitznamen.\footnote{Durch ChatGPT generiertes Mapping geläufiger deutscher Vor- und Kosenamen auf ihre kanonische Form, einige Fälle sind händisch ergänzt}
Diverse Blacklists statuieren Worte, die beispielsweise nicht menschlich sind (bsp. \texttt{"freundliche Grüsse")}. Zu den Blacklists zählt auch \code{Unmatchable\_Single\_Names}, die auf nicht eindeutig verknüpfbare Namen hinweist.\footnote{\texttt{"Otto"} oder \texttt{"Döbele}" sind so häufig vorkommende Namen, dass sie alleine nicht für ein \nameref{paragraph:Pers_match/Matching} ausreichen} \code{Pronoun} und nach Geschlecht zusammengefasste \code{Title\_Tokens}\footnote{\code{TITLE\_TOKENS = MALE\_TITLE\_TOKENS | FEMALE\_TITLE\_TOKENS | NEUTRAL\_TITLE\_TOKENS}} markieren Tokens zur gezielten Weiterverarbeitung in nachgelagerten Matching- und Anreicherungsschritten. Alle Ausschlusslisten werden in \code{NON\_PERSON\_TOKENS} zusammengeführt und dienen der Negativselektion im gesamten Matchingprozess. 

\paragraph{Thresholds, CSV-Laden und Groundtruth-Erstellung}\label{paragraph:Pers_match/Threstholds}

FÜr das Matching werden zwei essentielle Thresholds definiert, die erreicht werden müssen. für \code{forename} muss eine 80\%  Ähnlichkeit, für \code{familyname} eine 85\% Ähnlichkeit erreicht werden. Diese Zahlen ergeben sich durch unterschiedliche Tests. Sie bieten einen Kompromiss aus Toleranz gegenüber beispielsweise OCR-Fehlern heraus, und liefern dennoch präzise Trefferquoten. Alle Personendaten werden als Liste von Dictionaries\footnote{\code{List[Dict[str, str]]}} aus der Groundtruth geladen. Sie werden in die Variablen \code{KNOWN\_PERSONS}, \code{GROUNDTRUTH\_SURNAMES} und \code{GROUNDTRUTH\_FORENAMES} übergeben, um später gegen die genannten Thresholds zu testen.

\paragraph{ Namensnormalisierung und Titelerkennung}\label{paragraph:Pers_match/Normalisierung}

Für eine konsistente Weiterverarbeitung werden alle erkannten Namenstrings standardisiert, und auf mögliche Rollenstrings überprüft, weil Rollen oft durch das LLM als Person mit markiert werden. Daher durchlaufen die Namestrings zwei Normalisierungen (normalize\_name\_string und normalize\_name). Erstere bereinigt den Text grob und trennt Titel vom Rest. Da in den Quellen Initialen oft vorkommen\footnote{Bsp.:\texttt{"A. Zimmermann"}}, werden zwischen den beiden Funktionen ebenfalls auf eine Initiale geprüft.
Die Funktion \code{normalize\_name()} extrahiert dann eventuell vorhandene Titel mithilfe eines dynamisch aus \code{TITLE\_TOKENS} generierten regulären Ausdrucks. Anschliessend werden Klammern entfernt, Unicode-Zeichen vereinheitlicht (NFKD-Normalisierung), Diakritika\footnote{Sonderbuchstaben, Bsp: \texttt{ä, ê, š, ç}} gelöscht und Sonderzeichen bereinigt.
Die verbleibenden Namensbestandteile werden in Vor- und Nachname aufgeteilt. Dabei berücksichtigt die Funktion auch Nickname-Mappings, indem erkannte Kurzformen wie \texttt{Hansi} automatisch in ihre kanonische Form \texttt{Johannes} überführt werden. Enthält der Name nur ein Token, wird dieses als Vorname gewertet; andernfalls erfolgt eine Trennung in \code{forename} und \code{familyname}. Das Ergebnis ist ein Dictionary mit den Feldern \code{title}, \code{forename} und \code{familyname}, die in späteren Matching- und Review-Schritten verwendet werden. So wird mit \code{normalize\_name\_string} aus \texttt{"Dr. Hansi Müller"}
→ \texttt{\code{forename}:johannes, \code{lastname}: müller, \code{title}: "Dr"}

\paragraph{ Levenshtein-Fallback}\label{paragraph:Pers_match/Fallback}

Nach der Namensnormalisierung folgen zwei zentrale Funktionen zur Fehlerkorrektur und Heuristik-basierter Umkehrung von Namensbestandteilen.

Die Funktion \code{ocr\_error\_match()} verwendet die Levenshtein-Distanz zur Berechnung der Ähnlichkeit zwischen einem gegebenen Namen und einer Liste möglicher Kandidaten, um einzelne Tokens gegen bekannte Groundtruth-Namen zu prüfen. Dabei wird der Eingabename \code{name} zunächst in Kleinschreibung normalisiert (\code{name\_lower}), ebenso wie jeder Kandidat aus der Vergleichsliste (\code{cand\_lower}).
Obwohl beide Seiten formal gleich behandelt werden, liegt der Unterschied in ihrer Herkunft: \code{name\_lower} entstammt der unkorrigierten Texterkennung und kann daher auch Schreibfehler beinhalten, während \code{cand\_lower} aus kuratierten Groundtruth-Listen stammt und damit als Referenz dient.
Die Distanz zwischen beiden wird per \code{Levenshtein.distance()} berechnet; der beste Treffer mit der geringsten Distanz wird als Ergebnis zurückgegeben, ergänzt um einen prozentualen Score.

Darauf aufbauend prüft \code{correct\_swapped\_name()}, ob Vor- und Nachname im gegebenen String möglicherweise vertauscht wurden. Dazu wird jeweils ermittelt, ob der Vorname einem bekannten Nachnamen entspricht oder umgekehrt. Neben direktem Lookup in \code{KNOWN\_FORENAMES} und \code{KNOWN\_SURNAMES} kommt hier ebenfalls ein Levenshtein-Fallback zum Einsatz, um kleine Abweichungen zu tolerieren. Wird auf diese Weise erkannt, dass eine Umkehrung wahrscheinlicher ist, wird das Tupel entsprechend getauscht zurückgegeben.


\paragraph{ Fuzzy-Matching}\label{paragraph:Pers_match/Fuzzy}
Um Namensähnlichkeiten robust zu erkennen, implementiert \code{fuzzy\_match\_name()} ein Fuzzy-Matching-Verfahren auf Basis des \code{fuzz.ratio()}-Scores. Für jedes Vergleichspaar (\code{name}/\code{candidate}) wird anschliessend ein Ähnlichkeitswert berechnet, und der beste Treffer bestimmt. Liegt der höchste Score über dem anfangs definierten Threshold, wird der entsprechende Kandidat samt Score zurückgegeben; andernfalls wird \code{None} geliefert.
Dieses Verfahren ergänzt die Levenshtein-basierte Fehlerbehandlung um eine robustere, Token-basierte Ähnlichkeitsmessung, die auch bei Namensvarianten und OCR-Fehlern zuverlässige Resultate liefert.

\paragraph{ Matching}\label{paragraph:Pers_match/Matching}
Der \code{Person\_Matcher} verwendet nun die vorverarbeiteten Namensstrings als \code{person}, und die wahrscheinlichen Personen der Groundtruth als \code{candidate}. Gleich zu Beginn wird der \subparagraph*{Sonderfall} \textit{"nur Rolle, keine Namen"} geprüft. Wenn die Person nur eine Rolle, aber keinen Namen hat, wird ein Minimal-Objekt mit niedrigem Score zurückgegeben. Zudem erhält das Objekt dann den \code{Review-Reason}:\texttt{"Nur Rolle ohne vollständigen Namen erkannt"}.

\subparagraph*{Inhalts-Filter} Es folgt zur Absicherung eine Normalisierung der Roh-(Vor-)Namen, und ein Check, ob die Strings in den Blacklists (\code{Pronoun\_Tokens},\code{Role\_Tokens}, \code{Non\_Person\_Tokens} vorkommen, um sie frühzeitig auszuschliessen. Diese Fälle liefern für \code{Match:None, 0}

\subparagraph*{Unique-Name} Wenn nur der Vor- oder Nachname vorhanden ist, wird geprüft, ob der Name eindeutig in der Groundtruth vorkommt. Falls ja, wird sofort ein Match ausgegeben.

\subparagraph*{Blacklist-Checks}

Hier werden problematische Tokens vor dem eigentlichen Matching identifiziert und blockiert. Getestet werden \code{Non\_Person\_Tokens} und \code{Role\_Tokens} in zwei if-Blöcken und deren Vorkommen in Vor- oder Nachnamen. Im zweiten Fall (\code{if not ln and fn.lower() in ROLE\_TOKENS}) wird erkannt, ob ein Rollenbegriff fälschlich als Vorname (\code{fn}) identifiziert wurde. In beiden Fällen wird das Matching abgebrochen, die Person mit \code{match\_score = 0} und \code{confidence = "blacklist"} markiert sowie zur manuellen Nachprüfung (\code{needs\_review = True}) ausgegeben.

\subparagraph*{Initialen-Check}
Falls der Vorname \code{fn} lediglich eine Initiale darstellt, wird mit \code{is\_initial(fn)} versucht, einen passenden Eintrag in \code{candidates} zu finden. Dazu wird die Initiale (\code{init = fn[0].upper()}) mit dem ersten Buchstaben des Vornamens jedes Kandidaten verglichen und zusätzlich geprüft, ob der normalisierte Nachname (\code{ln}) mit demjenigen des Kandidaten übereinstimmt. Wird ein solcher Eintrag gefunden, wird er mit hoher Übereinstimmung (\code{match\_score = 95}) zurückgegeben.

\subparagraph*{Fuzzy Matching über Vornamen und Nachnamen}
Liegt ein Vorname \code{fn} mit mindestens drei Zeichen sowie ein Nachname \code{ln} vor, wird zunächst versucht, anhand eines Teilabgleichs zu matchen: Der Anfang des gegebenen Vornamens (\code{fn}) muss mit dem Kandidatennamen übereinstimmen, während der Nachname exakt übereinstimmt (\code{normalize\_name\_string(ln)}). In diesem Fall wird der Treffer mit einem Score von \code{89} zurückgegeben.

Zusätzlich erfolgt eine \emph{Reverse-Prüfung}, um mögliche Vertauschungen von Vor- und Nachname zu erkennen. Dabei wird geprüft, ob die normalisierten ersten vier Zeichen des eingegebenen Nachnamens (\code{ln}) mit dem Vornamen eines Kandidaten übereinstimmen und gleichzeitig die normalisierten ersten vier Zeichen des eingegebenen Vornamens (\code{fn}) mit dessen Nachnamen. Voraussetzung ist ausserdem, dass nicht bereits eine Übereinstimmung von \code{fn} und \code{ln} mit den Feldern \code{forename} und \code{familyname} des Kandidaten vorliegt. Wird diese Bedingung erfüllt, werden die Namensfelder im Rückgabeobjekt vertauscht und der Match mit einem reduzierten Score von \code{88} zurückgegeben.

\subparagraph*{Levenshtein-Fallback bei Namensvertauschung} 
Zunächst wird geprüft, ob der normalisierte Vorname \code{fn} exakt mit dem eines Kandidaten übereinstimmt und der Nachname eine Levenshtein-Distanz von höchstens 1 zum Nachnamen des Kandidaten aufweist. In diesem Fall wird ein Match mit Score \code{90} zurückgegeben.

Anschliessend erfolgt eine Reverse-Prüfung. Dabei wird getestet, ob der Nachname \code{ln} mit dem Vornamen eines Kandidaten übereinstimmt und der Vorname \code{fn} dem Nachnamen des Kandidaten bis auf eine maximale Levenshtein-Distanz von 1 entspricht. Das soll absichern, dass vertauschte Namensbestandteile auch mit geringen OCR-Fehlern korrekt gematched werden.


\subparagraph{Matching über Einzelkomponenten} 

Dieser Abschnitt deckt Fälle ab, in denen nur unvollständige Namensinformationen vorliegen. Ist lediglich ein Nachname vorhanden, wird ein fuzzy-Matching gegen alle Kandidaten-Nachnamen mittels \nameref{paragraph:Pers_match/Fuzzy} durchgeführt; erreicht ein Kandidat den konfigurierten Schwellenwert (\code{thr["familyname"]}), wird dieser mit dem berechneten Score übernommen. Liegen weder Vor- noch Nachname vor, aber eine Rollenbezeichnung (\code{role\_raw}), erfolgt ein Abgleich mit \code{role} und \code{alternate\_name} der Kandidaten; bei Übereinstimmung wird ein Treffer mit Score \code{80} zurückgegeben.

Zusätzlich werden Namensbestandteile, die als generisch oder nicht personenspezifisch klassifiziert sind (\code{NON\_PERSON\_TOKENS}, speziell behandelt. Ist der Vorname ein solcher unerwünschter Token, aber ein Nachname vorhanden, wird dieser gegen alle \code{familyname}-Felder geprüft (Score \code{85}); umgekehrt wird bei generischem Nachnamen der Vorname geprüft. Als letzte Rückfallebene kommt \code{ocr\_error\_match()}\footnote{aus \nameref{paragraph:Pers_match/Fallback}} zum Einsatz: Ist nur ein Nachname vorhanden, wird dieser mit einer Toleranz von bis zu zwei Zeichen mit den Nachnamen aller Kandidaten verglichen; bei Übereinstimmung wird der entsprechende Treffer mit Score \code{85} übernommen. Alle Varianten markieren strukturunscharfe, aber potenziell gültige Entitäten.


\subparagraph{Matching über Nachname und Gender}
Dieser Abschnitt implementiert eine gender-basierte Disambiguierung, die dann greift, wenn der Nachname mit mehreren Personen übereinstimmt, aber das Geschlecht bekannt oder ableitbar ist. Zunächst wird versucht, dieses anhand von Titeln wie \code{"Frau"} oder \code{"Herr"} zu bestimmen. Ist anschliessend sowohl ein Nachname als auch ein valides Geschlecht verfügbar, werden alle Kandidaten mit identischem Nachnamen gesammelt und auf Übereinstimmung des Geschlechts gefiltert. Existiert genau ein solcher Treffer, wird dieser mit hoher Konfidenz (\code{confidence = "gender\_ln\_match"}) und einem Score von \code{95} zurückgegeben, ohne dass ein manueller Review erforderlich ist. Das Matching über Geschlecht und Nachname ist besonders wichtig, da Frauen in den Unterlagen bis auf sehr wenige Ausnahmen nie mit Vornamen genannt werden. Ohne den Geschlechtsabgleich würden sie mit (Ehe-)Männern gematched werden. Diese Funktion versucht also aktiv, gegen den Geschlechterbias in den historischen Unterlagen vorzugehen\cite[Geschlechterbias tritt bei dieser Arbeit in diversen Formen auf, \textit{Source Bias, Archival Bias, Model Bias, und Algorythic Bias}. Letzterer soll mit dieser Funktion abgeschwächt werden. Vgl.:][]{burkhardt_bias_2025}

\subparagraph*{Last Fallback - Aufnahme mit Review}

Falls mindestens eines der Felder \code{fn} (Vorname), \code{ln} (Nachname) oder \code{role\_raw} (Rolle) vorhanden ist, aber kein reguläres Matching möglich war, wird ein Fallback-Objekt erzeugt und zur manuellen Nachprüfung markiert (\code{needs\_review = True}). Die Funktion prüft, welche Felder fehlen, und protokolliert dies als \code{review\_reason} (z.\,B. \enquote{missing\_forename; missing\_role}).

Eine Aufnahme erfolgt nur, wenn entweder der Vor- oder Nachname in einer Groundtruth-Liste vorkommt (\code{appears\_in\_groundtruth()}) oder eine Rollenbezeichnung vorhanden ist. In diesem Fall wird ein Personeneintrag mit \code{confidence = "partial-no-id"} und \code{match\_score = None} zurückgegeben. Andernfalls wird die Person vollständig verworfen.


\paragraph{Extract Person Data mit Rolleninfos}\label{paragraph:Pers_match/Dataextract}

Die Funktion \code{extract\_person\_data(row)} dient der strukturierten Aufbereitung einzelner Personeneinträge. Dafür holt sich die Funktion Informationen über die jeweilige Datenzeile (\code{row}) aus der \code{export-person.csv}. Ziel ist es, aus den Namensangaben und Metadaten ein standardisiertes Personenobjekt zu erzeugen, wie in \nameref{subsec:document_schema} beschrieben.

Der übergebene Namensstring wird zu Beginn nach typischen Titeln durchsucht. Wird ein solcher Titel erkannt, wird er im Feld \code{title} gespeichert und aus dem Namensstring entfernt. Der verbleibende Name wird auf mögliche Rolleninhalte analysiert, die über \code{extract\_role\_from\_raw\_name()} aus dem \nameref{subsection:assigned_roles_module} ermittelt werden. Hierbei wird beispielsweise der String „Schriftführer Ganter“ in die Bestandteile \code{role=Schriftführer} und \code{name=Ganter} aufgetrennt.

Anschliessend wird der bereinigte Namensstring in Einzelteile zerlegt. Das erste Token wird als Vorname, das letzte als Nachname interpretiert. Um typische OCR-Fehler oder Namensinversionen zu korrigieren, wird nochmal die Funktion \code{correct\_swapped\_name()} aufgerufen, welche Vor- und Nachnamen bei Bedarf vertauscht.

Für den Rollenteil des Eintrags wird zunächst das Feld \code{role} aus der ursprünglichen CSV-Zeile übernommen. Falls bereits beim Parsing eine Rolle erkannt wurde, wird diese priorisiert. Der erkannte Rollentext wird dann durch \code{normalize\_and\_match\_role()} in eine standardisierte Form überführt. Über \code{map\_role\_to\_schema\_entry()} erfolgt anschliessend die semantische Abbildung auf ein normiertes Rollenschema\footnote{\code{role} ist der detektierte String, \code{role\_schema} die normalisierte Form}.

Auch das Geschlecht der Person wird in diesem Schritt bestimmt. Dazu wird zunächst das übergebene Geschlechtsfeld bereinigt und vereinheitlicht. Sollte dieses keine gültige Angabe enthalten, wird als Fallback \code{infer\_gender\_for\_person()} aufgerufen. Diese Funktion versucht, das Geschlecht über den Titel oder durch Abgleich mit der Groundtruth-Liste zu ermitteln.

Neben Namen, Titel, Rolle und Geschlecht werden auch zusätzliche Kontextinformationen aus der Groundtruth extrahiert, sofern vorhanden. Dazu zählen \code{alternate\_name}, der Wohnort und weitere. So kann für jedes Dokument garantiert werden, dass alle verfügbaren Daten für eine gegebenenfalls nötige manuelle Prüfung vorhanden sind. Plausibilitätsprüfungen funktionieren so unmittelbar, ohne Zwischenschritte.

Die Funktion erzeugt abschliessend ein vollständiges Personenobjekt mit Attributen wie \code{confidence}, \code{match\_score} und \code{needs\_review}, die für spätere Matching- und Validierungsschritte relevant sind. Personen ohne Nodegoat-ID werden standardmässig zur manuellen Nachprüfung markiert.

Zur systematischen Dokumentation unvollständiger Einträge wird zusätzlich die Funktion \code{get\_review\_reason\_for\_person()} bereitgestellt. Sie prüft, ob für die betreffende Person kritische Felder wie Vorname, Nachname oder Rolle fehlen und erzeugt eine entsprechende Review-Kennzeichnung. 

Für Sonderfälle, bei denen keine Namen, sondern lediglich Rollentitel wie \code{„Vereinsführer“} erkannt wurden, dient die Funktion \code{detect\_and\_convert\_role\_only\_entries()}. Sie prüft, ob das einzige übergebene Token einem bekannten Rollenschema entspricht. Ist dies der Fall, wird eine strukturierte Rollenangabe ohne Namensinformation erzeugt und ebenfalls zur Überprüfung markiert.

Schliesslich wird in Zusammenarbeit mit dem \code{Assigned\_Roles\_Module} mit \code{extract\_metadata\_names()} eine Funktion bereitgestellt, um auch aus dem Fliesstext zusätzliche Personenhinweise zu extrahieren. Hierbei kommen wieder RegEx zum Einsatz, die typische Anredeformen oder Signaturen am Briefende erkennen und zur weiteren Analyse aufbereiten.




\paragraph{ Split und Enrichment}\label{paragraph:Pers_match/Split&Enrich}

\paragraph{ Deduplication }\label{paragraph:Pers_match/Dedub}

\paragraph{ Detail‑Info zum besten Match}\label{paragraph:Pers_match/DetailBestMatch}






\newpage
\begin{minipage}[t]{0.38\textwidth}
  \setstretch{1.5}
  \justifying%

  \subsubsection{Assigned\_Roles\_Module.py}\label{subsection:assigned_roles_module}

  Hier steht eine Menge schöner Text

  \end{minipage}%
% ================================
% Roles PROZESSDIAGRAMM
% ================================
  \hfill
\begin{minipage}[t]{0.55\textwidth}
  \vspace*{0.7cm} % ► schiebt alles um 0.7cm nach unten
\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}
% TikZ styles
\tikzset{
  module/.style={rectangle, draw=black, fill=blue!10, minimum width=0.4cm, minimum height=0.2cm},
  source/.style={rectangle, draw=black, fill=green!20, minimum width=0.4cm, minimum height=0.2cm,align=center},
  highlight/.style={rectangle, draw=black, fill=blue!40, minimum width=0.4cm, minimum height=0.2cm},
  arrow/.style={-stealth, line width=0.15mm},
  arrowboth/.style={stealth-stealth, line width=0.15mm},
  process/.style={rectangle, draw=black, fill=orange!20, thick, minimum width=6cm, minimum height=1cm, align=center},
  large/.style={rectangle, draw=black, fill=blue!10, thick, minimum width=4.5cm, minimum height=1.2cm, align=center}
};
%==========================
% Mini-Diagramm oben rechts
%==========================

\begin{scope}[shift={(9,0.45)}, node distance=0.3cm]
\node[module] (init) at (0,0) {};
\node[module] (xml) at (0, -0.5) {};
\node[module]    (events)  at (-2.0, -1.3) {};
\node[highlight]    (roles)   at (-1.2, -1.3) {};
\node[module] (persons) at (-0.4, -1.3) {};
\node[module]    (orgs)    at ( 0.4, -1.3) {};
\node[module]    (places)  at ( 1.2, -1.3) {};
\node[module]    (dates)   at ( 2.0, -1.3) {};
\node[module] (authors)    at (-0.4, -2  ) {};

\node[coordinate] (joinpoint) at ($(xml.south) + (0, -1.8cm)$) {};
\node[coordinate] (joinpointxml) at ($(xml.south) + (0, -0.3cm)$) {};

\node[module, below=of joinpoint] (unmatched) {};
\node[module, below=of unmatched] (json) {};
\node[source, below=of json] (save) {};
\node[source, right=0.5cm of save] (unmatchedjson) {};
\draw[arrow, dashed] (persons.south) -- (authors.north);
\draw[arrow] (unmatched.east) -| (unmatchedjson.north);

\draw[arrow] (json.south) -- (save.north);
\draw[arrow] (init.south) -- (xml.north);
\draw[arrowboth] (roles.east) -- (persons.west);
\draw[arrowboth] (persons.east) -- (orgs.west);
\draw[arrowboth] (orgs.east) -- (places.west);
%Pfeile oben
\draw[arrow] (xml.south) -- (joinpointxml.south);
\draw[arrow] (joinpointxml) -- ($(roles.north |- joinpointxml)$) -- (roles.north);
\draw[arrow] (joinpointxml) -- ($(persons.north |- joinpointxml)$) -- (persons.north);
\draw[arrow] (joinpointxml) -- ($(orgs.north |- joinpointxml)$) -- (orgs.north);
\draw[arrow] (joinpointxml) -- ($(places.north |- joinpointxml)$) -- (places.north);
\draw[arrow] (joinpointxml) -- ($(dates.north |- joinpointxml)$) -- (dates.north);
\draw[arrow] (joinpointxml) -- ($(events.north |- joinpointxml)$) -- (events.north);

%Pfeile unten Doppelte Pfeile mit definiertem Stil
\draw[arrow] (events.south) -- ($(events.south |- joinpoint)$);
\draw[arrow] ($(events.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (roles.south) -- ($(roles.south |- joinpoint)$);
\draw[arrow] ($(roles.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (authors.south) -- ($(authors.south |- joinpoint)$);
\draw[arrow] ($(authors.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (orgs.south) -- ($(orgs.south |- joinpoint)$);
\draw[arrow] ($(orgs.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (places.south) -- ($(places.south |- joinpoint)$);
\draw[arrow] ($(places.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (dates.south) -- ($(dates.south |- joinpoint)$);
\draw[arrow] ($(dates.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (joinpoint.south) -- (unmatched.north);
\draw[arrow] (unmatched.south) -- (json.north);
\draw[arrow] (json.south) -- (save.north);
\end{scope}


%==========================
% Hauptprogramm Assigned_roles_module
%==========================
\begin{scope}
\node[large] (rolesmod) at (0,0) {\textbf{Assigned\_Roles\_Module.py} \\ match, split, enrich};

\node[process, below=1.2cm of rolesmod] (normalize)  {\textbf{def normalize\_and\_match\_role} \\ (Normalisiert Rollen mit Role\_Mappings\_DE)};

\node[source, below= 3.6cm of joinpointxml] (ROLE)  {\textbf{Role\_Mappings\_DE} \\ (interne Lookup-Tabelle)};

\node[process, below=of normalize] (loadknown)  {\textbf{def load\_known\_persons} \\ (Lädt bekannte Personen aus GT-CSV)};

\node[source, below= of ROLE] (PERSONGT)  {\textbf{export-person.csv} \\ (Groundtruth Personen)};

\node[process, below=of loadknown] (maprole)  {\textbf{def map\_role\_to\_schema\_entry} \\ (Wandelt normierte Rolle in Dict um)};

\node[process, below=of maprole] (assign)  {\textbf{def assign\_roles\_to\_known\_persons} \\ (Weist Rollen zu bekannten Personen zu)};

\node[process, below=of assign] (findindx)  {\textbf{def find\_line\_index\_for\_person} \\ (Findet Person im Text)};

\node[process, below=of findindx] (search)  {\textbf{def search\_roles\_nearby} \\ (Sucht Rollen im Umkreis)};

\node[source, right=3.5cm of search] (ROLEGTSOURCE)  {\textbf{export-roles.csv} \\ (Groundtruth Rollen)};

\node[process, below=of search] (standalone)  {\textbf{def extract\_standalone\_roles} \\ (Findet neue Personen+Rollen-Kombis)};

\node[process, below=of standalone] (intoken)  {\textbf{def extract\_role\_in\_token} \\ (Token-Analyse Rolle+Name)};

% === Pfeile ===
\draw[arrow] (rolesmod.south) -- (normalize.north);
\draw[arrow] (normalize.south) -- (loadknown.north);
\draw[arrow] (loadknown.south) -- (maprole.north);
\draw[arrow] (maprole.south) -- (assign.north);
\draw[arrow] (assign.south) -- (findindx.north);
\draw[arrow] (findindx.south) -- (search.north);
\draw[arrow] (search.south) -- (standalone.north);
\draw[arrow] (standalone.south) -- (intoken.north);

% === Ressource-Pfeile ===
\draw[arrow] (ROLE.west) -- ++(-1.4,0) |- (normalize.east);
\draw[arrow] (PERSONGT.west) -- ++(-1.5,0) |-  (loadknown.east);
\draw[arrow] (ROLEGTSOURCE.west) -- (search.east);

% === Umrandung ===
\node[draw=gray, dashed, rounded corners, inner sep=0.5cm, fit=(normalize)(intoken)] (group) {};
\end{scope}

\end{tikzpicture}
}
\captionof{figure}{\\\small Links: \small Prozessdiagramm für \code{Assigned\_Roles\_Module.py},\\{\small Rechts: Pipelineübersicht}}\label{fig:assigned_roles_module}
\end{minipage}





\newpage
\begin{minipage}[t]{0.38\textwidth}
  \setstretch{1.5}
  \justifying%

  \subsubsection{place\_matcher.py}\label{subsec:place_matcher_chapter}
    
  Hier steht eine Menge schöner Text

  Ortsnamenserkennung basiert stark auf GT, beispielsweise Murg 26 unterschiedliche Alternativnamen, daher Code vergleichsweise kurz

  \end{minipage}%
% ================================
% PlaceMatcher PROZESSDIAGRAMM
% ================================
  \hfill
\begin{minipage}[t]{0.55\textwidth}
  \vspace*{0.7cm} % ► schiebt alles um 0.7cm nach unten
\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}


% =============================
% TikZ Styles
% =============================
\tikzset{
  module/.style={rectangle, draw=black, fill=blue!10, minimum width=0.4cm, minimum height=0.2cm},
  source/.style={rectangle, draw=black, fill=green!20, minimum width=0.4cm, minimum height=0.2cm, align=center},
  highlight/.style={rectangle, draw=black, fill=blue!40, minimum width=0.4cm, minimum height=0.2cm},
  process/.style={rectangle, draw=black, fill=orange!20, thick, minimum width=6cm, minimum height=1cm, align=center},
  large/.style={rectangle, draw=black, fill=blue!10, thick, minimum width=4.5cm, minimum height=1.2cm, align=center},
  arrow/.style={-stealth, line width=0.15mm},
  arrowboth/.style={stealth-stealth, line width=0.15mm}
};
%==========================
% Mini-Diagramm Placematcher
%==========================

\begin{scope}[shift={(9,0.45)}, node distance=0.3cm]

\node[module] (init) at (0,0) {};
\node[module] (xml) at (0, -0.5) {};
\node[module]    (events)  at (-2.0, -1.3) {};
\node[module]    (roles)   at (-1.2, -1.3) {};
\node[module] (persons) at (-0.4, -1.3) {};
\node[module]    (orgs)    at ( 0.4, -1.3) {};
\node[module]    (places)  at ( 1.2, -1.3) {};
\node[highlight]    (dates)   at ( 2.0, -1.3) {};
\node[module] (authors)    at (-0.4, -2  ) {};

\node[coordinate] (joinpoint) at ($(xml.south) + (0, -1.8cm)$) {};
\node[coordinate] (joinpointxml) at ($(xml.south) + (0, -0.3cm)$) {};

\node[module, below=of joinpoint] (unmatched) {};
\node[module, below=of unmatched] (json) {};
\node[source, below=of json] (save) {};
\node[source, right=0.5cm of save] (unmatchedjson) {};
\draw[arrow, dashed] (persons.south) -- (authors.north);
\draw[arrow] (unmatched.east) -| (unmatchedjson.north);

\draw[arrow] (json.south) -- (save.north);
\draw[arrow] (init.south) -- (xml.north);
\draw[arrowboth] (roles.east) -- (persons.west);
\draw[arrowboth] (persons.east) -- (orgs.west);
\draw[arrowboth] (orgs.east) -- (places.west);
%Pfeile oben
\draw[arrow] (xml.south) -- (joinpointxml.south);
\draw[arrow] (joinpointxml) -- ($(roles.north |- joinpointxml)$) -- (roles.north);
\draw[arrow] (joinpointxml) -- ($(persons.north |- joinpointxml)$) -- (persons.north);
\draw[arrow] (joinpointxml) -- ($(orgs.north |- joinpointxml)$) -- (orgs.north);
\draw[arrow] (joinpointxml) -- ($(places.north |- joinpointxml)$) -- (places.north);
\draw[arrow] (joinpointxml) -- ($(dates.north |- joinpointxml)$) -- (dates.north);
\draw[arrow] (joinpointxml) -- ($(events.north |- joinpointxml)$) -- (events.north);

%Pfeile unten Doppelte Pfeile mit definiertem Stil
\draw[arrow] (events.south) -- ($(events.south |- joinpoint)$);
\draw[arrow] ($(events.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (roles.south) -- ($(roles.south |- joinpoint)$);
\draw[arrow] ($(roles.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (authors.south) -- ($(authors.south |- joinpoint)$);
\draw[arrow] ($(authors.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (orgs.south) -- ($(orgs.south |- joinpoint)$);
\draw[arrow] ($(orgs.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (places.south) -- ($(places.south |- joinpoint)$);
\draw[arrow] ($(places.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (dates.south) -- ($(dates.south |- joinpoint)$);
\draw[arrow] ($(dates.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (joinpoint.south) -- (unmatched.north);
\draw[arrow] (unmatched.south) -- (json.north);
\draw[arrow] (json.south) -- (save.north);
\end{scope}


% =============================
% Haupt-Block: PlaceMatcher
% =============================
\begin{scope}
\node[large] (placematcher) at (0,0) {\textbf{PlaceMatcher.py} \\ load, match, deduplicate};

% CSV links von init
\node[source, below=2.5cm of joinpoint] (CSV) {\textbf{export-place.csv} \\ known places};

% Hauptfluss vertikal
\node[process, below=1.2cm of placematcher] (initp) {\textbf{\_\_init\_\_ + sanitize\_id} \\ (Lädt CSV, alt\_name\_map, known\_name\_map)};

\node[process, below=of initp] (normalize) {\textbf{\_normalize\_place\_name} \\ (Normiert Ortsnamen)};

\node[process, below=of normalize] (match) {\textbf{match\_place} \\ (Exact, Fuzzy, Kontext, API)};

\node[process, below=of match] (buildres) {\textbf{\_build\_match\_result} \\ (APIs ergänzen, Wikidata enrich)};

\node[process, below=of buildres] (dedup) {\textbf{deduplicate\_places} \\ (Gruppiert nach ID)};

\node[process, below=of dedup] (pipeline) {\textbf{enrich\_and\_deduplicate} \\ (Pipeline Wrapper)};

\node[process, below=of pipeline] (output) {\textbf{mentioned\_places\_from\_custom\_data} \\ (Erzeugt Place-Objekte)};

% Abzweige rechts
\node[process, left=1.5cm of match] (context) {\textbf{\_generate\_combined\_place\_names} \\ (Kontext-Kombis)};

\node[source, below=4.8cm of joinpoint] (geoapi) {\textbf{Geonames API}};
\node[source, below=0.8cm of geoapi] (wikiapi) {\textbf{Wikidata SPARQL}};

\node[process, left=1.5cm of buildres] (log) {\textbf{log\_unmatched\_place} \\ (Speichert unmatched)};

% Verbindungen Hauptfluss
\draw[arrow] (placematcher.south) -- (initp.north);
\draw[arrow] (initp.south) -- (normalize.north);
\draw[arrow] (normalize.south) -- (match.north);
\draw[arrow] (match.south) -- (buildres.north);
\draw[arrow] (buildres.south) -- (dedup.north);
\draw[arrow] (dedup.south) -- (pipeline.north);
\draw[arrow] (pipeline.south) -- (output.north);

% CSV-Input
\draw[arrow] 
  (CSV.west)
  -- ++(-1,0)       % 1 cm rechts
  |- (initp.east); % dann runter/hoch bis initp.west

% Kontext-Pfeil
\draw[arrowboth] (match.west) -- (context.east);

% APIs

\draw[arrow]
  % 1) Verschobener Start um 0,3 cm nach oben
  ($(buildres.east)+(0cm,0.3cm)$)
  % 2) von dort 1 cm nach rechts
  -- ++(1cm,0)
  % 3) und dann im L-Bogen zur GeoAPI
  |- (geoapi.west);

% Pfeil zu Wiki
\coordinate (pivot) at ($(buildres.east)+(0cm,-0.3)$);
\draw[arrow] 
  (wikiapi.south)    % wieder am buildres
  -- ++(0,0)       % dieselbe Hilfsstrecke
  |- (pivot);

\draw[arrow] (geoapi.south) -- (wikiapi.north);


% Logging
\draw[arrow] (buildres.west) -- ++(0,0) |- (log.east);

% Optional: normalize nutzt context auch
% \draw[arrow, dashed] (normalize.east) -- ++(0.5,0) |- (context.west);

% Umrandung
\node[draw=gray, dashed, rounded corners, inner sep=0.5cm, fit=(initp)(output)(log)(context)] {};
\end{scope}

\end{tikzpicture}
}
\captionof{figure}{\\\small Links: \small Prozessdiagramm für \code{place\_matcher.py},\\{\small Rechts: Pipelineübersicht}}\label{fig:placematcher_final}
\end{minipage}


\begin{minipage}[t]{0.38\textwidth}
  \setstretch{1.5}
  \justifying%

\subsubsection{organization\_matcher.py}\label{subsection:organization_matcher}

  \code{organization\_matcher.py} dient der Erkennung und Normalisierung von Organisationen in historischen Transkripten. Es kombiniert reguläre Ausdrücke mit unscharfem Vergleich (fuzzy matching), um eingegebene Strings mit bekannten Organisationen aus der Groundtruth-Datei \texttt{export-organisationen.csv} abzugleichen.

  \noindent \code{extract\_organization(\{\})} bereinigt im nächsten Schritt die Eingabestrings zunächst, indem 
\end{minipage}%
\hfill
\begin{minipage}[t]{0.55\textwidth}
  \vspace*{0.7cm} % ► schiebt alles um 0.7cm nach unten
\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}
% =============================
% TikZ Styles
% =============================
\tikzset{
  module/.style={rectangle, draw=black, fill=blue!10, minimum width=0.4cm, minimum height=0.2cm},
  source/.style={rectangle, draw=black, fill=green!20, minimum width=0.4cm, minimum height=0.2cm, align=center},
  highlight/.style={rectangle, draw=black, fill=blue!40, minimum width=0.4cm, minimum height=0.2cm},
  process/.style={rectangle, draw=black, fill=orange!20, thick, minimum width=6cm, minimum height=1cm, align=center},
  large/.style={rectangle, draw=black, fill=blue!10, thick, minimum width=4.5cm, minimum height=1.2cm, align=center},
  arrow/.style={-stealth, line width=0.15mm},
  arrowboth/.style={stealth-stealth, line width=0.15mm}
};

% =============================
% Mini-Diagramm oben rechts
% =============================
\begin{scope}[shift={(9,0.45)}, node distance=0.3cm]
\node[module] (init) at (0,0) {};
\node[module] (xml) at (0, -0.5) {};
\node[module] (events)  at (-2.0, -1.3) {};
\node[module] (roles)   at (-1.2, -1.3) {};
\node[module] (persons) at (-0.4, -1.3) {};
\node[highlight] (orgs) at ( 0.4, -1.3) {}; % Highlight für Org-Matcher
\node[module] (places)  at ( 1.2, -1.3) {};
\node[module] (dates)   at ( 2.0, -1.3) {};
\node[module] (authors) at (-0.4, -2  ) {};

\node[coordinate] (joinpoint) at ($(xml.south) + (0, -1.8cm)$) {};
\node[coordinate] (joinpointxml) at ($(xml.south) + (0, -0.3cm)$) {};

\node[module, below=of joinpoint] (unmatched) {};
\node[module, below=of unmatched] (json) {};
\node[source, below=of json] (save) {};
\node[source, right=0.5cm of save] (unmatchedjson) {};

\draw[arrow, dashed] (persons.south) -- (authors.north);
\draw[arrow] (unmatched.east) -| (unmatchedjson.north);
\draw[arrow] (json.south) -- (save.north);
\draw[arrow] (init.south) -- (xml.north);
\draw[arrowboth] (roles.east) -- (persons.west);
\draw[arrowboth] (persons.east) -- (orgs.west);
\draw[arrowboth] (orgs.east) -- (places.west);
\draw[arrowboth] (places.east) -- (dates.west);

\draw[arrow] (xml.south) -- (joinpointxml.south);
\draw[arrow] (joinpointxml) -- ($(roles.north |- joinpointxml)$) -- (roles.north);
\draw[arrow] (joinpointxml) -- ($(persons.north |- joinpointxml)$) -- (persons.north);
\draw[arrow] (joinpointxml) -- ($(orgs.north |- joinpointxml)$) -- (orgs.north);
\draw[arrow] (joinpointxml) -- ($(places.north |- joinpointxml)$) -- (places.north);
\draw[arrow] (joinpointxml) -- ($(dates.north |- joinpointxml)$) -- (dates.north);
\draw[arrow] (joinpointxml) -- ($(events.north |- joinpointxml)$) -- (events.north);

\draw[arrow] (events.south) -- ($(events.south |- joinpoint)$);
\draw[arrow] ($(events.south |- joinpoint)$) -- (joinpoint);
\draw[arrow] (roles.south) -- ($(roles.south |- joinpoint)$);
\draw[arrow] ($(roles.south |- joinpoint)$) -- (joinpoint);
\draw[arrow] (authors.south) -- ($(authors.south |- joinpoint)$);
\draw[arrow] ($(authors.south |- joinpoint)$) -- (joinpoint);
\draw[arrow] (orgs.south) -- ($(orgs.south |- joinpoint)$);
\draw[arrow] ($(orgs.south |- joinpoint)$) -- (joinpoint);
\draw[arrow] (places.south) -- ($(places.south |- joinpoint)$);
\draw[arrow] ($(places.south |- joinpoint)$) -- (joinpoint);
\draw[arrow] (dates.south) -- ($(dates.south |- joinpoint)$);
\draw[arrow] ($(dates.south |- joinpoint)$) -- (joinpoint);
\draw[arrow] (joinpoint.south) -- (unmatched.north);
\draw[arrow] (unmatched.south) -- (json.north);
\draw[arrow] (json.south) -- (save.north);
\end{scope}

% =============================
% Haupt-Block: Organization Matcher
% =============================

\begin{scope}
\node[large] (orgmatcher) at (0,0) {\textbf{Organization\_matcher.py}\\clean, match, deduplicate};

% Groundtruth CSV links
\node[source, below= 3.6cm of joinpointxml] (CSV) {\textbf{export-organisationen.csv}\\export-organisationen.csv};

% Load
\node[process, below=1.2cm of orgmatcher] (load) {\textbf{load\_organizations\_from\_csv} \\ (Lädt Orgs + Aliase)};

% Global List
\node[process, below=of load] (known) {\textbf{KNOWN\_ORGS} \\ (Globale Liste aller Orgs)};

% Cleaning
\node[process, below=of known] (extract) {\textbf{extract\_organization} \\ (Säubert Name, entfernt Müll)};

% Normalize
\node[process, below=of extract] (normalize) {\textbf{normalize\_org\_name} \\ (Normiert Name für Fuzzy)};

% Special Case
\node[process, left=1.5cm of normalize] (acro) {\textbf{match\_acronym} \\ (Akronym-Fall)};

% Main Matcher
\node[process, below=of normalize] (match) {\textbf{match\_organization} \\ (Exact + Fuzzy Vergleich)};

% Entities Matcher
\node[process, below=of match] (entities) {\textbf{match\_organization\_entities} \\ (Mehrere Tokens + Kombis)};

% Fulltext
\node[process, below=of entities] (fulltext) {\textbf{match\_organization\_from\_text} \\ (Ganzer Text auf Orgs prüfen)};

% Direct Lookup
\node[process, below=of fulltext] (byname) {\textbf{match\_organization\_by\_name} \\ (Direkter Einzelaufruf)};

% Wikidata
\node[process, left=1.5cm of extract] (wikidata) {\textbf{extract\_wikidata\_id} \\ (Wikidata-ID extrahieren)};

% === Pfeile ===
\draw[arrow] (orgmatcher.south) -- (load.north);
\draw[arrow] (load.south) -- (known.north);
\draw[arrow] (known.south) -- (extract.north);
\draw[arrow] (extract.south) -- (normalize.north);
\draw[arrow] (normalize.south) -- (match.north);
\draw[arrow] (match.south) -- (entities.north);
\draw[arrow] (entities.south) -- (fulltext.north);
\draw[arrow] (fulltext.south) -- (byname.north);

% CSV Verbindung
\draw[arrow]
  (load.east)       % Start am load-Knoten
  -- ++(1cm,0)      % 1 cm nach rechts (spiegelt das ursprüngliche -- ++(-1,0))
  |- ($(CSV.west)+(0cm,0.3cm)$);    % dann im L-Bogen zurück zum CSV-Knoten

\draw[arrow]
  % Start 0,3 cm unter CSV.west
  ($(CSV.west)+(0cm,-0.3cm)$)
  % dann gerade zur known.east
  -- (known.east);

% Branches
\draw[arrow] (normalize.west) -- (acro.east);
\draw[arrow] (extract.west) -- (wikidata.east);

% Acronym-Verbindung zu Main Matcher
\draw[arrow] (acro.south) |- (match.east);

% Umrandung
\node[draw=gray, dashed, rounded corners, inner sep=0.5cm, fit=(load)(byname)(wikidata)(acro)] {};
\end{scope}

\end{tikzpicture}
}
\captionof{figure}{
  \small
  Links: Prozessdiagramm für \texttt{Organization\_matcher.py},\\
  Rechts: Pipelineübersicht
}
\label{fig:organization_matcher}
\end{minipage}
\vspace{1em}
\justifying

sie umschliessende Klammern entfernt, innere Satzzeichen säubert und führende bzw. abschliessende Interpunktion streicht. Darüber hinaus wird eine Blacklist abgeglichen, um etwa Einträge wie \enquote{Verein} als alleinstehende Organisation auszuschliessen - ein klares Match kann hier algorithmisch vorerst nicht gewährleistet werden.

Die bekannten Organisationen werden über die Funktion \code{load\_organizations\_from\_csv()} eingelesen. Dabei entsteht eine strukturierte Liste von Dictionaries, die mit den Anforderungen aus \code{document\_schemas.py} kompatibel ist und Hauptnamen, alternative Bezeichnungen sowie Identifier enthält.

Um die Organisationen vergleichbar zu machen wird in \code{def normalize\_org\_name ( )} in drei schritten normalisiert.

Zunächst werden mithilfe einer RegEx\footnote{\textbf{Abk.:} Regular Expression} sämtliche Zeichen entfernt, die nicht alphanumerisch sind. Erlaubt bleiben dabei Gross- und Kleinbuchstaben (einschliesslich deutscher Umlaute), Ziffern sowie Leerzeichen. So wird beispielsweise aus dem Ausdruck \enquote{Männerchor Murg e.V.} der bereinigte String \enquote{Männerchor Murg eV}.

Im zweiten Schritt wird der bereinigte String durch \code{.lower()} vollständig in Kleinbuchstaben transformiert.
Abschliessend werden mit \code{.strip()} etwaige führende oder nachfolgende Leerzeichen entfernt, sodass der Rückgabewert für weitere Vergleichsoperationen normiert vorliegt. Verdeutlichend wird aus:
\begin{verbatim}
Input:  "  (Männerchor Murg e.V.)  "
Output: "männerchor murg ev"
\end{verbatim}
Um nicht nur vollständige Namen zu prüfen, sondern auch Akronyme\footnote{Abkürzungen}, wird gezielt nach solchen in den \texttt{alternate\_names} bekannter Organisationen gesucht.
 
Das Matching der Organisationen erfolgt im Anschluss in den drei zentralen Funktionen \code{match\_organization()}, \code{match\_organization\_from\_text()} und \code{match\_organization\_entities()}.

Die Funktion \code{match\_organization()} bildet dabei den Kern des Vergleichsprozesses. Sie prüft zunächst, ob es sich bei der Eingabe um ein kurzes Akronym handelt und nutzt hierfür die Funktion \code{match\_acronym()}. Falls kein solcher Sonderfall vorliegt, wird der Eingabewert mithilfe von Fuzzy Matching mit \code{fuzz.ratio} gegen alle bekannten Organisationsnamen sowie deren alternative Bezeichnungen geprüft. Das Ergebnis ist das bestbewertete Match mit einem Score oberhalb eines standardmässig auf vordefinierten Schwellenwerts .

Die Funktion \code{match\_organization\_from\_text()} erweitert diesen Vergleich auf vollständige Transkriptionstexte. Dabei wird zunächst überprüft, ob ein bekannter Organisationsname direkt im Text vorkommt. Falls dies nicht der Fall ist, wird ein fuzzy-Teilvergleich durchgeführt (\code{fuzz.partial\_ratio}), um auch unvollständige oder abweichende Schreibweisen zu erkennen.

\code{match\_organization\_entities()} verarbeitet eine Liste roher Organisationseinträge aus der Extraktion und bereinigt sie mit \code{extract\_organization()}. Zudem werden aufeinanderfolgende Namen wie \enquote{Männerchor} und \enquote{Murg} zu einem kombinierten Eintrag zusammengeführt. Anschliessend wird für jeden dieser bereinigten Namen ein fuzzy-Matching gegen die Groundtruth durchgeführt, wobei Treffer mit Score und \code{confidence = "fuzzy"} zurückgegeben werden.

\subsubsection{letter\_metadata\_matcher.py}\label{subsection:letter_metadata_matcher}

Der letter\_metadata\_matcher widmet sich auf der Extraktion inhaltlicher Metadaten aus Briefen und Postkarten. Vornehmlich hier Informationen über die Autorenschaft, den Empfängern und Empfängerinnen sowie deren Rollen. Darüber hinaus sollen auch Daten und Orte nachvollzogen werden, mit einem Fokus auf den Absende- und Empfangsort. Da diese Briefe und Postkarten sehr ähnlich strukturiert sind, liegt der Fokus dieses Moduls auf der Extraktion ihrer Daten.

\paragraph{Pattern Matching in Briefen und Postkarten}
Die zentrale Annahme ist, dass diese Metadaten meist im Textkörper selbst enthalten sind und anhand wiederkehrender sprachlicher Muster bzw. Indikatoren erkannt werden können. Ein Autor oder eine Autorin beendet einen Brief beispielsweise in der Regel mit einer Grussformel und einer Unterschrift. Die Pipeline macht sich das zunutze, indem diese Patterns als Marker in einem String dienen, und zusätzlich zu den Custom-Tags im XML File auf eine Autorenschaft hinweisen können. Diese Marker sind in fünf Listen auf RegEx-Basis notiert.
\begin{itemize}
    \item \code{GREETING\_PATTERNS} als Schlussformel in einem Brief zeigt die Autorenschaft an. \\Beispiel: \texttt{mit freundlichen Grüßen} oder \texttt{Heil Hitler}.
    \item \code{RECIPIENT\_PATTERNS} Erkennt direkte Anreden von Empfänger:innen im Text. \\Beispiel: \texttt{An Herrn/Frau} oder \texttt{Lieber Fritz}
    \item \code{direct\_patterns} zeigen die unmittelbaren Empfänger eines Briefes an.
    \item \code{INDIRECT\_RECIPIENT\_PATTERNS} zeigen an, dass ein Brief oder Postkarte an einen Zwischenempfänger geht. \\Beispiel: \texttt{zu Händen von}
    \item \code{SIGNATURE\_PATTERNS} ist eine auf das nachfolgende ROLE\_PATTERNS stützende RegEx, die reine Rollenzeilen am Ende eines Briefes erkennt. Beispiel:\\
    \texttt{der Vereinsführer\\Alfons Zimmermann}. \\ 
    \item \code{ROLE\_PATTERNS} ist eine dynamisch generierte RegEx, die auf der Rolle basierende Signaturen überprüft. Grundlage ist eine aus \code{export-roles.csv} geladene Liste bekannter Rollenbegriffe.
\end{itemize}

\paragraph{Extraktion von Autorinnen und Autoren}
Die konkrete Anwendung dieser Patterns erfolgt in den jeweiligen Funktionen, deren Grundlage eine Verarbeitung der raw strings ist. Die Funktion \code{extract\_authors\_raw} analysiert hierfür zu Beginn alle Textzeilen innerhalb des Transkription, ohne dabei auf strukturierte XML-Tags angewiesen zu sein. Stattdessen orientiert sich die Funktion an den oben erläuterten typischen linguistischen Mustern. Somit kann die Funktion auch dann Ergebnisse liefern, wenn strukturierte XML-Tags wie \code{<author>} fehlen oder unvollständig sind, was als Backup-Lösung die LLM-Annotation komplettiert. Ziel ist es, möglichst frühzeitig Hinweise auf Autor:innen zu erkennen und diese mit einem einfachen Scoring-System zu bewerten. Das erkannte Scoring wird in einem separaten Feld \code{author\_score} festgehalten und dient als Bewertungsgrundlage für die spätere Auswahl der wahrscheinlichsten Autor:innen.

Da Autoren in der Regel am Briefende unterschreiben, werden die Zeilen des Transkripts von unten nach oben analysiert. Dabei wird zunächst geprüft, ob eine typische Rollensignatur wie \texttt{Der Vereinsführer} alleine am Dokumentende steht. Wird eine solche erkannt, wird unmittelbar eine entsprechende Rolle extrahiert, ohne dass ein Name erforderlich ist.

Findet sich keine Rollenzeile, durchsucht die Funktion den Text nach einer Schlussformel basierend auf den in \code{GREETING\_PATTERNS} definierten Mustern. Der Textabschnitt, der auf diese Grussformel folgt, wird im nächsten Schritt nach Signaturen durchsucht. Die Auswertung erfolgt dabei in mehreren aufeinanderfolgenden Prüfungen.

Zunächst wird versucht, eine Kombination aus Rolle und Nachname zu identifizieren\footnote{z.B. \texttt{Der Vereinsführer Zimmermann}}.
Alternativ wird geprüft, ob Rolle und Name in zwei aufeinanderfolgenden Zeilen vorkommen. In Fällen, in denen sowohl Rolle und Name in benachbarten Zeilen auftreten – etwa \texttt{"Der Vereinsführer"} gefolgt von \texttt{"Alfons Zimmermann"} – wird diese kontextuelle Nähe berücksichtigt. Die Funktion vergibt in solchen Fällen einen erhöhten \code{match\_score}, da die unmittelbare Abfolge als zuverlässiger Hinweis auf eine zusammengehörige Signatur gewertet wird. Technisch erfolgt die Prüfung auf $\pm1$ Zeile Abstand im Transkripttext.\\

Weitere Varianten umfassen typische Signaturformate wie \texttt{Max Ganter, Schriftführer} oder Namensinitialen wie \texttt{M. Ganter}.
Auch vollständige Namen (Vorname + Nachname) oder einzelne Namen (z.B. nur \texttt{Fritz}) werden erkannt und mit Platzhaltern übernommen. In Ausnahmefällen werden auch unvollständige Namensformen\footnote{etwa Initialen oder Einzelnamen} übernommen, sofern sie in typischen Signaturkontexten auftreten. Dabei wird der fehlende Vor- oder Nachname mit einem Platzhalter (\code{None}) belegt und der Eintrag entsprechend mit \code{needs\_review = true} markiert. Die Übernahme solcher Platzhalter erfolgt jedoch selektiv und nur bei klarer Signaturstruktur.

Wird kein valides Format erkannt, liefert die Funktion ein leeres Dictionary mit einem Eintrag für \code{closing}, der die gefundene Grussformel dokumentiert.

Neben der heuristischen Pattern-Suche durch \code{extract\_authors\_raw()} stützt sich die Pipeline auch auf strukturierte Annotationen aus dem XML. Die Funktion \code{extract\_author\_from\_custom\_tag()} sucht gezielt nach \code{<author>}-Tags und übernimmt diese, sofern sie vollständig sind. Im Anschluss wird jeder Kandidateneintrag durch \code{resolve\_llm\_custom\_authors\_recipients()} validiert und bewertet. Insbesondere dann, wenn er von einem LLM vorgeschlagen oder aus unstrukturiertem Text extrahiert wurde kommt die Bewertung zum tragen. Abschliessend übernimmt \code{letter\_match\_and\_enrich()} die Anreicherung durch bekannte Personen aus der Groundtruth-Liste. Hierbei werden unvollständige Einträge (z.B. nur ein Vorname) durch vollständigere bereits in \code{mentioned\_persons} aufgenommene Personen ersetzt, sofern möglich. Auch in Fällen, in denen keine strukturierte LLM-Annotation vorliegt, erfolgt eine nachgelagerte Prüfung gegen die Groundtruth. Die Funktion \code{resolve\_llm\_custom\_authors\_recipients()} gleicht unvollständige oder generische Einträge – etwa lediglich \texttt{"Otto"} oder \texttt{"Herrn"} – mit den zuvor erkannten \code{mentioned\_persons} ab. So können auch partielle oder schwach formatierte Namensangaben aufgelöst und angereichert werden.

Die Pipeline deckt jedoch nicht nur einzelne Autoren ab. \code{match\_authors~(~)} kombiniert die Roh-Extraktion der Autor:in (\code{extract\_authors\_raw}) mit der Anreicherung durch \code{letter\_match\_and\_enrich}. Ist das Ergebnis leer, prüft sie, ob zumindest eine Rolle ohne Namensangabe vorliegt. In diesem Sonderfall wird ein Eintrag mit leerem Namen, aber übernommener Rolle erstellt. Die Rolle wird normalisiert (\code{normalize\_and\_match\_role}) und in ein \code{role\_schema} überführt. Der Rückgabewert enthält \code{needs\_review = True} sowie eine niedrige \code{match\_score} (10), um auf die Unsicherheit der reinen Rollenangabe hinzuweisen.


\paragraph{Extraktion von Empfängerinnen und Empfängern}

Die Extraktion potenzieller Empfänger:innen erfolgt analog zur Autorenerkennung über eine Kombination aus regulären Ausdrücken, strukturierten XML-Tags und nachgelagerter Anreicherung. Im Gegensatz zur Autorenschaft, die typischerweise am Ende eines Dokuments steht, werden Empfänger:innen meist im Briefkopf genannt. Die Funktion \code{extract\_recipients\_raw()} konzentriert sich daher auf die ersten fünf Zeilen des Transkripts. Dieser Abschnitt enthält häufig Adressierungen wie \texttt{\enquote{An Herrn Otto Bollinger}} oder zeilenweise strukturierte Formate, die auf eine Empfängeradresse hindeuten. Die Funktion arbeitet unabhängig von expliziten XML-Tags wie \code{<recipient>}, wodurch sie auch bei unvollständig annotierten Dokumenten Ergebnisse liefern kann.

Die Erkennung erfolgt in drei heuristischen Stufen, abhängig von der Länge und Struktur der jeweiligen Adressierung. Einzeilige Formulierungen wie \texttt{\enquote{An Herrn Otto Bollinger}} werden als besonders zuverlässig bewertet (\code{recipient\_score = 90}, \code{confidence \= "header-inline"}). Dreizeilige Strukturen wie 
\\ \begin{center}
\texttt{An\\Herrn\\Otto Bollinger}
\end{center}
\justifying werden als semistrukturiert interpretiert und mit einem Score von 80 markiert (\code{confidence = "header-3line"}). Bei zweizeiligen Konstruktionen – etwa \texttt{Herrn} in einer Zeile und ein Name in der nächsten – wird aus methodischer Vorsicht ein niedrigerer Score (70) vergeben, da diese auch als Fliesstextnennung interpretiert werden könnten (\code{confidence = "header-2line"}). Diese Liste von Empfängerkandidaten wird anschliessend in mehreren Schritten angereichert und validiert.

In einem weiteren Schritt werden \code{<recipient>}-Tags aus dem XML durch \code{extract\_recipient\_from\_custom\_tag()} übernommen, sofern vorhanden. Wie bei den Authoren durchlaufen Rezipienten danach die Funktion \code{resolve\_llm\_custom\_authors\_recipients()}. So werden die Rohdaten zusammengeführt, und anschliessen eine Validierung vorgenommen. Insbesondere unvollständige oder generische Einträge wie \texttt{\enquote{Herrn}} oder häufig vorkommende Namen wie \texttt{\enquote{Otto}}\footnote{die Groundtruth kennt 11 verschiedene Personen namens Otto} werden entweder mit dem Vermerk \code{needs\_review = true} versehen oder durch bereits \code{mentioned\_persons} ersetzt. Diese Ersetzung erfolgt abschliessend in \code{letter\_match\_and\_enrich()}, wo wie bei den Autor:innen unvollständige Einträge – etwa nur der Vorname – durch vollständigere Einträge aus \code{mentioned\_persons} ersetzt werden, sofern diese zuvor im Text erkannt wurden. Mit \code{extract\_multiple\_recipients\_raw()} können darüber hinaus potenzielle Empfänger:innen aus dem Transkripttext anhand direkter und indirekter Anredeformen aus den jeweiligen Patternlisten extrahiert werden. Im Gegensatz zu \code{extract\_recipients\_raw()}, das sich auf die ersten Zeilen des Dokuments konzentriert, analysiert die Funktion \code{extract\_multiple\_recipients\_raw()} den gesamten Transkripttext. Sie erkennt direkte und indirekte Anredeformen auch im Fliesstext\footnote{etwa im Kontext von Zwischenempfängern oder weitergeleiteten Briefen} und erweitert damit die Empfängerliste über die Briefkopfregion hinaus. Direkte Formulierungen wie \texttt{Lieber Otto} werden mit \code{recipient\_score = 100} als besonders zuverlässig bewertet. Indirekte Anreden wie \texttt{zu Händen des Herrn Alfons Zimmermann} werden mit Score 70 erfasst. Die Ergebnisse enthalten Vorname, Nachname (falls vorhanden), Rolle (leer), Score und Confidence-Label. Durch die unterschiedlichen Ratings im Recipient-Scores können so Staffelungen dargestellt werden.


\paragraph{Extraktion des Absendeort und Empfangsort}

Die Extraktion des Absendeorts (\code{creation\_place}) und des Empfangsorts (\code{recipient\_place}) erfolgt in einem mehrstufigen Verfahren, das strukturierte XML-Informationen, heuristische Pattern-Erkennung und ein darauf aufbauendes Groundtruth-Matching kombiniert. Ziel ist es, auch in unvollständig annotierten oder uneinheitlich formulierten Briefen möglichst präzise Ortsinformationen zu identifizieren und anzureichern. Die Verarbeitung ist dabei modular aufgebaut und erfolgt durch vier aufeinander abgestimmte Funktionen.

Die Funktion \code{extract\_places\_and\_date()} bildet den Einstiegspunkt für die Ortserkennung. Sie durchsucht alle Zeilen im XML-Dokument (\code{<TextLine>}) nach Attributen vom Typ \texttt{custom}, in denen strukturierte Angaben zu \code{creation\_place}, \code{recipient\_place} und \code{creation\_date} hinterlegt sein können. Wird ein solches Attribut gefunden, wird der zugehörige Ort mittels RegEx aus dem Text extrahiert. Grundlage hierfür sind die LLM-ergänzten, im Transkribus-Export enthaltenen Offset- und Length-Parameter, die den Ortstext innerhalb der Zeile markieren. Die Funktion liefert drei \code{raw-strings} zurück: den vermuteten Absendeort (\code{raw\_creation\_place}), den Empfangsort (\code{raw\_recipient\_place}) sowie das Datum der Entstehung (\code{creation\_date})\footnote{dazu mehr im nächsten Abschnitt}. Sind keine Custom-Tags vorhanden, aktiviert die Funktion ein Fallback-Verfahren, das den Briefkopf anhand typischer Formate mit regulären Ausdrücken analysiert. Erkannt werden insbesondere Muster wie \texttt{Ort, den dd.mm.yyyy} oder \texttt{An ... in Ort}.

Die so ermittelten Raw-Strings werden in \code{assign\_sender\_and\_recipient\_place(...)} weiterverarbeitet. Diese Hauptfunktion ruft zunächst \code{extract\_places\_and\_date()} auf, um die Rohdaten zu extrahieren, und analysiert dann alle Zeilen des Transkripts, um auch Kontexte für zusammengesetzte Ortsbezeichnungen\footnote{z.B. Laufenburg-Rhina} zu erfassen. Anschliessend wird das Orts-Matching über die Hilfsfunktion \code{enrich\_place\_candidate()} durchgeführt. Diese Funktion gleicht den erkannten Komposit-Ortsnamen mit einer Groundtruth-Liste ab, in der alle bekannten Orte und ihre Varianten hinterlegt sind. Neben exakten Treffern erlaubt das Matching auch Fuzzy Matching über die Levenshtein-Distanz. Ergibt sich dabei ein Treffer, wird ein Score berechnet und mit weiteren Metadaten \footnote{z.B. \code{nodegoat\_id}} angereichert. Im Kontext von Empfängeradressen wird zusätzlich geprüft, ob im unmittelbaren Umfeld ein Vereins- oder Organisationsname vorkommt. In diesem Fall wird ein Score-Malus vergeben, da  das ein Hinweis auf eine mögliche Verwechslung zwischen Ort und Organisation sein kann.

Eine besondere Stärke der Pipeline liegt in der Erkennung zusammengesetzter Ortsnamen. Die Funktion \code{find\_combined\_place()} durchsucht benachbarte Zeilen nach Kombinationen wie \texttt{Laufenburg} und \texttt{Rhina}, die zusammen in einem Eintrag der Groundtruth-Liste vorkommen (\texttt{Laufenburg-Rhina}). Wird eine solche Kombination erkannt, wird sie bevorzugt, erhält einen Bonus im Matching-Score und ersetzt die Einzeltreffer. Weil so auch Teilorte oder Stadtteile abgebildet werden können, wird die Ortserkennung deutlich präziser.

Abschliessend wird durch \code{finalize\_recipient\_places()} der am besten bewertete Empfängerort ausgewählt. Die Funktion sortiert alle erkannten Ortskandidaten nach Score und übernimmt den besten Eintrag als \code{recipient\_place}. Weitere valide, aber weniger eindeutige Orte werden ebenfalls gespeichert, jedoch mit dem Attribut \code{needs\_review = true} gekennzeichnet, um eine manuelle Prüfung zu ermöglichen.

Die finale Funktion \code{match\_place()} kann mehrere potenzielle Ortstreffer zurückgeben, insbesondere wenn es konkurrierende Varianten mit ähnlichem Namen gibt. In diesem Fall wählt die Funktion \code{finalize\_recipient\_places()} den Ort mit dem höchsten Score als \code{recipient\_place} aus, während weitere plausible, aber unsicherere Treffer mit dem Vermerk \code{needs\_review = true} gespeichert werden. Durch das modulare Zusammenspiel der vier Funktionen können auch in schwierig strukturierten oder unvollständig annotierten Dokumenten präzise Ortsangaben extrahiert werden.

\paragraph{Extraktion des Erstellungsdatum}

Die Erkennung von Entstehungsdaten (\code{creation\_date}) erfolgt im Modul \code{letter\_metadata\_matcher.py} primär über die von oben bereits bekannte Funktion \code{extract\_places\_and\_date( )}. Im Zentrum steht die Auswertung strukturierter Custom-Tags im PAGE-XML sowie ein regulärer Fallback-Mechanismus zur heuristischen Analyse des Briefkopfs.

Im ersten Schritt durchsucht die Funktion alle \code{<TextLine>}-Elemente nach dem Attribut \code{custom}, das von Transkribus für semantische Annotationen verwendet wird. Wird ein Eintrag vom Typ \texttt{creation\_date} oder \texttt{date} erkannt, wird mittels RegEx nach einem \texttt{when="YYYY-MM-DD"}-Feld innerhalb des custom-Attributs gesucht. Dieser Wert wird direkt übernommen und als \code{creation\_date} gespeichert. Da bei dem manuellen Tagging besonders auf die korrekten Daten geachtet, und diese händisch mit \code{when\=} immer ergänzt wurden, sind diese Angaben in jedem Fall vorhanden.

Wird jedoch kein Custom-Tag erkannt, greift ein Fallback-Verfahren: Die Funktion analysiert hierfür die ersten fünf Zeilen des Dokuments – typischerweise der Briefkopf – auf typische Orts-Datumskombinationen. Dazu zählt insbesondere das Muster

\begin{center}
\texttt{Ort, den dd.mm.yyyy}
\end{center}

\noindent Die RegEx erfasst dabei sowohl den Ort (als möglichen Absendeort) als auch das Datum. Sie sind daher auch gemeinsam in dieser Funktion aufgenommen. Wird ein solches Datum gefunden, wird es als \code{creation\_date} übernommen, selbst wenn kein zugehöriger Ort erkannt werden konnte. Die Erkennung erfolgt dabei tolerant gegenüber dem optionalem Wort \enquote{den} und erlaubt sowohl zwei- als auch vierstellige Jahresangaben\footnote{erkannt werden also sowohl beispielsweise 1939, als auch nur 39}. Ist kein solcher Eintrag vorhanden, bleibt \code{creation\_date = None}.

In Kombination mit \code{assign\_sender\_and\_recipient\_place()} wird das erkannte Datum anschliessend gemeinsam mit den Ortseinträgen an die Hauptverarbeitung zurückgegeben und als\code{ attributes["creation\_date"]} in den Metadaten des Dokuments gespeichert. Im Gegensatz zu \code{mentioned\_dates}\footnote{mehr in \nameref{subsection:date_matcher}}, die beliebige Datumsangaben im Fliesstext zählen, bezieht sich \code{creation\_date} ausschliesslich auf das Erstellungsdatum des Dokuments


\subsubsection{type\_matcher.py}\label{subsection:type_matcher}
Dieses Modul besteht aus nur einer einzigen Funktion: \code{def get\_document\_type~(~)}. Diese zerlegt den Namen der zu verarbeitenden Datei und extrahiert daraus die siebenstellige Transkribus-ID und die Seitennummer. Es folgt ein Vergleich in der Akten\_Gesamtübersich.csv, die als Groundtruth alle Dateien im Korpus listet.

Da während der Transkription in dieser Tabelle auch die Dokumententypen festgehalten wurden, kann über einen einfachen Lookup für jede Seite der zugehörige Typ extrahiert werden. Mögliche Kategorien sind unter anderem \enquote{Brief}, \enquote{Postkarte}, \enquote{Protokoll}.\footnote{siehe Kapitel \nameref{Dokumententypen}} 

Falls kein passender Eintrag in der Groundtruth gefunden wird, bietet die Funktion ein optionales Fallback: Wird zusätzlich ein \code{xml\_path} übergeben, so wird versucht, den Dokumenttyp direkt aus einem \code{<Dokumententyp>}-Tag innerhalb der zugehörigen XML-Datei auszulesen. Auf diese Weise wird sichergestellt, dass jede Seite im JSON-Output einen Typ zugewiesen erhält – entweder verlässlich aus der CSV oder über das XML-Fallback.

Die extrahierte Typinformation wird im Attributblock des finalen JSON unter \code{attributes["document\_type"]} gespeichert. So kann später seitengenau nach Dokumententypen gefiltert und eine differenzierte Verteilung der Typen innerhalb einer Akte nachvollzogen werden.


\subsubsection*{event\_matcher.py}\label{subsection:event_matcher}

Das Modul \texttt{event\_matcher.py} dient der Erkennung, Strukturierung und Anreicherung von 
Ereignistags aus den Transkribus-Dokumenten. Die zugrundeliegende Funktion \texttt{extract\_events\_from\_xml()} 
verarbeitet die gesamte XML-Datei zeilenweise und extrahiert jene Textstellen, die als Ereignis markiert 
wurden oder im unmittelbaren Zusammenhang mit einem Ereignisblock stehen. Grundlage dafür ist die Auswertung 
des \texttt{custom}-Attributs einzelner \texttt{<TextLine>}-Elemente, insbesondere wenn dieses explizit mit 
\enquote{event} gekennzeichnet ist.

Die Funktion arbeitet blockbasiert: Ereignisblöcke bestehen in der Regel aus mehreren aufeinanderfolgenden 
Zeilen, die durch inhaltliche Merkmale wie Bindestriche, kleingeschriebene Fortsetzungszeilen oder fehlende 
Datumsmarkierungen zusammenhängend interpretiert werden. Die Funktion \texttt{is\_continuation()} prüft dabei, 
ob eine Zeile an die vorhergehende angeschlossen werden kann, etwa durch typische Anschlusswörter oder formale Merkmale. 
Sobald ein abgeschlossener Block erkannt ist, wird dieser mit der Hilfsfunktion \texttt{build\_event()} zu einem 
vollständigen Ereignisobjekt zusammengeführt.

In \texttt{build\_event()} wird der aus mehreren Zeilen bestehende Textblock zunächst als Fliesstext zusammengesetzt 
und inhaltlich analysiert. Es werden mehrere Entitäten erkannt und dem Ereignis zugeordnet:

\begin{itemize}
  \item \textbf{Orte:} Über die Funktion \texttt{match\_place()} aus dem Modul \texttt{place\_matcher.py} werden potenzielle Ortsnamen im Text identifiziert und mit der Groundtruth abgeglichen. Dabei wird jeder erkannte Ort zusätzlich durch eine Plausibilitätsprüfung überprüft, bevor er als \texttt{Place}-Objekt übernommen wird.
  \item \textbf{Daten:} Die Funktion \texttt{extract\_custom\_date()} durchsucht die XML-Zeile nach Datumsangaben in den XML-Tags. Wenn kein strukturiertes Datum vorhanden ist, aber einfache numerische Formate wie \enquote{15.03} im Fliesstext erkannt werden, werden diese als Datum übernommen.
  \item \textbf{Organisationen:} Über \texttt{match\_organization\_from\_text()} wird der Textblock mit bekannten Organisationseinträgen abgeglichen. Bei Übereinstimmung werden entsprechende Organisationen als strukturierte Objekte ergänzt.
  \item \textbf{Personen:} Mögliche Namen werden durch reguläre Ausdrücke identifiziert und mit Hilfe der Funktion \texttt{extract\_name\_with\_spacy()} in Vor- und Nachnamen getrennt. Anschliessend erfolgt ein Abgleich mit der Personen-Groundtruth über \texttt{match\_person()}. Positive Treffer werden inklusive Match-Score und Herkunftskennzeichnung als \texttt{Person}-Objekte dem Ereignis zugeordnet.
\end{itemize}

Die fertigen Ereignisse bestehen jeweils aus einer Kurzbeschreibung (in der Regel der ersten Zeile), 
einer ausführlichen Beschreibung (bestehend aus allen zugehörigen Textzeilen), einem Datumsfeld, einer 
Ortsangabe und einer strukturierten Liste aller beteiligten Orte, Organisationen und Personen. Der vollständige 
Satz aller so erkannten Ereignisse wird am Ende als Liste von \texttt{Event}-Objekten zurückgegeben.

Das Modul arbeitet vollständig dateibasiert und benötigt als einzige Eingabe den Pfad zur Transkribus-XML-Datei 
sowie eine initialisierte Instanz des \texttt{PlaceMatcher}. Es greift auf zentrale Komponenten der Projektarchitektur 
zurück, darunter die Groundtruth-Listen für Orte, Personen und Organisationen. Die extrahierten Ereignisse werden 
im finalen JSON unter dem Attribut \texttt{events} gespeichert. Da ein Event ein eher abstraktes Konstrukt ist, liegt der
Fokus der Pipeline weniger auf diesem Modul, das zu einem späteren Zeitpunkt beispielsweise durch präziseres Prompten für das
Eventtagging optimiert werden soll. 

\subsubsection*{date\_matcher.py}\label{subsection:date_matcher}
Das Modul \texttt{date\_matcher.py} dient der systematischen Extraktion, Normalisierung und 
Zählung von Datumsangaben in den Transkribus-Dokumenten. Es basiert auf der 
Auswertung strukturierter Angaben, die im Rahmen der Transkription über XML-Custom-Tags im 
\texttt{custom}-Attribut einzelner \texttt{<TextLine>}-Elemente eingebettet wurden. Diese 
Daten gelten innerhalb des Korpus als zuverlässig, da sie während der manuellen Korrekturprozesse
in Transkribus einheitlich normiert und im Format \texttt{dd.mm.yyyy} ergänzt wurden.


Treten im historischen Text verkürzte Datumsangaben wie \textit{\enquote{1.~d.~Mts}} auf, so handelt
es sich um Abkürzungen, die bei der Transkription mit einem entsprechenden \texttt{abbreviation}-Tag
markiert werden. Die Funktionsweise dieser Markierungen sowie die heuristische Auflösung solcher 
verkürzter Angaben wird im Kapitel \ref{subsec:TaggingKapitel} erläutert. Lässt sich aus dem weiteren 
Kontext\footnote{Zum Beispiel durch Hinweise im Seiteninhalt oder durch übergeordnete Informationen im Umfeld der Akte} 
ein vollständiges Datum erschliessen, kann dieses anschliessend in strukturierter Form übernommen und im JSON als 
normiertes Datum gespeichert werden.

Innerhalb der Verarbeitungspipeline wird das Modul über die Funktionen \texttt{extract\_custom\_date()} und 
\texttt{combine\_dates()} aufgerufen. Zunächst durchläuft \texttt{extract\_custom\_date()} das XML-Dokument 
und extrahiert alle \texttt{custom}-Attribute, die ein \texttt{date\{…\}}-Muster enthalten. Die Inhalte dieser 
Attribute werden bereinigt und zur weiteren Analyse an die Funktion \texttt{extract\_date\_from\_custom()} übergeben.\\
Diese Funktion überprüft mithilfe RegEx, ob der String tatsächlich eine gültige Datumsangabe enthält. 
Dabei wird insbesondere nach einem \texttt{when}-Feld gesucht, das im Inneren des \texttt{date}-Blocks enthalten ist. 
Die in diesem Feld hinterlegten Daten werden anschliessend mit der Funktion \texttt{parse\_custom\_attributes()} als 
Key-Value-Paare interpretiert. Liegt ein gültiges Datum vor, wird dessen Format mit \texttt{normalize\_to\_ddmmyyyy()} 
überprüft und gegebenenfalls vereinheitlicht.
\\
Unterstützt werden mehrere Eingabeformate, darunter standardisierte Formen wie \texttt{dd.mm.yyyy}, ISO-Formate 
wie \texttt{yyyy-mm-dd} oder zweistellige Jahresangaben, die automatisch in vierstellige Jahre des 20.
Jahrhunderts umgewandelt werden. Zusätzlich erkennt die Funktion auch Intervallangaben wie \texttt{01/03.04.1944}, 
bei denen ein Datumsbereich über einen Schrägstrich kodiert ist. Solche Intervalle werden in strukturierter Form mit 
einem \texttt{from}- und \texttt{to}-Wert als \texttt{date\_range} gespeichert.

Die Funktion \texttt{combine\_dates()} führt schliesslich alle erkannten Einzel- und Intervallangaben zusammen, 
zählt deren Häufigkeit im Dokument und erstellt eine deduplizierte, sortierte Liste für den späteren Export. Dabei 
wird jede identifizierte Angabe – ob Einzeldatum oder Zeitspanne – um eine Zählung der Nennungen ergänzt. Bei 
Intervallen wird zusätzlich der Originalstring dokumentiert, aus dem die Angabe hervorging.

Das Ergebnis der Verarbeitung wird im Feld \texttt{mentioned\_dates} gespeichert. Jeder Eintrag enthält entweder 
ein einzelnes Datum oder einen Datumsbereich, ergänzt um die Häufigkeit und gegebenenfalls den ursprünglichen Wortlaut 
aus dem \texttt{custom}-Attribut.

Das Modul arbeitet unabhängig von externen Ressourcen und benötigt lediglich das XML-Baumobjekt des jeweiligen 
Dokuments. Die so gewonnenen Zeitangaben bilden die Grundlage für die chronologische Einordnung, 
Kontextualisierung und Auswertung der digitalen Quellenbasis.

\begin{minipage}[t]{0.52\textwidth}
  \setstretch{1.5}
  \justifying%

  \subsubsection{unmatched\_logger.py}\label{subsec:unmatched_logger}

Das Modul \texttt{unmatched\_logger.py} dient der systematischen Protokollierung von Entitäten, 
die in der aktuellen Version der Groundtruth noch nicht enthalten sind. 
Diese Protokolle bilden die Grundlage für weiterführende Recherchen, 
durch die die Groundtruth schrittweise ergänzt und verbessert werden kann.

\noindent Innerhalb der Verarbeitungs-Pipeline wird das Modul \texttt{unmatched\_logger.py} über die Funktion \texttt{process\_single\_xml()} im Hauptprogramm aufgerufen. 

\noindent Bereits in der Testphase kam das Modul mehrfach zum Einsatz, um die Erkennung und Zuordnung bislang nicht erfasster Entitäten zu überprüfen.
\end{minipage}%
\hfill
\begin{minipage}[t]{0.4\textwidth}
  \vspace*{0.7cm} % ► schiebt alles um 0.7cm nach unten
\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}


% TikZ styles
\tikzset{
  module/.style={rectangle, draw=black, fill=blue!10, minimum width=0.4cm, minimum height=0.2cm},
  source/.style={rectangle, draw=black, fill=green!20, minimum width=0.4cm, minimum height=0.2cm},
  highlight/.style={rectangle, draw=black, fill=blue!40, minimum width=0.4cm, minimum height=0.2cm},
  arrow/.style={-stealth, line width=0.15mm},
  arrowboth/.style={stealth-stealth, line width=0.15mm},
  process/.style={rectangle, draw=black, fill=orange!20, thick, minimum width=6cm, minimum height=1cm, align=center},
  large/.style={rectangle, draw=black, fill=blue!10, thick, minimum width=4.5cm, minimum height=1.2cm, align=center}
};
%==========================
% Mini-Diagramm oben rechts
%==========================

\begin{scope}[shift={(9,0.45)}, node distance=0.3cm]

\node[module] (init) at (0,0) {};
\node[module] (xml) at (0, -0.5) {};
\node[module]    (events)  at (-2.0, -1.3) {};
\node[module]    (roles)   at (-1.2, -1.3) {};
\node[module] (persons) at (-0.4, -1.3) {};
\node[module]    (orgs)    at ( 0.4, -1.3) {};
\node[module]    (places)  at ( 1.2, -1.3) {};
\node[module]    (dates)   at ( 2.0, -1.3) {};
\node[module] (authors)    at (-0.4, -2  ) {};

\node[coordinate] (joinpoint) at ($(xml.south) + (0, -1.8cm)$) {};
\node[coordinate] (joinpointxml) at ($(xml.south) + (0, -0.3cm)$) {};

\node[highlight, below=of joinpoint] (unmatched) {};
\node[module, below=of unmatched] (json) {};
\node[source, below=of json] (save) {};
\node[source, right=0.5cm of save] (unmatchedjson) {};
\draw[arrow, dashed] (persons.south) -- (authors.north);
\draw[arrow] (unmatched.east) -| (unmatchedjson.north);

\draw[arrow] (json.south) -- (save.north);
\draw[arrow] (init.south) -- (xml.north);
\draw[arrowboth] (roles.east) -- (persons.west);
\draw[arrowboth] (persons.east) -- (orgs.west);
\draw[arrowboth] (orgs.east) -- (places.west);
%Pfeile oben
\draw[arrow] (xml.south) -- (joinpointxml.south);
\draw[arrow] (joinpointxml) -- ($(roles.north |- joinpointxml)$) -- (roles.north);
\draw[arrow] (joinpointxml) -- ($(persons.north |- joinpointxml)$) -- (persons.north);
\draw[arrow] (joinpointxml) -- ($(orgs.north |- joinpointxml)$) -- (orgs.north);
\draw[arrow] (joinpointxml) -- ($(places.north |- joinpointxml)$) -- (places.north);
\draw[arrow] (joinpointxml) -- ($(dates.north |- joinpointxml)$) -- (dates.north);
\draw[arrow] (joinpointxml) -- ($(events.north |- joinpointxml)$) -- (events.north);

%Pfeile unten; Doppelte Pfeile mit definiertem Stil
\draw[arrow] (events.south) -- ($(events.south |- joinpoint)$);
\draw[arrow] ($(events.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (roles.south) -- ($(roles.south |- joinpoint)$);
\draw[arrow] ($(roles.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (authors.south) -- ($(authors.south |- joinpoint)$);
\draw[arrow] ($(authors.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (orgs.south) -- ($(orgs.south |- joinpoint)$);
\draw[arrow] ($(orgs.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (places.south) -- ($(places.south |- joinpoint)$);
\draw[arrow] ($(places.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (dates.south) -- ($(dates.south |- joinpoint)$);
\draw[arrow] ($(dates.south |- joinpoint)$) -- (joinpoint);

\draw[arrow] (joinpoint.south) -- (unmatched.north);
\draw[arrow] (unmatched.south) -- (json.north);
\draw[arrow] (json.south) -- (save.north);
\end{scope}

% Grosses Prozessdiagramm
\begin{scope}
% --- Kopf-Node ---
\node[large] (logger) at (1.5,0) {\textbf{log\_unmatched\_entities} \\ (Starte Logging aller Entitätstypen)};

% --- Schleifen ---
\node[process, below=1.2cm of logger] (persons)  {\textbf{Personen} \\ (Nicht in mentioned\_persons? → log)};
\node[process, below=of persons] (places)  {\textbf{Orte} \\ (Nicht in mentioned\_places? → log)};
\node[process, below=of places] (roles)  {\textbf{Rollen} \\ (Nicht in Personenrollen? → log)};
\node[process, below=of roles] (orgs)  {\textbf{Organisationen} \\ (Nicht als associated\_organisation? → log)};
\node[process, below=of orgs] (events)  {\textbf{Events} \\ (Nicht in mentioned\_events? → log)};
\node[process, below=of events] (done)  {\textbf{Speichern} \\ (save\_json, print)};

% --- Pfeile ---
\draw[arrow] (logger.south) -- (persons.north);
\draw[arrow] (persons.south) -- (places.north);
\draw[arrow] (places.south) -- (roles.north);
\draw[arrow] (roles.south) -- (orgs.north);
\draw[arrow] (orgs.south) -- (events.north);
\draw[arrow] (events.south) -- (done.north);


\node[draw=gray, dashed, rounded corners, inner sep=0.5cm, fit=(persons)(orgs)(done)] (group) {};
\end{scope}
\end{tikzpicture}} % ► Die Caption direkt darunter, blockweise formatiert
  \captionof{figure}{\\\small Oben links: \small Prozessdiagramm für \texttt{unmatched\_logger.py},\\{\small Oben rechts: Pipelineübersicht}}
  \label{fig:unmatched_logger.py}
\end{minipage}


Im Kern stellt das Modul die Funktion \texttt{log\_unmatched\_entities} bereit. 
Diese übernimmt die von den zuvor beschriebenen Matcher-Funktionen ermittelten Entitäten 
und prüft, ob sie in den entsprechenden Groundtruth-CSV-Dateien vorhanden sind. 

Die Suche erfolgt iterativ innerhalb der Listenstrukturen für Personen, Orte, Rollen, Organisationen und Ereignisse. 
Wird eine Entität über ein XML-Custom-Tag einer dieser Kategorien zugewiesen, ohne dass sie in der Groundtruth verzeichnet ist, 
wird sie in einer spezifischen JSON-Datei protokolliert.

Die folgenden Dateien werden dabei erzeugt:
\begin{itemize}
  \item unmatched\_persons.json
  \item unmatched\_places.json
  \item unmatched\_roles.json
  \item unmatched\_events.json
  \item unmatched\_organisations.json
\end{itemize}

Zusätzlich werden alle Einträge in einer zusammengeführten Datei \texttt{unmatched.json} gespeichert, 
um einen vollständigen Überblick nicht zugeordneter Entitäten zu gewährleisten.


Alle Ergebnisse werden zudem in einer Datei \code{unmatched.json} gespeichert, um einen Gesamtüberblick zu erhalten.

\subsubsection{validation\_module.py}\label{subsec:validation_module}

Das Modul \code{validation\_module.py} übernimmt die strukturierte Qualitätskontrolle der erzeugten JSON-Dokumente in der finalen Verarbeitungsphase. Es wird in der zentralen Verarbeitungsfunktion \code{process\_single\_xml()} unmittelbar vor dem Speichern der Ausgabedateien aufgerufen:

\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=LightGray,
fontsize=\footnotesize,
linenos,
breaklines=true,
breakanywhere=true
]{python}
validation_result = validate_extended(doc)
    if validation_result:
        print(f"[WARN] Validierungsfehler im Dokument {full_doc_id}:")
        for err_type, errors in validation_result.items():
            print(f"  - {err_type}: {', '.join(errors)}")

\end{minted}

Die Validierung erfolgt anhand des erzeugten \code{BaseDocument}-Objekts und basiert auf festen Regeln zur Überprüfung von Pflichtfeldern, Formatvorgaben sowie typischen Erkennungsfehlern bei Personen- oder Ortsangaben.

Die Hauptfunktion \code{validate\_extended(doc)} analysiert das Dokument auf Inkonsistenzen und gibt ein Dictionary mit Fehlermeldungen pro Feld zurück. Neben formalen Anforderungen (z.\,B. korrektes Datumsformat) werden auch häufige Named-Entity-Fehlklassifikationen erkannt, etwa wenn Titelwörter als Vorname interpretiert werden.

Beispielhafte Prüfungen:

\begin{itemize}
    \item \textbf{Datumsformat}: Ein Eintrag wie \texttt{1941.5.28} wird zurückgewiesen, da das erwartete Format \texttt{YYYY.MM.DD} lautet.\\
    \texttt{[creation\_date] Fehlend oder ungültiges Format (YYYY.MM.DD)}

    \item \textbf{Leere Empfängerliste}: Bei Dokumenten vom Typ \texttt{Brief} oder \texttt{Postkarte} wird überprüft, ob mindestens ein valider Empfänger mit Name oder Rolle vorhanden ist. Ist dies nicht der Fall, erfolgt eine Fehlermeldung.\\
    \texttt{[recipients] Empfänger fehlt oder ist ungültig}

    \item \textbf{Fehlerhafte Personennamen}: Wird ein Vorname wie \texttt{des} oder \texttt{Herrn} erkannt, erfolgt eine heuristische Warnung auf möglichen OCR- oder Parsingfehler.\\
    \texttt{[mentioned\_persons[0]] Möglicher Fehlname: des}
\end{itemize}

Die Hinweise des Validierungsmoduls werden insbesondere während der Erprobungs- und Optimierungsphasen intensiv genutzt, etwa zur Feinjustierung der Methoden der Named-Entity-Recognition bei Personen. Sie dienen nicht nur der strukturellen Sicherung eines gültigen JSON-Outputs, sondern ermöglichen auch gezielte Nachkorrekturen im Rahmen der qualitativen Auswertung und visuellen Darstellung der Daten.



\subsection{KEINE AHNUNG WAS DIE HIER MACHEN}

\subsubsection{llm\_enricher.py}\label{subsection:llm_enricher}

Das Modul \code{llm\_enricher.py} dient der nachträglichen semantischen Anreicherung von JSON-Dokumenten mithilfe von GPT-4. Es wird gegen Ende des Verarbeitungsprozesses innerhalb von \code{transkribus\_to\_base.py} aufgerufen, nachdem die initiale Konvertierung der Transkribus-XML-Dateien abgeschlossen und das Entitätenmatching bereits erfolgt ist. Das Ziel besteht darin, fehlende Felder wie \code{author}, \code{recipient}, \code{creation\_date} oder \code{content\_tags\_in\_german} auf abzufangen und ggf. zu ergänzen.

Die Funktion \code{enrich\_document\_with\_llm} übergibt das vollständige JSON-Dokument an das Sprachmodell. Dieses erhält einen Prompt mit präzisen Anweisungen zur Vervollständigung strukturierter Felder, inklusive Regeln z.B. Schutz von IDs wie der \code{nodegoat\_id}, Kombination von Ortsnamen, Erkennung von Vereinsnamen als Organisationen). Der Rückgabewert wird mit dem Originaldokument zusammengeführt und unter \code{\_enriched.json} gespeichert. Zusätzlich wird die LLM-Nutzung dokumentiert (\code{input\_tokens}, \code{output\_tokens}, \code{cost\_usd}).

\paragraph{Abgrenzung zu \code{llm\_preprocessing.py}}%
Trotz des gemeinsamen Einsatzes eines Sprachmodells unterscheiden sich die beiden Module grundlegend in Funktion, Datenbasis und Zielsetzung. Während \code{llm\_preprocessing.py} im Frühstadium der Verarbeitung eingesetzt wird, um Textabschnitte vorzubereiten, Prompts zu formulieren oder Entitäten lokal zu erkennen, operiert \code{llm\_enricher.py} am Ende des Workflows auf dokumentweiter Ebene des neu generierten JSON. Die folgende Tabelle fasst die Unterschiede zusammen:

\begin{center}
\begin{tabular}{p{1.5cm} p{6.2cm} p{6.2cm}}

\toprule
~ & \textbf{\code{llm\_preprocessing.py}} & \textbf{\code{llm\_enricher.py}} \\
\midrule
\textbf{Ziel} & 
Vorbereitung des LLM-Einsatzes: Segmentierung, Prompt-Generierung und lokale Entitätenerkennung &
Nachträgliche Anreicherung strukturierter JSONs mit fehlenden Metadaten auf Dokumentebene \\
\midrule
\textbf{Fokus} & 
Lokale NER in isolierten Transkriptzeilen oder Abschnitten &
Globale Dokumentinterpretation, Rollen- und Feldzuordnung, Dublettenfilterung \\
\midrule
\textbf{Input} & 
Textsegmente oder XML-Zeilen aus Transkribus &
Strukturiertes JSON-Dokument nach algorithmischer Vorverarbeitung \\
\midrule
\textbf{Output} & 
Markierte Entitäten in Form von XML-Tags oder Listen (z.B. Personen, Orte) &
Vervollständigtes JSON mit algorithmisch unausgefüllten Feldern\\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Notwendigkeit beider Module trotz LLM-Einsatz}%
Auch wenn beide Module aktuell das selbe Sprachmodell verwenden, besteht keine redundante Dopplung. Sie erfüllen komplementäre Aufgaben in einer zweistufigen Pipeline: Zuerst erfolgt mit \code{llm\_preprocessing.py} die Erkennung und Markierung von Entitäten in isolierten Textblöcken zur Named-Entity-Recognition. Anschliessend übernimmt \code{llm\_enricher.py} die Integration, Interpretation und strukturelle Vervollständigung dieser Einheiten im Gesamtkontext des Dokuments. Es handelt sich somit um ein hybrides Verfahren nach dem Prinzip: \textit{Erkennen → Zuordnen → Strukturieren}.

Das Modul \code{llm\_enricher.py} wird in \code{transkribus\_to\_base.py} optional aufgerufen, nachdem die regulären Verarbeitungsschritte abgeschlossen sind. Nur wenn bestimmte Kernfelder im JSON-Dokument fehlen, wird das Enrichment aktiviert. Dies verhindert unnötige LLM-Aufrufe und erhöht die Effizienz. Die Rückgabe wird mit dem Originaldokument vereinigt, bestehende IDs und Daten werden geschützt, neue Felder ergänzt. Damit wird eine nachträgliche Qualitätssteigerung des Outputs erzielt – bei vollständiger Nachvollziehbarkeit.

\subsubsection{enrich\_pipeline.py}


%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%---------------    Analyse & Diskussion der Ergebnisse   ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––% 
\section{Analyse \& Diskussion der Ergebnisse}
\subsection{Visualisierung auf der VM}


%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––           Fazit        ––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––% 
\section{Fazit und Ausblick}
\subsection{Zusammenfassung der zentralen Erkenntnisse}
\subsection{Methodische Herausforderungen und Lösungen}
\subsection{Ausblick auf zukünftige Forschung und mögliche Erweiterungen der Datenbank}
\newpage

%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––           Bibliographie         –––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
\pagecolor{white}
\newpage
\begingroup
\small
\singlespacing%

% Heading-Stil für jede Teilbibliografie
\defbibheading{subbib}[\refname]{\subsubsection*{#1}}

% Titel für das Gesamtverzeichnis (optional)
\section*{Bibliographie}
\addcontentsline{toc}{section}{Bibliographie}

% Nach Typ sortierte Blöcke
\subsection*{Artikel}
\addcontentsline{toc}{subsection}{Artikel}
\printbibliography[type=article, heading=none]

%\subsection*{Beiträge in Sammelbänden}
%\addcontentsline{toc}{subsection}{Beiträge in Sammelbänden}
%\printbibliography[type=incollection, heading=none]

\subsection*{Monografien}
\addcontentsline{toc}{subsection}{Monografien}
\printbibliography[type=book, heading=none]

\subsection*{Onlinequellen}
\addcontentsline{toc}{subsection}{Onlinequellen}
\printbibliography[type=online, heading=none]

\subsection*{Software}
\addcontentsline{toc}{subsection}{Software}
\printbibliography[type=misc, heading=none]

\subsection*{Vorträge und Manuskripte}
\addcontentsline{toc}{subsection}{Vorträge und Manuskripte}
\printbibliography[type=unpublished, heading=none]



\endgroup

%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––                                        ––––––––––––––––––––––% 
%––––––––––––––––––––––––           Anhang                       ––––––––––––––––––––––%
%––––––––––––––––––––––––                                        ––––––––––––––––––––––% 
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––%
%––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––% 

\newpage
\appendix
\section{Anhang}
\begingroup
\small
\subsection{PDF\_to\_JPEG.py}\label{section:PDF_to_JPEG}
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=LightGray,
fontsize=\footnotesize,
linenos,
breaklines=true,  %Automatischer Zeilenumbruch
breakanywhere=true % Minted darf überall umbrechen (optional)
]{python}
import os
import fitz  # PyMuPDF

def convert_pdf_to_jpg(src_folder, dest_folder):
    # Überprüfen, ob der Zielordner existiert, und ihn ggf. erstellen
    if not os.path.exists(dest_folder):
        os.makedirs(dest_folder)

    # Durchgehen durch alle Dateien im Quellordner
    for root, dirs, files in os.walk(src_folder):
        for file in files:
            # Überprüfen, ob die Datei eine PDF-Datei ist
            if file.lower().endswith(".pdf"):
                # Vollständigen Pfad zur PDF-Datei erstellen
                pdf_path = os.path.join(root, file)
                # PDF-Datei öffnen
                doc = fitz.open(pdf_path)
                # Durch alle Seiten der PDF-Datei gehen
                for page_num in range(len(doc)):
                    page = doc[page_num]
                    # Seite in ein PixMap-Objekt umwandeln (für die Konvertierung in JPG)
                    pix = page.get_pixmap()
                    # Dateinamen ohne Dateiendung extrahieren
                    filename_without_extension = os.path.splitext(file)[0]
                    # Ausgabedateinamen erstellen mit führenden Nullen für die 
                    # Seitennummer
                    output_filename = f"{filename_without_extension}_S{page_num + 1:03d}.jpg"


                    # Vollständigen Pfad zur Ausgabedatei erstellen
                    output_path = os.path.join(dest_folder, output_filename)
                    # Bild speichern
                    pix.save(output_path)
                # PDF-Datei schliessen
                doc.close()
                
                # Erfolgsmeldung ausgeben
                print(f"{file} wurde erfolgreich umgewandelt und gespeichert
                in {dest_folder}")

# Pfade zu den Ordnern mit den PDF-Dateien (Quelle) und den JPG-Dateien (Ziel)
src_folder = r"/Users/svenburkhardt/Documents/D_Murger_Männer_Chor_Forschung/Scan_Männerchor/Männerchor_Akten_1925–1945/Scan_Männerchor_PDF"
dest_folder = r"/Users/svenburkhardt/Documents/D_Murger_Männer_Chor_Forschung/Masterarbeit/JPEG_Akten_Scans"


# Funktion aufrufen, um die Konvertierung durchzuführen
convert_pdf_to_jpg(src_folder, dest_folder)

\end{minted}

\subsection{Tagging in Transkribus}\label{subsec:Taggingregeln_Anhang}


    Transkribus und seine Modelle unterstützen nicht nur beim Transkribieren der Texte, sondern erlauben auch das Taggen von \textit{Named Entities}.  
    Für die vorliegende Arbeit sind dabei besonders Personen, Orte, Organisationen und Daten relevant.  
    Um hierfür ein stringentes Verfahren zu entwickeln, wurden die Tags wie folgt definiert:
  
\subsubsection{Strukturelle Tags}
    \begin{description}

    % Abbreviations
    \item\texttt{\textbf{{\colorbox{abbrev}{abbrev}}}}
    
        
    Mit dem Tag \texttt{\texttt{\textbf{{\colorbox{abbrev}{abbrev}}}}} werden alle Abkürzungen getaggt, die für eine eindeutige Entität stehen.

    
    \noindent\textbf{\ding{43} Beispiel 1}: Dr., Prof., St., Hr., Frl., Dipl.-Ing., etc.

    \textbf{\ding{43} Beispiel 2}: Organisationskürzel, wenn sie eindeutig sind:\\\code{<abbrev>~V.D.A.~</abbrev>}.

    \textbf{\textbf{\ding{43} Beispiel 3}}: Falls eine dazugehörige Entität vorhanden ist, wird die Abkürzung getaggt und wird gleichzeitig als zugehörige Entität getaggt:

    \code{<person>~<abbrev>~Dr.~</abbrev>~Weiss~</person>}
    
    % Unclear    
    \item\texttt{\textbf{{\colorbox{unclear}{unclear}}}}
    

    Mit dem Tag \texttt{\texttt{\textbf{{\colorbox{unclear}{unclear}}}}} werden unleserliche oder schwer entzifferbare Textstellen markiert.
    
    \noindent\textbf{\ding{43} Beispiel 1}: Unklare Zeichen oder fehlende Buchstaben: 

    \code{Er wohnte in~<unclear>~[\ldots]~<unclear>}.

    \textbf{\ding{43} Beispiel 2}: Teilweise lesbare Wörter:

    \code{<place>~Frei~<unclear>~[\ldots]~<unclear>~<place>}.

    
%Sic   
    \item\texttt{\textbf{{\colorbox{sic}{sic}}}} 

    Mit dem Tag \texttt{sic} werden Wörter markiert, die im Originaltext in einer falschen oder ungewöhnlichen Schreibweise geschrieben wurden.

    \noindent\ding{43} Beispiel 1: Veraltete oder falsche Schreibweisen: 

    \code{<sic>~daß~</sic>} für dass mit tz.

    \ding{43} Beispiel 2: Offensichtliche Tippfehler, wenn sie im Originaltext so vorkommen: 

    \code{Wir haben {<sic>~einen~</sic>} grosse Freude.}

    \ding{43} Beispiel 3: Falls eine Korrektur notwendig ist, kann sie als Kommentar ergänzt werden. 

    \end{description}
    \endgroup


    \subsubsection{Inhaltliche Tags}
    \begin{description}
    % Person
    \item\texttt{\textbf{{\colorbox{person}{person}}}}
        
    Mit dem Tag \texttt{\colorbox{person}{person}} sollen alle Strings getaggt, die eine direkte Zuordnung einer Person ermöglichen.
    
    \noindent \textbf{\ding{43} Beispiel 1}: Vereinsführer, Alfons, Zimmermann, Alfons Zimmermann, Z. A. Zimmermann, Herr Zimmermann, Herr Alfons Zimmermann, etc. 

    \textbf{\ding{43} Beispiel 2}: Funktionen wie Oberlehrer, Chorleiter, etc.
    Wenn Ort, Name oder Organisation bekannt sind. Eine Person kann sowohl mit ihrem Namen als auch ihrer Funktion (wie Dirigent) getaggt werden.  
    Aus der Korrespondenz ist in der Regel eine zugehörige Organisation ersichtlich, mit deren Verknüpfung eine namentlich nicht genannte Person identifiziert werden könnte.

    
    % Signature    
    \item\texttt{\textbf{{\colorbox{signature}{signature}}}}
        
    Mit dem Tag \texttt{\colorbox{signature}{signature}} werden alle Strings getaggt, die eine handschriftliche Unterschrift darstellen.  
    Der Tag \texttt{\colorbox{signature}{signature}} ist nahezu deckungsgleich mit dem Tag \texttt{\colorbox{person}{person}}.  
    Er dient zur \textbf{graduellen Unterscheidung}, ob ein Name im Fliesstext als gesichert leserlich oder handschriftlich als Signatur vorliegt.  
    
    \noindent \textbf{\ding{43} Beispiel 1}: Eindeutig lesbare Signaturen werden direkt getaggt:  

    \code{<signature>~A. Zimmermann~</signature>}. 

    \textbf{\ding{43} Beispiel 2}: Teilweise unleserliche Signaturen werden mit dem Tag \texttt{\colorbox{unclear}{unclear}} innerhalb von \texttt{\colorbox{signature}{signature}} markiert: 

    \code{<signature>~R. We~<unclear>~[\ldots]~</unclear>~</signature>}. \\
    
    \textbf{\ding{43} Beispiel 3}: Wenn nur ein Teil des Namens lesbar ist, aber eine Identifikation unsicher bleibt, sollte die Unterschrift vollständig im Tag \texttt{\colorbox{unclear}{unclear}} innerhalb von \texttt{\colorbox{signature}{signature}} stehen:\\

    \code{<signature>~<unclear>~\textit{etwas unleserliches}~</unclear>~</signature>}.
    
    \textbf{\ding{43} Beispiel 4}: Wenn eine Signatur einer bekannten Person zugeordnet werden kann, aber nicht vollständig lesbar ist, bleibt die Signatur erhalten und wird \textbf{ohne} den Tag \texttt{\colorbox{person}{person}} zu verwenden: 

    \code{<signature>~A. Zimm~<unclear>~[\ldots]~</unclear>~</signature>}.
    
    \textbf{\ding{43} Beispiel 5}: Wenn eine Unterschrift vollständig transkribiert wurde und die Person bekannt ist, wird sie nur mit \texttt{\colorbox{signature}{signature}} getaggt, \textbf{ohne} den Tag \texttt{\colorbox{person}{person}} zu verwenden:  

    \code{<signature>~Alfons Zimmermann~</signature>}.
    
    % Organization    
    \item\texttt{\textbf{{\colorbox{organization}{organization}}}}
        
    Mit dem Tag \texttt{\colorbox{organization}{organization}} werden alle Strings getaggt, die eine direkte Zuordnung einer Organisation ermöglichen.  
    
    \noindent \textbf{\ding{43} Beispiel 1}: Männerchor Murg, Verein Deutscher Arbeiter (V.D.A.), Murgtalschule, etc.

    \textbf{\ding{43} Beispiel 2}: Abkürzungen, wenn sie eine Organisation eindeutig bezeichnen, z.B. V.D.A., NSDAP, STAGMA, etc.
    

    % Place    
    \item\texttt{\textbf{{\colorbox{place}{place}}}}
        
    Mit dem Tag \texttt{\colorbox{place}{place}} werden alle Strings getaggt, die sich auf einen geografischen Ort beziehen.  
    
    \noindent \textbf{\ding{43} Beispiel 1}: Murg (Baden), Freiburg, Berlin, Murgtal, Schwarzwald, etc.

    \textbf{\ding{43} Beispiel 2}: Orte mit näherer Bestimmung, z.B. \enquote{bei Berlin}, \enquote{im Murgtal} werden getaggt:  

    \code{<place>~im Murgtal</place>}.

    
    % Date    
    \item\texttt{\textbf{{\colorbox{date}{date}}}}
        
    Mit dem Tag \texttt{\colorbox{date}{date}} werden alle expliziten und implizierten Datumsangaben markiert.  
    
    \noindent \textbf{\ding{43} Beispiel 1}:  29.05.1936

    \textbf{\ding{43} Beispiel 2}: 29. Mai 1936

    \textbf{\ding{43} Beispiel 3}: den 29.\ \  d. Mts.:

    \code{\string<date when=\enquote{29.05.1936}>~den 2.~<abbrev>\ \ d. Mts.~</abbrev> </date>}


% Event
    \item\texttt{\textbf{{\colorbox{eventTag}{event}}}}
    
    Mit dem Tag \texttt{\colorbox{eventTag}{event}} werden expliziten und implizierten Ereignisse markiert. Diese Ereignisse haben einen zeitlichen oder räumlichen Bezug, und können benannt werden. Dazu zählen:

    \noindent \textbf{\ding{43} Beispiel 1}: \enquote{Jubiläumskonzert}

    \textbf{\ding{43} Beispiel 2} \enquote{Gründung des Vereins} 

    \textbf{\ding{43} Beispiel 2}\enquote{Kriegsausbruch} oder \enquote{Kriegsende}

    Konzepte, die nicht klar in den Texten benannt werden, wie beispielsweise die Suche nach einem Dirigenten, können nicht immer Ereignis getaggt werden. Sie sollen später aber in der Datenbank implementiert werden.
    \end{description}


\subsection{Prompt der LLM Vorverarbeitung}\label{subsec:LLM-Promt}

    \begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=LightGray,
fontsize=\tiny,
linenos,
breaklines=true,  %Automatischer Zeilenumbruch
breakanywhere=true % Minted darf überall umbrechen (optional)
]{python}

    prompt = f"""
    Systemrolle:
    Du bist ein spezialisiertes XML-Annotationstool für historische Transkribus-Dokumente.

    Aufgabe:
    Analysiere das gesamte PAGE-XML-Dokument. Extrahiere Entitäten aus dem Unicode-Text aller <TextLine>-Elemente und füge strukturierte `custom="..."`-Attribute hinzu. 

    Strikte Regeln:

    1. Dokumentanalyse:
    - Verarbeite ausschliesslich <TextLine>-Elemente.
    - Verwende nur <Unicode>-Inhalte als Eingabetext.

    2. Globale Personenerkennung:
    - Erkenne Personen (inkl. Titel, Vorname, Nachname).
    - Speichere `offset` und `length` für jede erkannte Person pro TextLine.
    - Verwende **immer dieselben Offsets** bei wiederholten Nennungen im Dokument.

    3. Empfängerkennung (`recipient`):
    - Der Kopfbereich endet an der ersten komplett leeren TextLine.
    - Erkenne dort Anreden (z. B. „Herr“, „Frau“, „Sehr geehrter Herr …“).
    - Verknüpfe Anrede mit passender Person und annotiere mit:
    `recipient {{offset:X; length:Y;}}`
    
    4. Autorenkennung (`author`):
    - Der Fussbereich beginnt nach der letzten Grussformel (z. B. „Mit freundlichen Grüssen“).
    - Namen → `author {{offset:X; length:Y;}}`.
    - Funktion (z. B. „Chorleiter“) → `role {{offset:X; length:Y;}}`.

     5. Ort- und Datumsannotation:
    - **Absendeort** (creation_place) und **Erstellungsdatum** (creation_date):
        zusätzlich zu den Tags place und date hinzu:
        creation_place {{offset:X; length:Y;}} und creation_date {{offset:X; length:Y; when:TT.MM.JJJJ;}}.
    - **Empfangsort** (recipient_place):
        Füge im Empfänger-Block die passende Zeile mit:
        place {{offset:X; length:Y;}}.

    6. Entitäten pro Zeile (in dieser Reihenfolge):
    Füge **ein** Attribut `custom="..."` ein mit nur den tatsächlich erkannten Entitäten:


    person {{offset:X; length:Y;}}
    recipient {{offset:X; length:Y;}}
    author {{offset:X; length:Y;}}
    organization {{offset:X; length:Y;}}
    place {{offset:X; length:Y;}}
    date {{offset:X; length:Y; when:TT.MM.JJJJ;}}
    role {{offset:X; length:Y;}}
    event {{offset:X; length:Y;}} → optional mit when:TT.MM.JJJJ;

    Hinweise:
    - Füge **nur tatsächlich vorhandene Entitäten** ein.
    - Keine Platzhalter.
    - Format für `date` und `event` (falls Datum erkennbar): `when:TT.MM.JJJJ;`
    - Mehrzeilige Events (z. B. bei Bindestrich am Ende oder fortgeführtem Satz) erhalten dieselbe `event`-Annotation in allen betroffenen Zeilen.

    6. XML-Regeln:
    - **Verändere nur** `custom`-Attribute innerhalb von `<TextLine>`.
    - Belasse alle anderen XML-Strukturen vollständig unverändert.

    7. Ausgabe:
    - Gib ausschliesslich ein vollständiges, wohlgeformtes XML zurück.
    - Kein Freitext, kein Kommentar, kein Markdown.

    Beispielausgabe:
    <?xml version="1.0" encoding="UTF-8"?>
    <PcGts xmlns="http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15">
    <Page imageFilename="dummy.jpg" imageWidth="1000" imageHeight="1000">
        <TextRegion id="r1">
        <TextLine id="tl1" custom="place {{offset:0; length:7;}} creation_place {{offset:0; length:7;}} date {{offset:8; length:9; when:28.05.1942;}} creation_date {{offset:8; length:9; when:28.05.1942;}}">
            <Coords points="0,0 100,0 100,10 0,10"/>
            <TextEquiv><Unicode>München 28.V.1942</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl2" custom="recipient {{offset:7; length:5;}} person {{offset:7; length:5;}} place {{offset:15; length:6;}} recipient_place {{offset:15; length:6;}}">
            <Coords points="0,20 100,20 100,30 0,30"/>
            <TextEquiv><Unicode>Lieber Otto, Berlin</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl3" custom="event {{offset:24; length:38; when:28.05.1942;}} place {{offset:42; length:7;}}">
            <Coords points="0,40 100,40 100,50 0,50"/>
            <TextEquiv><Unicode>Heute abend fand ein Konzert im Opernhaus in München statt, und ich</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl4" custom="organization {{offset:43; length:28;}} place {{offset:66; length:16;}}">
            <Coords points="0,60 100,60 100,70 0,70"/>
            <TextEquiv><Unicode>lauschte den himmlischen Stimmen des Männerchors Hintertuüpfingen eV.</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl5" custom="organization {{offset:34; length:3;}} organization {{offset:40; length:18;}}">
            <Coords points="0,80 100,80 100,90 0,90"/>
            <TextEquiv><Unicode>Das alles fand im Rahmen des WhW - des Winterhilfswerk statt.</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl6" custom="organization  {{offset:50; length:17;}} place {{offset:72; length:4;}} place {{offset:83; length:6;}}">
            <Coords points="0,100 100,100 100,110 0,110"/>
            <TextEquiv><Unicode>Ich hoffe wir sehen uns bald bei einem Auftritt des Männerchors Murg wieder, oder in Hänner?</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl7" custom="role {{offset:14; length:14;}} person {{offset:29; length:4;}}">
            <Coords points="0,120 100,120 100,130 0,130"/>
            <TextEquiv><Unicode>Grüss mir den Vereinsführer Asal,</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl8">
            <Coords points="0,140 100,140 100,150 0,150"/>
            <TextEquiv><Unicode>Alles Liebe,</Unicode></TextEquiv>
        </TextLine>
        <TextLine id="tl9" custom="author {{offset:6; length:17;}} person {{offset:6; length:17;}}">
            <Coords points="0,160 100,160 100,170 0,170"/>
            <TextEquiv><Unicode>Deine Lina Fingerdick</Unicode></TextEquiv>
        </TextLine>
        <!-- Neue Zeile für den Empfangsort -->
        <TextLine id="tl10" custom="salutation {{offset:0; length:2;}} recipient {{offset:3; length:13;}} address {{offset:18; length:21;}} place {{offset:41; length:4;}}">
            <Coords points="0,180 100,180 100,190 0,190"/>
            <TextEquiv>
            <Unicode>An Otto Bolliger, Adolf-Hitler Platz 1, Murg</Unicode>
            </TextEquiv>
        </TextLine>
        </TextRegion>
    </Page>
    </PcGts>

    Hier ist das zu annotierende XML:

```
{xml_content}
```
"""
\end{minted}

    
   \end{document}
